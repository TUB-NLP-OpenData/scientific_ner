{"id": 4388, "text": "Max-sum diversity is a fundamental primitive for web search and data mining. For a given set S of n elements, it returns a subset of k«l n representatives maximizing the sum of their pairwise distances, where distance models dissimilarity. An important variant of the primitive prescribes that the desired subset of representatives satisfies an additional orthogonal requirement, which can be specified as a matroid constraint (i.e., a feasible solution must be an independent set of size k). While unconstrained max-sum diversity admits efficient coreset-based strategies, the only known approaches dealing with the additional matroid constraint are inherently sequential and are based on an expensive local search over the entire input set. We devise the first coreset constructions for max-sum diversity under various matroid constraints, together with efficient sequential, MapReduce and Streaming implementations. By running the local-search on the coreset rather than on the entire input, we obtain the first practical solutions for large instances. Technically, our coresets are subsets of S containing a feasible solution which is no more than a factor 1-ε away from the optimal solution, for any fixed ε <1, and, for spaces of bounded doubling dimension, they have a small size independent of n. Extensive experiments show that, with respect to full-blown local search, our coreset-based approach yields solutions of comparable quality, with improvements of up to two orders of magnitude in the running time, also for input sets of unknown dimensionality.", "meta": {}, "annotation_approver": null, "labels": [[49, 59, "Task"], [64, 75, "Task"], [934, 946, "Method"], [548, 572, "Method"], [878, 887, "Method"], [892, 901, "Method"], [408, 426, "Method"], [703, 715, "Method"], [628, 647, "Method"], [821, 840, "Method"], [0, 17, "Task"], [789, 806, "Task"]]}
{"id": 4389, "text": "The key objective of an information retrieval (IR) system is to identify and return to the user content relevant or useful in addressing the information need which required them to use the system. The development and evaluation of IR systems relies on the availability of suitable datasets or test collections. These typically consist of a target document collection, example search queries representative of those that users of the system to be developed, are expected to enter, and relevance data indicating which documents in the collection are relevant to the information needed as expressed in each query. Public research in IR has focused on popular content, e.g. news corpora or web content, for which average users can pose queries expressing information needs and judge the relevance of retrieved documents. This is not the case for professional search applications, for example legal, medical, financial search where domain experts are required for these tasks. We describe our experiences from the development of a professional legal IR application employing semantic search technologies. Our activities indicate the vital need for close interaction between the professionals for which the application is being developed and the IR researchers throughout the development life cycle of the search system. Such engagement is vital in order for the IR researchers to understand the working practices of the professional searchers, the specifications of their information needs and the domain in which they are searching, and to study how they engage and interact with information. A key reason to seek to understand these topics so deeply is to facilitate meaningful evaluation of the effectiveness of IR technologies as components in the system being developed.", "meta": {}, "annotation_approver": null, "labels": [[1039, 1047, "Task"], [1070, 1098, "Method"], [231, 233, "Task"], [630, 632, "Task"], [670, 682, "Material"], [24, 50, "Task"], [1710, 1712, "Task"]]}
{"id": 4390, "text": "In this paper, we propose a method to find out public social media accounts of refugees, and trace them back to infer temporal and spatial events.These data will have many future applications in planning and design of solutions for their problems and needs, starting from legislations and social ones, to architectural and housing needs.It will lead to better understanding of the obstacles they face which they fear to express in direct interviews and inquests. In this first application, we present our method to retrieve information from social media, share the characteristics of the data gathered and perform initial analysis with a discussion of future opportunities and difficulties.", "meta": {}, "annotation_approver": null, "labels": [[38, 87, "Task"], [541, 553, "Material"], [112, 145, "Task"]]}
{"id": 4391, "text": "Network embedding methods aim at learning low-dimensional latent representation of nodes in a network. While achieving competitive performance on a variety of network inference tasks such as node classification and link prediction, these methods treat the relations between nodes as a binary variable and ignore the rich semantics of edges. In this work, we attempt to learn network embeddings which simultaneously preserve network structure and relations between nodes. Experiments on several real-world networks illustrate that by considering different relations between different node pairs, our method is capable of producing node embeddings of higher quality than a number of state-of-the-art network embedding methods, as evaluated on a challenging multi-label node classification task.", "meta": {}, "annotation_approver": null, "labels": [[375, 393, "Task"], [0, 17, "Task"], [698, 715, "Task"], [755, 786, "Task"], [191, 210, "Task"]]}
{"id": 4392, "text": "Cybercriminals abuse Online Social Networks (OSNs) to lure victims into a variety of spam. Among different spam types, a less explored area is OSN abuse that leverages the telephony channel to defraud users. Phone numbers are advertized via OSNs, and users are tricked into calling these numbers. To expand the reach of such scam / spam campaigns, phone numbers are advertised across multiple platforms like Facebook, Twitter, GooglePlus, Flickr, and YouTube. In this paper, we present the first data-driven characterization of cross-platform campaigns that use multiple OSN platforms to reach their victims and use phone numbers for monetization. We collect -23M posts containing -1.8M unique phone numbers from Twitter, Facebook, GooglePlus, Youtube, and Flickr over a period of six months. Clustering these posts helps us identify 202 campaigns operating across the globe with Indonesia, United States, India, and United Arab Emirates being the most prominent originators. We find that even though Indonesian campaigns generate highest volume (-3.2M posts), only 1.6% of the accounts propagating Indonesian campaigns have been suspended so far. By examining campaigns running across multiple OSNs, we discover that Twitter detects and suspends -93% more accounts than Facebook. Therefore, sharing intelligence about abuse-related user accounts across OSNs can aid in spam detection. According to our dataset, around -35K victims and -$8.8M could have been saved if intelligence was shared across the OSNs. By analyzing phone number based spam campaigns running on OSNs, we highlight the unexplored variety of phone-based attacks surfacing on OSNs.", "meta": {}, "annotation_approver": null, "labels": [[664, 669, "Material"], [713, 720, "Material"], [722, 730, "Material"], [732, 742, "Material"], [744, 751, "Material"], [757, 763, "Material"], [810, 815, "Material"], [1218, 1225, "Material"], [1271, 1279, "Material"]]}
{"id": 4393, "text": "In this paper we investigate the accuracy and overall suitability of a variety of Entity Linking systems for the task of disambiguating entities in 17th century depositions obtained during the 1641 Irish Rebellion. The depositions are extremely difficult for modern NLP tools to work with due to inconsistent spelling, use of language and archaic references. In order to assess the severity of difficulty faced by Entity Linking systems when working with the depositions we use them to create an evaluation corpus. This corpus is used as an input to the General Entity Annotator Benchmarking Framework a standard benchmarking platform for entity annotation systems. Based on this corpus and the results obtained from General Entity Annotator Benchmarking Framework we observe that the accuracy of existing Entity Linking systems is lacking when applied to content like these depositions. This is due to a number of issues ranging from problems with existing state-of-the-art systems to poor representation of historic entities in modern knowledge bases. We discuss some interesting questions raised by this evaluation and put forward a plan for future work in order to learn more.", "meta": {}, "annotation_approver": null, "labels": [[785, 793, "Metric"], [121, 144, "Task"], [414, 428, "Task"], [554, 601, "Method"], [33, 41, "Metric"], [82, 96, "Task"], [639, 656, "Task"], [717, 764, "Method"], [806, 820, "Task"]]}
{"id": 4394, "text": "Generating Linked Data remains a complicated and intensive engineering process. While different factors determine how a Linked Data generation algorithm is designed, potential alternatives for each factor are currently not considered when designing the tools’ underlying algorithms. Certain design patterns are frequently applied across different tools, covering certain alternatives of a few of these factors, whereas other alternatives are never explored. Consequently, there are no adequate tools for Linked Data generation for certain occasions, or tools with inadequate and inefficient algorithms are chosen. In this position paper, we determine such factors, based on our experiences, and present a preliminary list. These factors could be considered when a Linked Data generation algorithm is designed or a tool is chosen. We investigated which factors are covered by widely known Linked Data generation tools and concluded that only certain design patterns are frequently encountered. By these means, we aim to point out that Linked Data generation is above and beyond bare implementations, and algorithms need to be thoroughly and systematically studied and exploited.", "meta": {}, "annotation_approver": null, "labels": [[0, 22, "Task"], [290, 306, "Method"], [504, 526, "Task"], [120, 142, "Task"], [764, 786, "Task"], [888, 910, "Task"], [1034, 1056, "Task"]]}
{"id": 4395, "text": "Gossip is one of the most widespread human activities with multiple functions such as enhancing human cooperation, establishing social order, information sharing, norm enhancing or stress reduction. Gossip has been analyzed mostly by qualitative or survey methods. In this paper, we describe a quantitative approach to identify gossip in a large corpus containing spontaneous talk with LDA topic modeling and quantitative analysis. We aim to identify gossip and its characteristics to analyze its topics, the verbal and non-verbal emotions that were used during gossiping, and other non-textual data such as the number of speakers and the number of persons present during the gossiping events. We also analyze the topics to distinguish gossiping and storytelling by dividing gossip and non-gossip texts in our large spontaneous speech corpora.", "meta": {}, "annotation_approver": null, "labels": [[319, 334, "Task"], [386, 404, "Method"], [409, 430, "Method"], [442, 457, "Task"], [828, 842, "Material"]]}
{"id": 4396, "text": "Bias, may it be in sampling or judgment, is not a new topic. However, with the increasing usage of data and models trained from them in almost all areas of everyday life, the topic rapidly gains relevance to the broad public. Even more, the opportunistic reuse of data (traces) that characterizes today’s data science calls for new ways to understand and mitigate the effects of biases. Here, we discuss biases in the context of Linked Data, ontologies, and reasoning services and point to the need for both technical and social solutions. We believe that debiasing knowledge graphs will become a pressing issue as these graphs enter everyday life rapidly. This is a provocative topic, not only from a technical perspective but because it will force us as a Semantic Web community to discuss whether we want to debias in the first place and who gets a say in how to do so.", "meta": {}, "annotation_approver": null, "labels": [[340, 385, "Task"], [429, 440, "Material"], [442, 452, "Material"], [556, 582, "Task"], [811, 817, "Task"]]}
{"id": 4397, "text": "ICT4D seeks to bridge the digital divide in developing countries. Important requirements of ICT4D projects are a demand-driven approach and participation of the local community. The fact that user collaboration is a principle of Agile software development (Agile), triggers our interest on whether Agile practices can improve ICT4D projects. This paper aims to investigate if and how Agile can contribute to the success of ICT4D projects. In order to achieve this, existing literature was consulted and an interview was held. This paper provides an overview of the critical success factors for ICT4D projects and Agile, as well as of the advantages of Agile. Agile can only work successfully when ICT4D projects are demand-driven, and when both a cultural understanding and trust are built. Notable ways in which Agile can improve ICT4D projects are by facilitating user collaboration, improving team communication, enhancing organizational learning, and by frequently delivering software.", "meta": {}, "annotation_approver": null, "labels": [[380, 437, "Task"]]}
{"id": 4398, "text": "To define salient rhetorical elements in scholarly text, we have earlier defined a set of Discourse Segment Types: semantically defined spans of discourse at the level of a clause with a single rhetorical purpose, such as Hypothesis, Method or Result. In this paper, we use machine learning methods to predict these Discourse Segment Types in a corpus of biomedical research papers. The initial experiment used features related to verb type and form, obtaining F-scores ranging from 0.41-0.65. To improve our results, we explored a variety of methods for balancing classes, before applying classification algorithms. We also performed an ablation study and stepwise approach for feature selection. Through these feature selection processes, we were able to reduce our 37 features to the 7 most informative ones, while maintaining F1 scores in the range of 0.630.65. Next, we performed an experiment with a reduced set of target classes. Using only verb tense features, logistic regression, a decision tree classifier and a random forest classifier, we predicted that a segment type was either a Result/Method or a Fact/Implication, with F1 scores above 0.8. Interestingly, findings from this machine learning approach are in line with a reader experiment, which found a correlation between verb tense and a biomedical reader’s interpretation of discourse segment type. This suggests that experimental and concept-centric discourse in biology texts can be distinguished by humans or machines, using verb tense as a key feature.", "meta": {}, "annotation_approver": null, "labels": [[461, 469, "Metric"], [590, 615, "Method"], [969, 988, "Method"], [992, 1016, "Method"], [1023, 1047, "Method"], [302, 339, "Task"], [355, 381, "Material"], [90, 107, "Task"], [679, 696, "Task"], [1137, 1139, "Metric"], [1434, 1447, "Material"], [274, 290, "Method"], [830, 832, "Metric"], [1192, 1208, "Method"]]}
{"id": 4399, "text": "Users leave a trail of their personal data, interests, and intents while surfing or sharing information on the Web. Web data could therefore reveal some private/sensitive information about users based on inference analysis. The possible identification of information corresponding to a single individual by an inference attack holds true even if the user identifiers are encoded or removed in the Web data. Several works have been done on improving privacy of Web data through obfuscation methods [6, 8, 12, 21]. However, these methods are neither comprehensive, generic to be applicable to any Web data, nor effective against adversarial attacks. To this end, we propose a privacy-aware obfuscation method for Web data addressing these identified drawbacks of existing methods. We use probabilistic methods to predict privacy risk of Web data that incorporates all key privacy aspects, which are uniqueness, uniformity, and linkability of Web data. The Web data with high predicted risk are then obfuscated by our method to minimize the privacy risk using semantically similar data. Our method is resistant against adversary who has knowledge about the datasets and model learned risk probabilities using differential privacy-based noise addition. Experimental study conducted on two real Web datasets validates the significance and efficacy of our method. Our results indicate that the average privacy risk reaches to 100% with a minimum of 10 sensitive Web entries, while at most 0% privacy risk could be attained with our obfuscation method at the cost of average utility loss of 64.3%.", "meta": {}, "annotation_approver": null, "labels": [[786, 799, "Method"], [439, 468, "Task"], [595, 603, "Material"], [674, 699, "Method"], [835, 843, "Material"], [940, 948, "Material"], [954, 962, "Material"], [1388, 1408, "Metric"], [1486, 1498, "Metric"], [1560, 1580, "Metric"]]}
{"id": 4400, "text": "Studying recommender systems under streaming scenarios has become increasingly important because real-world applications produce data continuously and rapidly. However, most existing recommender systems today are designed in the context of an offline setting. Compared with the traditional recommender systems, large-volume and high-velocity are posing severe challenges for streaming recommender systems. In this paper, we investigate the problem of streaming recommendations being subject to higher input rates than they can immediately process with their available system resources (i.e., CPU and memory). In particular, we provide a principled framework called as SPMF (Stream-centered Probabilistic Matrix Factorization model), based on BPR (Bayesian Personalized Ranking) optimization framework, for performing efficient ranking based recommendations in stream settings. Experiments on three real-world datasets illustrate the superiority of SPMF in online recommendations.", "meta": {}, "annotation_approver": null, "labels": [[451, 476, "Task"], [668, 731, "Method"], [742, 800, "Method"], [826, 875, "Task"], [948, 952, "Method"]]}
{"id": 4401, "text": "The iterative continuum of scientific production generates a need for filtering and specific crossing of ideas and papers. In this paper, we present BIBLME RecSys software which is dedicated to the analysis of bibliographical references extracted from scientific collections of papers. Our goal is to provide users with paper suggestions guided by the papers they are reading and by the references they contain. To do so, we propose a new approach based on a new bibliometric measure. We propose to determine the impact, the inner representativeness, of each bibliographical reference according to their occurrences in the paper the user is reading. By means of this approach, we suggest central references of the author’s paper. As a result, we obtain papers that are related to the paper selected by the user according to the influence of references on it. We evaluate the recommendation in the context of a digital library dedicated to humanities and social sciences.", "meta": {}, "annotation_approver": null, "labels": [[198, 236, "Task"], [459, 483, "Method"], [252, 284, "Material"]]}
{"id": 4402, "text": "Building the collection of an institutional repository requires a complex understanding of both digital library infrastructure and staff resources, as well as the institution's faculty awareness and attitudes toward self-archiving. For collection development decisions, institutional repository (IR) managers weigh the influence of these factors when pursuing strategies to increase content and faculty participation. To evaluate strategies for collection development, the authors will apply the Analytic Hierarchy Process (AHP) to create a model through which collection development strategies can be evaluated based on the unique context of the institution.", "meta": {}, "annotation_approver": null, "labels": [[496, 528, "Method"], [0, 54, "Task"]]}
{"id": 4403, "text": "Networks are natural analytic tools in modeling adversarial activities(e.g., human trafficking, illicit drug production, terrorist financial transaction) using different intelligence data sources. However, such activities are often covert and embedded across multiple domains and contexts. They are generally not detectable and recognizable from the perspective of an isolated network, and only become apparent when multiple networks are analyzed in a joint manner. Thus, one of the main research topics in modeling adversarial activities is to develop effective techniques to align and fuse information from different networks into a unified representation for global analysis. Based on the combined network representation, an equally important research topic is on detecting and matching indicating patterns to recognize the underlining adversarial activities in the integrated network. The focus of this workshop is to gather together the researchers from all relevant fields to share their experience and opinions on graph mining techniques in the era of big data, with emphasis on two fundamental problems - \"Connecting the dots\" and \"finding a needle in a haystack\", in the context of graph-based adversarial activity analytics.", "meta": {}, "annotation_approver": null, "labels": [[507, 538, "Task"], [1021, 1033, "Method"], [39, 70, "Task"], [1114, 1133, "Task"], [1140, 1170, "Task"], [1191, 1233, "Task"]]}
{"id": 4404, "text": "Knowledge graphs (KG) are being used extensively in different industries for data driven applications. These industrial knowledge graphs, due to their large scale and heterogeneity, are often constructed using automated information extraction (IE) toolkits. Owing to the diverse nature of the sources, such extractions are often noisy and contain many semantic inaccuracies. High quality, consistent KGs are critical to effective predictive analytics and decision support. For example, many commercial question answering systems rely heavily on accurate and consistent knowledge graphs generated from life sciences content. These systems typically require an extensible, scalable, and generalizable framework. To address these issues, we build on previous work in ontology and instance data evaluation and propose a method for Large-Scale Knowledge Graph Evaluation. The approach leverages domain ontologies to detect possible inconsistencies. We construct an RDF/RDFS knowledge graph from the output of a state-of-the-art biomedical IE system, ODIN, and demonstrate that it is easy to construct general inconsistency rules for quality control. In this paper we present our results after applying these rules to the KG and then discuss how our approach and implementation can generalize to many large scale industrial knowledge graphs.", "meta": {}, "annotation_approver": null, "labels": [[210, 256, "Method"], [827, 865, "Task"], [400, 403, "Material"], [502, 520, "Task"], [569, 585, "Material"], [786, 801, "Task"], [960, 984, "Material"], [1034, 1036, "Task"], [1216, 1218, "Material"], [1295, 1334, "Material"], [0, 21, "Material"], [120, 136, "Material"]]}
{"id": 4405, "text": "In recent years there has been an increasing interest in approaches to scientific summarization that take advantage of the citations a research paper has received in order to extract its main contributions. In this context, the CL-SciSumm 2017 Shared Task has been proposed to address citation-based information extraction and summarization. In this paper we present several systems to address three of the CL-SciSumm tasks. Notably, unsupervised systems to match citing and cited sentences (Task 1A), a supervised approach to identify the type of information being cited (Task 1B), and a supervised citation-based summarizer (Task 2).", "meta": {}, "annotation_approver": null, "labels": [[504, 523, "Method"], [458, 490, "Task"], [527, 571, "Task"], [71, 95, "Task"], [285, 322, "Task"], [327, 340, "Task"], [434, 454, "Method"], [589, 625, "Method"]]}
{"id": 4406, "text": "We describe an automated assistant for answering frequently asked questions; our system has been deployed, and is currently answering HR-related queries in two different areas (leave management and health insurance) to a large number of users. The needs of a large global corporate lead us to model a frequently asked question (FAQ) to be an equivalence class of actually asked questions, for which there is a common answer (certified as being consistent with the organization's policy). When a new question is posed to our system, it finds the class of question, and responds with the answer for the class. At this point, the system is either correct (gives correct answer); or incorrect (gives wrong answer); or incomplete (says \"I don't know''). We employ a hybrid deep-learning architecture in which a BiLSTM-based classifier is combined with second BiLSTM-based Siamese network in an iterative manner: Questions for which the classifier makes an error during training are used to generate a set of misclassified question-question pairs. These, along with correct pairs, are used to train the Siamese network to drive apart the (hidden) representations of the misclassified pairs. We present experimental results from our deployment showing that our iteratively trained hybrid network: (a) results in better performance than using just a classifier network, or just a Siamese network; (b) performs better than state-of-the art sentence classifiers in the two areas in which it has been deployed, in terms of both accuracy as well as precision-recall tradeoff; and (c) also performs well on a benchmark public dataset. We also observe that using question-question pairs in our hybrid network, results in marginally better performance than using question-to-answer pairs. Finally, estimates of precision and recall from the deployment of our automated assistant suggest that we can expect the burden on our HR department to drop from answering about 6000 queries a day to about 1000.", "meta": {}, "annotation_approver": null, "labels": [[761, 794, "Method"], [806, 829, "Method"], [854, 882, "Method"], [1596, 1620, "Material"], [1097, 1112, "Method"], [1372, 1387, "Method"], [1342, 1360, "Method"], [1517, 1525, "Metric"], [1537, 1546, "Metric"], [1547, 1553, "Metric"], [1796, 1805, "Metric"], [1810, 1816, "Metric"]]}
{"id": 4407, "text": "There is considerable interest among both researchers and the mass public in understanding the topics of discussion on social media as they occur over time. Scholars have thoroughly analysed samplingbased topic modelling approaches for various text corpora including social media; however, another LDA topic modelling implementation— Variational Bayesian (VB)—has not been well studied, despite its known e ciency and its adaptability to the volume and dynamics of social media data. In this paper, we examine the performance of the VB-based topic modelling approach for producing coherent topics, and further, we extend the VB approach by proposing a novel time-sensitive Variational Bayesian implementation, denoted as TVB. Our newly proposed TVB approach incorporates time so as to increase the quality of the generated topics. Using a Twitter dataset covering 8 events, our empirical results show that the coherence of the topics in our TVB model is improved by the integration of time. In particular, through a user study, we find that our TVB approach generates less mixed topics than state-of-the-art topic modelling approaches. Moreover, our proposed TVB approach can more accurately estimate topical trends, making it particularly suitable to assist end-users in tracking emerging topics on social media.", "meta": {}, "annotation_approver": null, "labels": [[334, 359, "Method"], [205, 220, "Task"], [658, 693, "Method"], [721, 724, "Method"], [298, 301, "Method"], [533, 557, "Method"], [745, 748, "Method"], [941, 950, "Method"], [1045, 1048, "Method"], [1159, 1171, "Method"], [839, 846, "Material"], [1300, 1312, "Material"]]}
{"id": 4408, "text": "In many Information Retrieval tasks, the boundary between classes is not well defined, and assigning a document to a specific class may be complicated, even for humans. For instance, a document which is not directly related to the user's query may still contain relevant information. In this scenario, an option is to define an intermediate class collecting ambiguous instances. Yet some natural questions arise. Is this annotation strategy convenient? how should the intermediate class be treated? To answer these questions, we explored two community question answering datasets whose comments were originally annotated with three classes. We re-annotated a subset of instances considering a binary good vs bad setting. Our main contribution is to show empirically that the inclusion of an intermediate class to assess Boolean relevance is not useful. Moreover, in case the data is already annotated with a 3-class strategy, the instances from the intermediate class can be safely removed at training time.", "meta": {}, "annotation_approver": null, "labels": [[813, 837, "Task"], [644, 719, "Method"], [8, 29, "Task"], [542, 570, "Task"], [908, 924, "Method"]]}
{"id": 4409, "text": "In recent years a huge amount of publications and scientific reports has become available through digital libraries and online databases. Digital libraries commonly provide advanced search interfaces, through which researchers can find and explore the most related scientific studies. Even though the publications of a single author can be easily retrieved and explored, understanding how authors have collaborated with each other on specific research topics and to what extent their collaboration have been fruitful is, in general, a challenging task. This paper proposes a new pattern-based approach to analyzing the correlations among the authors of most influential research studies. To this purpose, it analyzes publication data retrieved from digital libraries and online databases by means of an itemset-based data mining algorithm. It automatically extracts patterns representing the most relevant collaborations among authors on specific research topics. Patterns are evaluated and ranked according to the number of citations received by the corresponding publications. The proposed approach was validated in a real case study, i.e., the analysis of scientific literature on genomics. Specifically, we first analyzed scientific studies on genomics acquired from the OMIM database to discover correlations between authors and genes or genetic disorders. Then, the reliability of the discovered patterns was assessed using the PubMed search engine. The results show that, for the majority of the mined patterns, the most influential (top ranked) studies retrieved by performing author-driven PubMed queries range over the same gene/genetic disorder indicated by the top ranked pattern.", "meta": {}, "annotation_approver": null, "labels": [[579, 601, "Method"], [1275, 1279, "Material"], [1434, 1440, "Material"], [371, 458, "Task"], [463, 516, "Task"], [803, 828, "Method"], [1599, 1605, "Material"]]}
{"id": 4410, "text": "Smart Cities have emerged as a global concept that argues for the effective exploitation of digital technologies to drive sustainable innovation and well-being for citizens. Despite the large investments being placed on Smart City infrastructure, however, there is still very scarce attention on the new learning approaches that will be needed for cultivating Digital Smart Citizenship competences, namely the competences which will be needed by the citizens and workforce of such cities for exploiting the digital technologies in creative and innovative ways for driving financial and societal sustainability. In this context, this paper introduces cyberphysical learning as an overarching model of cultivating Digital Smart Citizenship competences by exploiting the potential of Internet of Things technologies and social media, in order to create authentic blended and augmented learning experiences.", "meta": {}, "annotation_approver": null, "labels": [[650, 672, "Method"], [700, 749, "Task"], [348, 397, "Task"]]}
{"id": 4411, "text": "The presence of militant group Islamic State of Iraq and Syria (ISIS) is growing. Terrorist attacks in Europe and an incoming stream of refugees in the south of the continent are some of the reasons Europe is getting socially involved in the Middle-Eastern war. It might seem that the Netherlands could become a target of the organization too. But are Dutch citizens concerned about this? In this paper, we describe the reaction of the Dutch on ISIS by analyzing what they say on Twitter about the organization. With the use of text classification, topic modeling and visualization tools, we were able to retrieve Tweets about ISIS and create a network graph displaying the ten main topics about ISIS which Dutch people tweeted about, in addition to a word cloud, displaying the words which were most used in the Tweets. The visualizations are used to analyze what topics people discussed most regarding ISIS on Twitter and how they felt about these topics. Understanding the social network responses to ISIS’ attacks can give us some clues to understand its impact on the society.", "meta": {}, "annotation_approver": null, "labels": [[407, 449, "Task"], [480, 487, "Material"], [813, 819, "Material"], [528, 547, "Method"], [549, 563, "Method"], [614, 620, "Material"], [958, 1017, "Task"]]}
{"id": 4412, "text": "We present the Word importance-based similarity of documents metric (WISDM), a fast and scalable novel method for document similarity/distance computation for analysis of scientific documents. It is based on recent advancements in the area of word embeddings. WISDM combines learned word vectors together with traditional count-based models for document similarity computation, eventually achieving state-of-the-art performance and precision. The novel method first selects from two text documents those words that carry the most information and forms a word set for each document respectively. Then it relies on an existing word embeddings model to get the vector representations of the selected words. In the final step, it computes the closeness of the two sets of word vector representations, fit into a matrix, using a correlation coefficient. The presented metric was evaluated on three tasks, relevant to the analysis of scientific documents, and three data sets of open access scientific research. The results demonstrate that WISDM achieves significant performance speed-up in comparison to state-of-the-art metrics with a very marginal drop in precision.", "meta": {}, "annotation_approver": null, "labels": [[171, 191, "Material"], [114, 154, "Task"], [15, 60, "Method"], [69, 74, "Method"], [345, 376, "Task"], [432, 441, "Metric"], [1074, 1082, "Metric"], [260, 265, "Method"], [1035, 1040, "Method"]]}
{"id": 4413, "text": "We use the ServiceWorker (SW) API to intercept HTTP requests for embedded resources and reconstruct Composite Mementos without the need for conventional URL rewriting typically performed by web archives. URL rewriting is a problem for archival replay systems, especially for URLs constructed by JavaScript, that frequently results in incorrect URI references. By intercepting requests on the client using SW, we are able to strategically reroute instead of rewrite. Our implementation moves rewriting to clients, saving servers' computing resources and allowing servers to return responses more quickly. In our experiments, retrieving the original instead of rewritten pages from the archive resulted in a one- third reduction in time overhead and a one-fifth reduction in data overhead. Our system, reconstructive.js, prevents the live web from leaking into Composite Mementos while being easy to distribute and maintain.", "meta": {}, "annotation_approver": null, "labels": [[190, 202, "Material"], [204, 217, "Task"]]}
{"id": 4414, "text": "We propose an extension to language models for information retrieval. Typically, language models estimate the probability of a document generating the query, where the query is considered as a set of independent search terms. We extend this approach by considering the concepts implied by both the query and words in the document. The model combines the probability of the document generating the concept embodied by the query, and the traditional language model probability of the document generating the query terms. We use a word embedding space to express concepts. The similarity between two vectors in this space is estimated using a weighted cosine distance. The weighting significantly enhances the discrimination between vectors. We evaluate our model on benchmark datasets (TREC 6–8) and empirically demonstrate it outperforms state-of-the-art baselines.", "meta": {}, "annotation_approver": null, "labels": [[784, 792, "Material"], [27, 42, "Method"], [47, 68, "Task"]]}
{"id": 4415, "text": "Given an instance (Julieta Pinto), most methods for open-domain information extraction focus on acquiring knowledge in the form of either class labels (Costa Rican short story writers, Women novelists) referring to concepts to which the instance belongs; or facts (nationality: Costa Rica) connecting the instance (Julieta Pinto) to other instances or concepts (Costa Rica), where the fact and the other instance often take the form of an attribute (nationality) and a value (Costa Rica) respectively. From extraction through internal representation and storage, class labels and facts are treated as if they carved out disconnected slices within the larger space of factual knowledge. This paper argues that class labels and facts pertaining to an instance exist in symbiosis rather than as a dichotomy. A constituent (Costa Rican) within a class label (Costa Rican short story writers) of an instance may be indicative of a fact (nationality: Costa Rica) applicable to the instance and vice-versa. As an illustration of the relationship between class labels and facts, the paper introduces an open-domain method for the better understanding of the semantics of class labels in one of the larger and most widely-used repositories of knowledge, namely the categories in the Wikipedia category network. The method exploits the category network to associate constituents (Costa Rican) within names of Wikipedia categories, with attributes (nationality) that explain their role.", "meta": {}, "annotation_approver": null, "labels": [[52, 86, "Task"], [1274, 1300, "Material"]]}
{"id": 4416, "text": "This paper describes the development of a teaching tool for exploring fifty-seven early Iberian writings on Southeast Asia from the sixteenth to mid-seventeenth century. This is an interdisciplinary grassroot effort initated by a historian and two librarians, in hope of stimulating conversations surrounding adoption of digital methods in the local historian community. The teaching tool takes the form of interactive connection maps. It augments the exploration of where these early texts were published, their subsequent reprints or translations over space and time, and the system of influences behind early European concepts of Southeast Asia.", "meta": {}, "annotation_approver": null, "labels": [[60, 122, "Task"]]}
{"id": 4417, "text": "We address the task of entity-relationship (E-R) retrieval, i.e, given a query characterizing types of two or more entities and relationships between them, retrieve the relevant tuples of related entities. Answering E-R queries requires gathering and joining evidence from multiple unstructured documents. In this work, we consider entity and relationships of any type, i.e, characterized by context terms instead of pre-defined types or relationships. We propose a novel IR-centric approach for E-R retrieval, that builds on the basic early fusion design pattern for object retrieval, to provide extensible entity-relationship representations, suitable for complex, multi-relationships queries. We performed experiments with Wikipedia articles as entity representations combined with relationships extracted from ClueWeb-09-B with FACC1 entity linking. We obtained promising results using 3 different query collections comprising 469 E-R queries.", "meta": {}, "annotation_approver": null, "labels": [[832, 837, "Material"], [814, 826, "Material"], [23, 58, "Task"], [206, 227, "Task"], [496, 509, "Task"], [568, 584, "Task"], [726, 744, "Material"]]}
{"id": 4418, "text": "In the medical context, causal knowledge usually refers to causal relations between diseases and symptoms, living habits and diseases, symptoms which get better and therapy, drugs and side-effects, etc [3]. All these causal relations are usually in medical literature, forum and clinical cases and compose the core part of medical diagnosis. Therefore, mining these causal knowledge to predict disease and recommend therapy is of great value for assisting patients and professionals.\n The task of mining these causal knowledge for diagnosis assistance can be decomposed into four constitutes: (1) mining medical causality from text; (2) medical treatment effectiveness measurement; (3) disease prediction and (4) explicable medical treatment recommendation. However, these tasks have never been systemically studied before. For my PhD thesis, I plan to formally define the problem of mining medical domain causality for diagnosis assistance and propose methods to solve this problem.\n 1. Ming these textual causalities can be very useful for discovering new knowledge and making decisions. Many studies have been done for causal extraction from the text [1, 4, 5]. However, all these studies are based on pattern or causal triggers, which greatly limit their power to extract causality and rarely consider the frequency of co-occurrence and contextual semantic features. Besides, none of them take the transitivity rules of causality leading to reject those causalities which can be easily get by simple inference. Therefore, we formally define the task of mining causality via frequency of event co-occurrence, semantic distance between event pairs and transitivity rules of causality, and present a factor graph to combine these three resources for causality mining.\n 2. Treatment effectiveness analysis is usually taken as a subset of causal analysis on observational data. For such real observational data, PSM and RCM are two dominant methods. On one hand, it is usually difficult for PSM to find the matched cases due to the sparsity of symptom. On the other hand, we should check every possible (symptom, treatment) pair by exploiting RCM, leading to make the characteristic of exploding up, especially when we want to check the causal relation between a combination of symptoms and a combination of drugs. Besides, the larger number of symptom or treatment in the combination the less number of patient case retrieved, which lead to the lack of statistical significance. Specifically, patients tend to take tens of herbs as the treatment each time in Traditional Chinese Medicine (TCM). Therefore, how to evaluate the effectiveness of herbs separately and jointly is really a big challenge. This is also a very fundamental research topic supporting many downstream applications.\n 3. Both hospitals and on-line forums have accumulated sheer amount of records, such as clinical text data and online diagnosis Q&A pairs. The availability of such data in large volume enables automatic disease prediction. There are some papers on disease prediction with electronic health record (EHR) [2], but the research on disease prediction with raw symptoms is still necessary and challenging. Therefore, we propose a general new idea of using the rich contextual information of diseases and symptoms to bridge the gap of disease candidates and symptoms, and detach it from the specific way of implementing the idea using network embedding.\n 4. Recommendation in medical domain is usually a decision-making issue, which requires the ability of explaining \"why\". The ability of explaining \"why\" are basically from two paths. Consider the recommendation suggest you eat more vegetables. You probably do not believe it if there is nothing attached. But if the recommendation gives the literally reasons why eating more vegetables is good you might like to take this suggestion. Consider another scenario, if the recommendation gives you the data of the contrast which show that people who eat more vegetables are healthier than those eat less, it is certain that you also want to take this recommendation. Based on these two intuitions, we present a recommendation model based on proofs which are either literally reasons or difference from contrast.\n This work was supported by the 973 program (No. 2014CB340503) and the NSFC (No. 61133012 and No. 61472107).", "meta": {}, "annotation_approver": null, "labels": [[637, 680, "Task"], [686, 704, "Task"], [713, 756, "Task"], [597, 631, "Task"], [884, 905, "Task"], [353, 359, "Task"], [386, 401, "Task"], [406, 423, "Task"], [497, 503, "Task"], [1557, 1573, "Task"], [1751, 1767, "Task"], [2875, 2893, "Material"], [2980, 3008, "Task"], [3035, 3053, "Task"], [3115, 3133, "Task"], [3416, 3433, "Method"], [4141, 4161, "Method"]]}
{"id": 4419, "text": "The Information Retrieval Lab at DA-IICT India participated in text summarization of the Data Challenge track of SMERP 2017. SMERP 2017 track organizers have provided the Italy earthquake tweet dataset along with the set of topics which describe important information required during any disaster related incident. The main goal of this task is to gather how well the participant’s system summarizes important tweets which are relevant to a given topic in 300 words. We have anticipated Text summarization as a clustering problem. Our approach is based on extractive summarization. We have submitted runs in both the levels with different methodologies. We have done query expansion on the topics using Wordnet. In the first level, we have calculated the cosine similarity score between tweets and expanded query. In the second level, we have used language model with Jelinek-Mercer smoothing to calculate relevance score between tweets and expanded query. We have selected tweets above a relevance threshold which are the initial candidate tweets for the summarization of each query. To ensure novelty, Jaccard Similarity is used to create a cluster for each topic. We have reported results in terms of ROGUE-1, ROGUE-2, ROGUE-L and ROGUE-SU4.", "meta": {}, "annotation_approver": null, "labels": [[63, 81, "Task"], [171, 201, "Material"], [556, 580, "Method"], [848, 862, "Method"], [1204, 1209, "Metric"], [703, 710, "Material"], [1104, 1122, "Metric"], [410, 416, "Material"], [487, 505, "Task"], [667, 682, "Method"], [755, 772, "Metric"], [787, 793, "Material"], [930, 936, "Material"], [974, 980, "Material"]]}
{"id": 4420, "text": "Sponsored search aims at retrieving the advertisements that in the one hand meet users' intent reflected in their search queries, and in the other hand attract user clicks to generate revenue. Advertisements are typically ranked based on their expected revenue that is computed as the product between their predicted probability of being clicked (i.e., namely clickability) and their advertiser provided bid. The relevance of an advertisement to a user query is implicitly captured by the predicted clickability of the advertisement, assuming that relevant advertisements are more likely to attract user clicks. However, this approach easily biases the ranking toward advertisements having rich click history. This may incorrectly lead to showing irrelevant advertisements whose clickability is not accurately predicted due to lack of click history. Another side effect consists of never giving a chance to new advertisements that may be highly relevant to be printed due to their lack of click history. To address this problem, we explicitly measure the relevance between an advertisement and a query without relying on the advertisement's click history, and present different ways of leveraging this relevance to improve user search experience without reducing search engine revenue. Specifically, we propose a machine learning approach that solely relies on text-based features to measure the relevance between an advertisement and a query. We discuss how the introduced relevance can be used in four important use cases: pre-filtering of irrelevant advertisements, recovering advertisements with little history, improving clickability prediction, and re-ranking of the advertisements on the final search result page. Offine experiments using large-scale query logs and online A/B tests demonstrate the superiority of the proposed click-oblivious relevance model and the important roles that relevance plays in sponsored search.", "meta": {}, "annotation_approver": null, "labels": [[1758, 1768, "Material"], [0, 16, "Task"], [1834, 1865, "Method"], [1313, 1329, "Method"]]}
{"id": 4421, "text": "The News Site Contrast (NSContrast) system analyzes news articles retrieved from multiple news sites based on the concept of contrast set mining. It can extract terms that characterize different topics of interest across news sites, countries, and regions. In this study, we used NSContrast to analyze Global Database of Events, Language, and Tone (GDELT) data by comparing news articles from different regions (e.g., USA, Asia, and the Middle East). We also present examples of analyses performed using this system.", "meta": {}, "annotation_approver": null, "labels": [[349, 354, "Material"], [43, 65, "Task"], [280, 290, "Method"]]}
{"id": 4422, "text": "At present, automatic discourse analysis is a relevant research topic in the field of NLP. However, discourse is one of the phenomena most difficult to process. Although discourse parsers have been already developed for several languages, this tool does not exist for Catalan. In order to implement this kind of parser, the first step is to develop a discourse segmenter. In this article we present the first discourse segmenter for texts in Catalan. This segmenter is based on Rhetorical Structure Theory (RST) for Spanish, and uses lexical and syntactic information to translate rules valid for Spanish into rules for Catalan. We have evaluated the system by using a gold standard corpus including manually segmented texts and results are promising.", "meta": {}, "annotation_approver": null, "labels": [[12, 40, "Task"], [571, 627, "Task"]]}
{"id": 4423, "text": "We present ZoneRec—a zone recommendation system for physical businesses in an urban city, which uses both public business data from Facebook and urban planning data. The system consists of machine learning algorithms that take in a business’ metadata and outputs a list of recommended zones to establish the business in. We evaluate our system using data of food businesses in Singapore and assess the contribution of different feature groups to the recommendation quality.", "meta": {}, "annotation_approver": null, "labels": [[189, 205, "Method"], [26, 47, "Task"], [145, 164, "Material"], [11, 18, "Method"], [132, 140, "Material"]]}
{"id": 4424, "text": "Intuitive user interfaces need a good understanding of the respective users and practices. One important mathematical practice consists in using mathematical expressions, which evolved as concise and precise representations of mathematical knowledge and as such guide and inform mathematical thinking. We present an exploratory eye-tracking study in which we investigate how humans perceive and understand mathematical expressions. The study confirms that math-oriented and not math-oriented users approach them differently and reveals implicit mathematical practices in the decoding and understanding processes. Further experiments are needed to confirm and study them. We discuss how these practices could be used to improve mathematical user interfaces.", "meta": {}, "annotation_approver": null, "labels": [[316, 346, "Task"], [382, 430, "Task"], [456, 506, "Method"]]}
{"id": 4425, "text": "Divergence From Randomness (DFR) ranking models assume that informative terms are distributed in a corpus differently than non-informative terms. Different statistical models (e.g. Poisson, geometric) are used to model the distribution of non-informative terms, producing different DFR models. An informative term is then detected by measuring the divergence of its distribution from the distribution of non-informative terms. However, there is little empirical evidence that the distributions of non-informative terms used in DFR actually fit current datasets. Practically this risks providing a poor separation between informative and non-informative terms, thus compromising the discriminative power of the ranking model. We present a novel extension to DFR, which first detects the best-fitting distribution of non-informative terms in a collection, and then adapts the ranking computation to this best-fitting distribution. We call this model Adaptive Distributional Ranking (ADR) because it adapts the ranking to the statistics of the specific dataset being processed each time. Experiments on TREC data show ADR to outperform DFR models (and their extensions) and be comparable in performance to a query likelihood language model (LM).", "meta": {}, "annotation_approver": null, "labels": [[0, 47, "Method"], [948, 986, "Method"], [1008, 1015, "Task"], [1100, 1104, "Material"], [282, 285, "Method"], [527, 530, "Method"], [757, 760, "Method"], [710, 717, "Task"], [874, 881, "Task"], [1115, 1118, "Method"], [1133, 1136, "Method"], [1205, 1241, "Method"]]}
{"id": 4426, "text": "Huge amount of users use Web search engines to learn new skills and knowledge everyday. Understanding how the users search to learn is essential for making search engines support these learning-related searches more effectively. Previous researches categorize these learning-related searches as exploratory searches, because they are often open-ended and multi-faceted, in which the user usually submits multiple queries iteratively to explore a large information space. In this position paper, we propose to conduct a user study to investigate whether and how users’ domain expertise affect their search processes in exploratory searches. We also set up a preliminary research framework, design the experiment protocol of the user study, and discuss about the limitations of this study and the potential implications for improving Web search engines.", "meta": {}, "annotation_approver": null, "labels": [[88, 131, "Task"], [519, 529, "Method"], [185, 210, "Task"]]}
{"id": 4427, "text": "Web archiving initiatives around the world capture ephemeral web content to preserve our collective digital memory. In this paper, we describe initial experiences in providing an exploratory search interface to web archives for humanities scholars and social scientists. We describe our initial implementation and discuss our findings in terms of desiderata for such a system. It is clear that the standard organization of a search engine results page (SERP), consisting of an ordered list of hits, is inadequate to support the needs of scholars. Shneiderman's mantra for visual information seeking (\"overview first, zoom and filter, then details-on-demand\") provides a nice organizing principle for interface design, to which we propose an addendum: \"Make everything transparent\". We elaborate on this by highlighting the importance of the temporal dimension of web pages as well as issues surrounding metadata and veracity.", "meta": {}, "annotation_approver": null, "labels": [[211, 223, "Material"], [0, 13, "Task"], [179, 207, "Method"]]}
{"id": 4428, "text": "This paper discusses an early stage project to develop a new, enhanced interface for Trinity College Dublin (TCD) Digital Collections website. We describe the current state of the portal and outline some of the unique issues observed when examining user engagement. A major factor in our development of enhanced search tools will be to leverage the entities present in the documents to establish more reliable connections between items in the collection. Not only do we expect that this will lead to better ranked search results, but we also wish to investigate how these entities may be used to encourage site visitors to explore the site beyond their initial research goal. The early stage of this project means that plans are still being finalised. Hence we speculate about other methods which may be applied to this corpus.", "meta": {}, "annotation_approver": null, "labels": [[47, 141, "Task"]]}
{"id": 4429, "text": "Efficient similarity retrieval from large-scale multimodal database is pervasive in modern search engines and social networks. To support queries across content modalities, the system should enable cross-modal correlation and computation-efficient indexing. While hashing methods have shown great potential in achieving this goal, current attempts generally fail to learn isomorphic hash codes in a seamless scheme, that is, they embed multiple modalities in a continuous isomorphic space and separately threshold embeddings into binary codes, which incurs substantial loss of retrieval accuracy. In this paper, we approach seamless multimodal hashing by proposing a novel Composite Correlation Quantization (CCQ) model. Specifically, CCQ jointly finds correlation-maximal mappings that transform different modalities into isomorphic latent space, and learns composite quantizers that convert the isomorphic latent features into compact binary codes. An optimization framework is devised to preserve both intra-modal similarity and inter-modal correlation through minimizing both reconstruction and quantization errors, which can be trained from both paired and partially paired data in linear time. A comprehensive set of experiments clearly show the superior effectiveness and efficiency of CCQ against the state of the art hashing methods for both unimodal and cross-modal retrieval.", "meta": {}, "annotation_approver": null, "labels": [[0, 67, "Task"], [673, 719, "Method"], [1351, 1385, "Task"], [577, 595, "Metric"], [624, 651, "Task"], [735, 738, "Method"], [1293, 1296, "Method"]]}
{"id": 4430, "text": "One of the most pressing problems in high schools is bullying. However, with today's online and mobile technologies, bullying is moving beyond the schoolyards via cell phones, social networks, online text, video and images, etc. As bad as fighting and bullying were before the advent of the mobile Internet, the recording and posting of hurtful content online has magnified the harmful reach of bullying, enabling 24/7 bullying. Although cyberbullying may not cause any physical damage initially, it has potentially devastating psychological effects like depression, low self-esteem, suicide ideation, and even suicide. Given the gravity of the consequences cyberbullying has on its victims and its rapid spread among middle and high school students, there is an immediate and pressing need for research to understand how cyber-bullying occurs in OSNs today, so that effective techniques can be developed to accurately detect cyberbullying. Most of the previous research on cyberbullying focused on the psychological component of cyberbullying. Only recently have researchers begun to explore computational methods for detecting cyberbullying. This survey provides insight into the problem of cyberbullying in social networks by investigating existing methods in the research literature. We will first review various computational approaches in analysis, detection and prediction of cyberbullying, harassment and aggressive behavior. Then we discuss the key technical limitations and challenges researchers are facing in this field.", "meta": {}, "annotation_approver": null, "labels": [[1119, 1142, "Task"], [1149, 1155, "Method"]]}
{"id": 4431, "text": "News article recommendation has the key problem of needing to eliminate the redundant information in a ranked list in order to provide more relevant information within a limited time and space. In this study, we tackle this problem by using image thumbnailing, which can be regarded as the summarization of news images. We propose a multimodal image thumbnailing method considering news text as well as images themselves. We evaluate this approach on a real data set based on news articles that appeared on Yahoo! JAPAN. Experimental results demonstrate the effectiveness of our proposed method.", "meta": {}, "annotation_approver": null, "labels": [[0, 27, "Task"], [241, 259, "Method"], [290, 318, "Method"], [333, 362, "Method"], [382, 391, "Material"], [403, 409, "Material"], [476, 489, "Material"]]}
{"id": 4432, "text": "Citation plays an important role in understanding the knowledge sharing among scholars. Citation sentences embed useful contents that signify the influence of cited authors on shared ideas, and express own opinion of citing authors to others' articles. The purpose of the study is to provide a new lens to analyze the topical relationship embedded in the citation sentences in an integrated manner. To this end, we extract citation sentences from full-text articles in the field of Oncology. In addition, we adopt Author-Journal-Topic (AJT) model to take both authors and journals into consideration of topic analysis. For the study, we collect the 6,360 full-text articles from PubMed Central and select the top 15 journals on Oncology. By applying AJT model, we identify what the major topics are shared among researchers in Oncology and which authors and journal lead the idea exchange in sub-disciplines of Oncology.", "meta": {}, "annotation_approver": null, "labels": [[679, 685, "Material"], [306, 363, "Task"], [514, 540, "Method"], [750, 753, "Method"]]}
{"id": 4433, "text": "In many questions in Community Question Answering sites users look for the advice or opinion of other users who might offer diverse perspectives on a topic at hand. The novel task we address is providing supportive evidence for human answers to such questions, which will potentially help the asker in choosing answers that fit her needs. We present a support retrieval model that ranks sentences from Wikipedia by their presumed support for a human answer. The model outperforms a state-of-the-art textual entailment system designed to infer factual claims from texts. An important aspect of the model is the integration of relevance oriented and support oriented features.", "meta": {}, "annotation_approver": null, "labels": [[21, 49, "Task"], [352, 375, "Method"], [402, 411, "Material"], [499, 517, "Task"]]}
{"id": 4434, "text": "As text data continues to grow quickly, it is increasingly important to develop intelligent systems to help people manage and make use of vast amounts of text data (\"big text data''). As a new family of effective general approaches to text data retrieval and analysis, probabilistic topic models---notably Probabilistic Latent Semantic Analysis (PLSA), Latent Dirichlet Allocations (LDA), and their many extensions---have been studied actively in the past decade with widespread applications. These topic models are powerful tools for extracting and analyzing latent topics contained in text data; they also provide a general and robust latent semantic representation of text data, thus improving many applications in information retrieval and text mining. Since they are general and robust, they can be applied to text data in any natural language and about any topics. This tutorial systematically reviews the major research progress in probabilistic topic models and discuss their applications in text retrieval and text mining. The tutorial provides (1) an in-depth explanation of the basic concepts, underlying principles, and the two basic topic models (i.e., PLSA and LDA) that have widespread applications, (2) an introduction to EM algorithms and Bayesian inference algorithms for topic models, (3) a hands-on exercise to allow the tutorial attendants to learn how to use the topic models implemented in the MeTA Open Source Toolkit and experiment with provided data sets, (4) a broad overview of all the major representative topic models that extend PLSA or LDA, and (5) a discussion of major challenges and future research directions.", "meta": {}, "annotation_approver": null, "labels": [[269, 295, "Method"], [637, 680, "Method"], [718, 739, "Task"], [744, 755, "Task"], [939, 965, "Method"], [1000, 1014, "Task"], [1019, 1030, "Task"], [1166, 1170, "Method"], [1175, 1178, "Method"], [1238, 1251, "Method"], [1256, 1302, "Method"], [1560, 1564, "Method"], [1568, 1571, "Method"], [235, 267, "Task"], [534, 596, "Task"], [1520, 1547, "Method"], [72, 182, "Task"], [306, 351, "Method"], [353, 387, "Method"]]}
{"id": 4435, "text": "To help scholars and businesses understand and analyse Twitter users, it is useful to have classifiers that can identify the communities that a given user belongs to, e.g. business or politics. Obtaining high quality training data is an important step towards producing an effective multi-community classifier. An efficient approach for creating such ground truth data is to extract users from existing public Twitter lists, where those lists represent different communities, e.g. a list of journalists. However, ground truth datasets obtained using such lists can be noisy, since not all users that belong to a community are good training examples for that community. In this paper, we conduct a thorough failure analysis of a ground truth dataset generated using Twitter lists. We discuss how some categories of users collected from these Twitter public lists could negatively affect the classification performance and therefore should not be used for training. Through experiments with 3 classifiers and 5 communities, we show that removing ambiguous users based on their tweets and profile can indeed result in a 10% increase in F1 performance.", "meta": {}, "annotation_approver": null, "labels": [[1133, 1135, "Metric"], [410, 417, "Material"], [841, 848, "Material"], [765, 772, "Material"], [47, 68, "Task"], [112, 165, "Task"], [283, 309, "Method"], [889, 904, "Method"], [194, 230, "Task"], [1009, 1020, "Material"], [991, 1003, "Method"], [1075, 1081, "Material"], [706, 748, "Method"]]}
{"id": 4436, "text": "Many scale-free networks exhibit a \"rich club\" structure, where high degree vertices form tightly interconnected subgraphs. In this paper, we explore the emergence of \"rich clubs\" in the context of shortest path based centrality metrics. We term these subgraphs of connected high closeness or high betweeness vertices as rich centrality clubs (RCC). Our experiments on real world and synthetic networks high- light the inter-relations between RCCs, expander graphs, and the core-periphery structure of the network. We show empirically and theoretically that RCCs exist, if the core-periphery structure of the network is such that each shell is an expander graph, and their density decreases from inner to outer shells. We further demonstrate that in addition to being an interesting topological feature, the presence of RCCs is useful in several appli- cations. The vertices in the subgraph forming the RCC are effective seed nodes for spreading information. Moreover, networks with RCCs are robust under perturbations to their structure. Given these useful properties of RCCs, we present a network modification model that can efficiently create a RCC within net- works where they are not present, while retaining other structural properties of the original network. The main contributions of our paper are: (i) we demonstrate that the formation of RCC is related to the core-periphery structure and particularly the expander like properties of each shell, (ii) we show that the RCC property can be used to find effective seed nodes for spreading information and for improving the resilience of the network under perturbation and, finally, (iii) we present a modification algorithm that can insert RCC within networks, while not affecting their other structural properties. Taken together, these contributions present one of the first comprehensive studies of the properties and applications of rich clubs for path based centralities.", "meta": {}, "annotation_approver": null, "labels": [[5, 24, "Method"], [443, 447, "Method"], [558, 562, "Method"], [820, 824, "Method"], [903, 906, "Method"], [983, 987, "Method"], [1148, 1151, "Method"], [1349, 1352, "Method"], [1479, 1482, "Method"], [1698, 1701, "Method"], [1072, 1076, "Method"], [142, 236, "Task"], [321, 348, "Method"], [384, 402, "Material"], [1091, 1117, "Method"], [1895, 1933, "Task"]]}
{"id": 4437, "text": "The proliferation of mobile devices especially smart phones brings remarkable opportunities for both industry and academia. In particular, the massive data generated from users’ usage logs provide the possibilities for stakeholders to know better about consumer behaviors with the aid of data mining. In this paper, we examine the consumer behaviors across multiple platforms based on a largescale mobile Internet dataset from a major telecom operator, which covers 9.8 million users from two regions among which 1.4 million users have visited e-commerce platforms within one week of our study. We make several interesting observations and examine users’ cultural differences from different regions. Our analysis shows among the multiple e-commerce platforms available, most mobile users are loyal to their favorable sites; people (60%) tend to make quick decisions to buy something online, which usually takes less than half an hour. Furthermore, we find that people in residential areas are much easier to perform purchases than in business districts and purchases take place during non-work time. Meanwhile, people with medium socioeconomic status like browsing and purchasing on e-commerce platforms, while people with high and low socioeconomic status are much easier to conduct purchases online. We also show the predictability of cross-platform shopping ∗1. H. Huang and H. Jin are with Service Computing Technology and System Lab / Big Data Technology and System Lab in Huazhong University of Science and Technology. 2. H. Huang and X. Fu are the corresponding authors. 3. Part of this work was done while H. Huang was with the University of Goettingen, Germany. This paper is published under the Creative Commons Attribution 4.0 International (CC BY 4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution. WWW 2018, April 23–27, 2018, Lyon, France © 2018 IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC BY 4.0 License. ACM ISBN 978-1-4503-5639-8/18/04. https://doi.org/10.1145/3178876.3186169 behaviors with extensive experiments on the basis of our observed data. Our discoveries could be a good guide for e-commerce future strategy making.", "meta": {}, "annotation_approver": null, "labels": [[319, 349, "Task"], [232, 299, "Task"], [1319, 1360, "Task"], [170, 189, "Material"], [398, 421, "Material"], [640, 675, "Task"]]}
{"id": 4438, "text": "Recently, there has been a growing number of questions about the relationship between preprints and double-blind peer-review at the SIGIR conference. These questions come from authors who wish to post papers which are under review to preprint servers such as arXiv.org, as well as reviewers who become aware of author identities through such postings, and are subsequently, unsure of how to proceed in a double-blind review process. A review of current conference publication guidelines was conducted, along with a survey of SIGIR community members to gain insight about their behaviors, practices and opinions. The paper presents results of this survey, as well as recommendations about submission and review policies for the SIGIR conference.", "meta": {}, "annotation_approver": null, "labels": [[435, 486, "Method"], [65, 124, "Task"], [404, 431, "Method"], [515, 548, "Method"]]}
{"id": 4439, "text": "The goal of search result diversification is to select a subset of documents from the candidate set to satisfy as many different subtopics as possible. In general, it is a problem of subset selection and selecting an optimal subset of documents is NP-hard. Existing methods usually formalize the problem as ranking the documents with greedy sequential document selection. At each of the ranking position the document that can provide the largest amount of additional information is selected. It is obvious that the greedy selections inevitably produce suboptimal rankings. In this paper we propose to partially alleviate the problem with a Monte Carlo tree search (MCTS) enhanced Markov decision process (MDP), referred to as M$^2$Div. In M$^2$Div, the construction of diverse ranking is formalized as an MDP process where each action corresponds to selecting a document for one ranking position. Given an MDP state which consists of the query, selected documents, and candidates, a recurrent neural network is utilized to produce the policy function for guiding the document selection and the value function for predicting the whole ranking quality. The produced raw policy and value are then strengthened with MCTS through exploring the possible rankings at the subsequent positions, achieving a better search policy for decision-making. Experimental results based on the TREC benchmarks showed that M$^2$Div can significantly outperform the state-of-the-art baselines based on greedy sequential document selection, indicating the effectiveness of the exploratory decision-making mechanism in M$^2$Div.", "meta": {}, "annotation_approver": null, "labels": [[183, 199, "Task"], [204, 244, "Task"], [334, 370, "Method"], [515, 532, "Method"], [805, 816, "Method"], [983, 1007, "Method"], [1212, 1216, "Method"], [1374, 1389, "Material"], [1480, 1516, "Method"], [753, 784, "Task"], [1554, 1591, "Method"], [906, 909, "Method"], [1305, 1338, "Task"], [12, 41, "Task"], [48, 76, "Task"], [103, 150, "Task"], [640, 670, "Method"], [680, 709, "Method"], [1054, 1085, "Task"], [1113, 1149, "Task"]]}
{"id": 4440, "text": "The proliferation of misinformation in online news and its amplification by platforms are a growing concern, leading to numerous efforts to improve the detection of and response to misinformation. Given the variety of approaches, collective agreement on the indicators that signify credible content could allow for greater collaboration and data-sharing across initiatives. In this paper, we present an initial set of indicators for article credibility defined by a diverse coalition of experts. These indicators originate from both within an article’s text as well as from external sources or article metadata. As a proof-of-concept, we present a dataset of 40 articles of varying credibility annotated with our indicators by 6 trained annotators using specialized platforms. We discuss future steps including expanding annotation, broadening the set of indicators, and considering their use by platforms and the public, towards the development of interoperable standards for content credibility. This paper is published under the Creative Commons Attribution 4.0 International (CC BY 4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution. In case of republication, reuse, etc., the following attribution should be used: “Published in WWW2018 Proceedings © 2018 International World Wide Web Conference Committee, published under Creative Commons CC BY 4.0 License.” WWW ’18 Companion, April 23–27, 2018, Lyon, France © 2018 IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC BY 4.0 License. ACM ISBN 978-1-4503-5640-4/18/04. https://doi.org/10.1145/3184558.3188731", "meta": {}, "annotation_approver": null, "labels": [[543, 557, "Material"], [574, 590, "Material"], [594, 610, "Material"], [682, 703, "Task"], [140, 195, "Task"]]}
{"id": 4441, "text": "Counting articles and citations, analyzing citations and co-authors graphs have become ways to assess researchers and institutions performance. Fairly enough, these measures are becoming targets for institutions and individual researchers thus triggering new behaviors. As a matter of fact, scientometrics and informetrics systems of all kinds have to separate the grain form the chaff. Among others, fields like Information Retrieval, network analysis and natural language processing may offers answers to deal with this kind of problems. Through several emblematic case studies (fake researcher, generated papers, paper mills), we show evidences of attempts to game indicators together with automatic ways to detect them (automatic detection of generated papers, errors detection. Pour en savoir plus : http://ceur-ws.org/Vol-2080/paper1.pdf (keynote du workshop BIR@ECIR 2018)", "meta": {}, "annotation_approver": null, "labels": [[413, 434, "Task"], [436, 452, "Task"], [457, 484, "Task"], [724, 763, "Task"], [765, 781, "Task"], [0, 31, "Task"], [33, 74, "Task"], [95, 142, "Task"], [581, 596, "Material"], [598, 614, "Material"], [616, 627, "Material"]]}
{"id": 4442, "text": "Large-scale search engines utilize inverted indexes which store ordered lists of document identifies (docIDs) relevant to query terms, which can be queried thousands of times per second. In order to reduce storage requirements, we propose a dictionary-based compression approach for the recently proposed bitwise data-structure BitFunnel, which makes use of a Bloom filter. Compression is achieved through storing frequently occurring blocks in a dictionary. Infrequently occurring blocks (those which are not represented in the dictionary) are instead referenced using similar blocks that are in the dictionary, introducing additional false positive errors. We further introduce a docID reordering strategy to improve compression. Experimental results indicate an improvement in compression by 27% to 30%, at the expense of increasing the query processing time by 16% to 48% and increasing the false positive rate by around 7.6 to 10.7 percentage points.", "meta": {}, "annotation_approver": null, "labels": [[241, 278, "Method"], [199, 226, "Task"], [682, 707, "Method"], [840, 861, "Metric"], [895, 914, "Metric"], [305, 337, "Method"], [711, 730, "Task"], [780, 791, "Metric"]]}
{"id": 4443, "text": "Rich high-quality annotated data is critical for semantic segmentation learning, yet acquiring dense and pixel-wise ground-truth is both labor- and time-consuming. Coarse annotations (e.g., scribbles, coarse polygons) offer an economical alternative, with which training phase could hardly generate satisfactory performance unfortunately. In order to generate high-quality annotated data with a low time cost for accurate segmentation, in this paper, we propose a novel annotation enrichment strategy, which expands existing coarse annotations of training data to a finer scale. Extensive experiments on the Cityscapes and PASCAL VOC 2012 benchmarks have shown that the neural networks trained with the enriched annotations from our framework yield a significant improvement over that trained with the original coarse labels. It is highly competitive to the performance obtained by using human annotated dense annotations. The proposed method also outperforms among other state-of-the-art weakly-supervised segmentation methods.", "meta": {}, "annotation_approver": null, "labels": [[608, 618, "Material"], [623, 649, "Material"], [989, 1027, "Method"], [669, 685, "Method"], [351, 434, "Task"], [49, 79, "Task"], [470, 500, "Method"]]}
{"id": 4444, "text": "Content-Based Image Retrieval in large archives through the use of visual features has become a very attractive research topic in recent years. The cause of this strong impulse in this area of research is certainly to be attributed to the use of Convolutional Neural Network (CNN) activations as features and their outstanding performance. However, practically all the available image retrieval systems are implemented in main memory, limiting their applicability and preventing their usage in big-data applications. In this paper, we propose to transform CNN features into textual representations and index them with the well-known full-text retrieval engine Elasticsearch. We validate our approach on a novel CNN feature, namely Regional Maximum Activations of Convolutions. A preliminary experimental evaluation, conducted on the standard benchmark INRIA Holidays, shows the effectiveness and efficiency of the proposed approach and how it compares to state-of-the-art main-memory indexes.", "meta": {}, "annotation_approver": null, "labels": [[0, 29, "Task"], [379, 402, "Method"], [494, 515, "Method"], [556, 559, "Method"], [711, 714, "Method"], [731, 775, "Method"], [842, 866, "Material"], [246, 280, "Method"]]}
{"id": 4445, "text": "The process of extracting, structuring, and organizing knowledge requires processing large and originally heterogeneous data sources. Offering existing data as Linked Data increases its shareability, extensibility, and reusability. However, using Linking Data as a means to represent knowledge can be easier said than done. In this tutorial, we elaborate on how to semantically annotate data, and generate and publish Linked Data. We introduce [R2]RML languages to generate Linked Data. We also show how to easily publish Linked Data on the Web as Triple Pattern Fragments. As a result, participants, independently of their knowledge background, can model, annotate and publish Linked Data on their own.", "meta": {}, "annotation_approver": null, "labels": [[448, 461, "Method"], [160, 171, "Material"], [522, 533, "Material"], [678, 689, "Material"], [247, 259, "Material"], [15, 64, "Task"], [365, 391, "Task"], [397, 429, "Task"], [465, 485, "Task"]]}
{"id": 4446, "text": "The World Wide Web is a massive network of interlinked documents. One of the reasons the World Wide Web is so successful is the fact that most content is available free of any charge. Inspired by the success of the World Wide Web, the Web of Data applies the same strategy of interlinking to data. To this point, most of data in the Web of Data is also free of charge. The fact that the data is freely available raises the question of financing these services, however. As we will discuss in this paper, advertisement and donations cannot easily be applied to this new setting. To create incentives to subsidize data providers, we propose that sponsors should pay the providers to promote sponsored data. In return, sponsored data will be privileged over non-sponsored data. Since it is not possible to enforce a certain ordering on the data the user will receive, we propose to split up the data into different batches and deliver these batches with different delays. In this way, we can privilege sponsored data without withholding any non-sponsored data from the user. In this paper, we introduce a new concept of a delayed-answer auction, where sponsors can pay to prioritize their data. We introduce a new model which captures the particular situation when a user access data in the Web of Data. We show how the weighted Vickrey-Clarke-Groves auction mechanism can be applied to our scenario and we discuss how certain parameters can influence the nature of our auction. With our new concept, we build a first step to a free yet financial sustainable Web of Data. Posted at the Zurich Open Repository and Archive, University of Zurich ZORA URL: https://doi.org/10.5167/uzh-149689 Accepted Version Originally published at: Grubenmann, Tobias; Bernstein, Abraham; Moor, Dmitrii; Seuken, Sven (2018). Financing the Web of Data with Delayed-Answer Auctions. In: WWW 2018: The 2018 Web Conference, Lyon, 23 April 2018 27 April 2018. Financing the Web of Data with Delayed-Answer Auctions Tobias Grubenmann University of Zurich Department of Informatics Zurich, Switzerland grubenmann@ifi.uzh.ch Abraham Bernstein University of Zurich Department of Informatics Zurich, Switzerland bernstein@ifi.uzh.ch Dmitry Moor University of Zurich Department of Informatics Zurich, Switzerland dmoor@ifi.uzh.ch Sven Seuken University of Zurich Department of Informatics Zurich, Switzerland seuken@ifi.uzh.ch", "meta": {}, "annotation_approver": null, "labels": [[4, 18, "Material"], [89, 103, "Material"], [215, 229, "Material"], [235, 246, "Material"], [276, 296, "Method"], [333, 344, "Material"], [578, 626, "Task"], [1119, 1141, "Method"], [1288, 1299, "Material"], [1326, 1365, "Method"], [1556, 1567, "Material"]]}
{"id": 4447, "text": "The integration of diverse structured and unstructured information sources into a unified, domain-specific knowledge base is an important task in many areas. A well-maintained knowledge base enables data analysis in complex scenarios, such as risk analysis in the financial sector or investigating large data leaks, such as the Paradise or Panama papers. Both the creation of such knowledge bases, as well as their continuous maintenance and curation involves many complex tasks and considerable manual effort.\n With CurEx, we present a modular system that allows structured and unstructured data sources to be integrated into a domain-specific knowledge base. In particular, we (i) enable the incremental improvement of each individual integration component; (ii) enable the selective generation of multiple knowledge graphs from the information contained in the knowledge base; and (iii) provide two distinct user interfaces tailored to the needs of data engineers and end-users respectively. The former has curation capabilities and controls the integration process, whereas the latter focuses on the exploration of the generated knowledge graph.", "meta": {}, "annotation_approver": null, "labels": [[4, 121, "Task"], [683, 758, "Task"], [765, 878, "Task"], [890, 993, "Task"], [381, 396, "Material"], [176, 190, "Material"], [765, 825, "Task"]]}
{"id": 4448, "text": "Citation contexts of an article refer to sentences or paragraphs that cite that article. Citation contexts are especially useful for recommendation and summarization tasks. However, few studies have recognized the diversity of these citation contexts, thus leading to redundant recommendation lists and abstract [3]. To address this gap, we compared several strategies that can recommend a set of diverse citation contexts by re-ranking extracted citation contexts. Diversification was achieved by combining one of two semantic distance algorithms with one of two re-ranking algorithms. Experimenting with CiteSeerX dataset, our program produced a diverse list of 10 citation contexts that could be recommended to users. We evaluated the experiment results based on a user case study of 15 articles. The case study revealed that a diversity strategy that combined the \"ESA\" and \"MMR\" led to a better reading experience for participants compared to other diversity strategies. Our study provides insights to develop better automatic academic recommendation and summarization systems.", "meta": {}, "annotation_approver": null, "labels": [[606, 623, "Material"], [133, 165, "Task"], [341, 422, "Task"], [426, 464, "Method"], [1041, 1073, "Task"], [0, 31, "Task"], [89, 106, "Task"], [519, 547, "Method"], [564, 585, "Method"]]}
{"id": 4449, "text": "Multimedia items account for an important part of Linked Open Data (LOD), but currently most of the semantic relations between multimedia items and semantic knowledge base (KB) are based on manual semantic annotation. With the popularity of multimedia hosting websites, plentiful tagging information makes it possible to automatically generate semantic relations between multimedia items and KB. In this paper, we propose a mechanism for linking multimedia items to KB with user-generated tags, while taking into account topical semantic similarity between tags. Experimental results show the effect of our work on this task.", "meta": {}, "annotation_approver": null, "labels": [[321, 394, "Task"], [190, 216, "Method"], [438, 468, "Task"], [521, 548, "Method"]]}
{"id": 4450, "text": "Players are ranked in various sports to show their importance over other players. Existing methods only consider intra-type links (e.g., player to player and team to team), but ignore inter-type links (e.g., one type of player to another type of player, such as batsman to bowler and player to team) based on cognitive aspects. They also ignore the spatiality of the players. There is a strong relationship among players and their teams, which can be represented as a network consisting of multi-type interrelated objects. In this paper, we propose a players’ ranking method, called Region-wise Players Link Fusion (RPLF) which is applied to the sport of cricket. RPLF considers players’ region-wise intra-type and inter-type relationbased features to rank the players. Considering multi-type interrelated objects is based on the intuition that a batsman scoring high against top bowlers of a strong team or a bowler taking wickets against top batsmen of a strong team is considered as a good player. The experimental results show that RPLF provides promising insights of players’ rankings. RLFP is a generic method and can be applied to different sports for ranking players. A. Daud, A. Hussain, R. A. Abbasi, N. R. Aljohani, T. Amjad, H. Dawood 2018. Region-wise Ranking of Sports Players based on Link Fusion for WWW 2018 Proceedings and Companion Submissions. In The 2018 Web Conference Companion (WWW 2018), April 23-27, 2018, Lyon, France, ACM, New York, NY, XX pages. DOI: https://doi.org/10.1145/3184558.3186335", "meta": {}, "annotation_approver": null, "labels": [[551, 567, "Task"], [664, 669, "Method"], [1036, 1041, "Method"], [1072, 1089, "Task"], [1091, 1096, "Method"], [1159, 1174, "Task"], [0, 80, "Task"], [583, 621, "Method"]]}
{"id": 4451, "text": "We present, GEM, the first heterogeneous graph neural network approach for detecting malicious accounts at Alipay, one of the world's leading mobile cashless payment platform. Our approach, inspired from a connected subgraph approach, adaptively learns discriminative embeddings from heterogeneous account-device graphs based on two fundamental weaknesses of attackers, i.e. device aggregation and activity aggregation. For the heterogeneous graph consists of various types of nodes, we propose an attention mechanism to learn the importance of different types of nodes, while using the sum operator for modeling the aggregation patterns of nodes in each type. Experiments show that our approaches consistently perform promising results compared with competitive methods over time.", "meta": {}, "annotation_approver": null, "labels": [[12, 15, "Method"], [75, 103, "Task"], [107, 113, "Material"], [234, 319, "Task"], [375, 418, "Method"], [498, 517, "Method"], [206, 224, "Method"], [40, 61, "Method"]]}
{"id": 4452, "text": "Data providers have a profound contribution to many fields such as finance, economy, and academia by serving people with both web-based and API-based query service of specialized data. Among the data users, there are data resellers who abuse the query APIs to retrieve and resell the data to make a profit, which harms the data provider's interests and causes copyright infringement. In this work, we define the \"anti-data-reselling\" problem and propose a new systematic method that combines feature engineering and machine learning models to provide a solution. We apply our method to a real query log of over 9,000 users with limited labels provided by a large financial data provider and get reasonable results, insightful observations, and real deployments.", "meta": {}, "annotation_approver": null, "labels": [[260, 305, "Task"], [413, 432, "Task"], [492, 511, "Method"], [588, 602, "Material"], [663, 687, "Material"], [515, 539, "Method"]]}
{"id": 4453, "text": "User action modeling and prediction has long been a topic of importance to recommender systems and user profiling. The quality of the model or accuracy of prediction plays a vital role in related applications like recommendation, advertisement displaying, searching, etc. For large scale systems with a massive number of users, beside the pure prediction performance, there are other practical factors like training and prediction latency, memory overhead, that must be optimized to ensure smooth operation of the system. We propose a fast linear computational framework to handle a vast number of second order crossed features with dimensionality reduction. By leveraging the training and serving system architecture, we shift heavy calculation burden from online serving to offline preprocessing, at the cost of a reasonable amount of memory overhead. The experiments on a 15-day data trace from Tencent MyApp shows that our proposed framework can achieve comparable prediction performance to much complex models like the field-aware factorization machine (FFM) while being served in 2 ms with a reasonable amount of memory overhead.", "meta": {}, "annotation_approver": null, "labels": [[837, 852, "Metric"], [1119, 1134, "Metric"], [440, 455, "Metric"], [143, 151, "Metric"], [633, 657, "Method"], [898, 911, "Material"], [534, 570, "Method"], [0, 35, "Task"], [758, 772, "Method"], [776, 797, "Method"], [407, 430, "Task"], [1024, 1063, "Method"]]}
{"id": 4454, "text": "Citations play a crucial role in the scientific discourse, in information retrieval, and in bibliometrics. Many initiatives are currently promoting the idea of having free and open citation data. Creation of citation data, however, is not part of the cataloging workflow in libraries nowadays. \n In this paper, we present our project Linked Open Citation Database, in which we design distributed processes and a system infrastructure based on linked data technology. The goal is to show that efficiently cataloging citations in libraries using a semi-automatic approach is possible. We specifically describe the current state of the workflow and its implementation. We show that we could significantly improve the automatic reference extraction that is crucial for the subsequent data curation. We further give insights on the curation and linking process and provide evaluation results that not only direct the further development of the project, but also allow us to discuss its overall feasibility.", "meta": {}, "annotation_approver": null, "labels": [[504, 537, "Task"], [714, 744, "Task"], [827, 855, "Task"], [62, 83, "Task"], [92, 105, "Task"], [196, 221, "Task"], [443, 465, "Method"], [546, 569, "Method"]]}
{"id": 4455, "text": "The collaboration of financial institutes against fraudsters is a promising path for reducing resource investments and increasing coverage. Yet, such collaboration is held back by two somewhat conflicting challenges: effective knowledge sharing and limiting leakage of private information. While the censorship of private information is likely to reduce knowledge sharing effectiveness, the generalization of private information to a desired degree can potentially allow, on one hand, to limit the leakage, and on the other hand, to reveal some properties of the private information that can be beneficial for sharing.\n In this demo we present a system that allows knowledge sharing via effective adaptation of fraud detection rules while preserving privacy. The system uses taxonomies to generalize concrete values appearing in fraud detection rules to higher level concepts which conform to some privacy/utility requirements set by the owner. Our demonstration will engage the CIKM'18 audience by showing that private information can be abstracted to enforce privacy while maintaining its usage by (partially) trusted allies.", "meta": {}, "annotation_approver": null, "labels": [[216, 244, "Task"], [249, 288, "Task"], [665, 682, "Task"], [829, 844, "Task"], [711, 726, "Task"]]}
{"id": 4456, "text": "Efficient implementations of range and nearest neighbor queries are critical in many large multimedia applications. Locality Sensitive Hashing (LSH) is a popular technique for performing approximate searches in high-dimensional multimedia, such as image or sensory data. Often times, these multimedia data are represented as a collection of important spatio-temporal features which are extracted by using localized feature extraction algorithms. When a user wants to search for a given entity (object, event, or observation), individual similarity search queries, which collectively form a set query, need to be performed on the features that represent the particular search entity. Existing LSH techniques require that users provide an accuracy guarantee for each query in the set query, instead of an overall guarantee for the entire set query, which can lead to misses or wasteful work. We propose a novel index structure, Point Set LSH (PSLSH), which is able to execute a similarity search for a given set of search points in the high-dimensional space with a user-provided guarantee for the entire set query. Experimental evaluation shows significant gains in efficiency and accuracy trade-offs for executing set queries in high-dimensional spaces.", "meta": {}, "annotation_approver": null, "labels": [[1180, 1188, "Metric"], [926, 939, "Method"], [940, 947, "Method"], [405, 444, "Method"], [692, 695, "Method"], [737, 745, "Metric"], [941, 946, "Method"], [116, 148, "Method"]]}
{"id": 4457, "text": "The goal of the BIIRRR 2018 workshop [2] was to serve as a starting point for a communitydriven effort to design and implement a platform for the collection, organization, maintenance, and sharing of resources for interactive information retrieval (IIR) experimentation. As in all scientific endeavors, progress in IIR research is contingent on the ability to build on previous ideas, approaches, and resources. Current trends towards open science and funding mandates to preserve and share research data lend support and even urgency to the notion of establishing a shared disciplinary repository of research tools and data for IIR. The need for an IIR (evaluation) framework was further highlighted by Pia Borlund in her 2016 CHIIR keynote [3]. Components from IIR experiments that could be valuable to archive for re-use include: the systems or platforms used for experimentation, the content or resources of the experimental platform (data collections), the search tasks or work situation, the experimental context and other important aspects of the test design, experimental protocols, questionnaire designs, etc., the gathered user and system interaction data, the tools used for analysis as well as the results and measures. We believe there to be a number of barriers to reproducibility and re-use of resources in IIR research: the fragmentary nature of how the community’s resources are organized, the lack of awareness of their existence, insufficient documentation and organization of the resources, the nature of the typical research publication cycle, and the effort required to make such resources available. The TREC initiative highlights the value of such a repository, as it provides a single access", "meta": {}, "annotation_approver": null, "labels": [[315, 318, "Task"], [629, 632, "Task"], [763, 766, "Task"], [1322, 1325, "Task"], [214, 253, "Task"], [650, 653, "Task"]]}
{"id": 4458, "text": "Content polluters, or bots that hijack a conversation for political or advertising purposes are a known problem for event prediction, election forecasting and when distinguishing real news from fake news in social media data. Identifying this type of bot is particularly challenging, with state-of-the-art methods utilising large volumes of network data as features for machine learning models. Such datasets are generally not readily available in typical applications which stream social media data for real-time event prediction. In this work we develop a methodology to detect content polluters in social media datasets that are streamed in real-time. Applying our method to the problem of civil unrest event prediction in Australia, we identify content polluters from individual tweets, without collecting social network or historical data from individual accounts. We identify some peculiar characteristics of these bots in our dataset and propose metrics for identification of such accounts. We then pose some research questions around this type of bot detection, including: how good Twitter is at detecting content polluters and how well state-of-the-art methods perform in detecting bots in our dataset.", "meta": {}, "annotation_approver": null, "labels": [[370, 386, "Method"], [504, 530, "Task"], [693, 722, "Task"], [783, 789, "Material"], [570, 653, "Task"], [1090, 1097, "Material"], [1103, 1132, "Task"]]}
{"id": 4459, "text": "Recent studies show that table scans are increasingly more common than using secondary indices. Given that the optimizer may choose table scans when the selectivity is as low as 0.5% with large data, it is important to make initial query results faster for interactive data explorations. We formulate it as a query result timeliness problem, and propose two complementary approaches. The first approach builds lightweight statistics and judiciously determines an access order to data blocks for a given query. The second approach performs adaptive microscopic tuple reordering online without relying on pre-built statistics. Our systematic experimental evaluation further verifies the efficiency and efficacy of our approaches.", "meta": {}, "annotation_approver": null, "labels": [[309, 340, "Task"], [539, 583, "Method"], [700, 708, "Metric"], [219, 286, "Task"]]}
{"id": 4460, "text": "With the exponential growth in digital research data, libraries are beginning to find opportunities to assist researchers with planning, maintaining, sharing, and accessing data through research data services. Using a content analysis with the lens of information architecture, this study sought to better understand how these services are organized in North American academic library websites and to what extent the research data lifecycle is supported within research data services. 50 academic library websites were studied and results yielded three provisions that make up research data services: Information Access, Technical Support, and Personalized Consultation. The data lifecycle was found to be strongly supported in research data services for planning, data curation, and data access stages.", "meta": {}, "annotation_approver": null, "labels": [[296, 393, "Task"], [485, 513, "Material"], [400, 483, "Task"], [99, 208, "Task"]]}
{"id": 4461, "text": "Dealing with large tabular datasets often requires extensive preprocessing. This preprocessing happens only once, so that loading and indexing the data in a database or triple store may be an overkill. In this paper, we present an approach that allows preprocessing large tabular data in Datalog – without indexing the data. The Datalog query is translated to Unix Bash and can be executed in a shell. Our experiments show that, for the use case of data preprocessing, our approach is competitive with state-of-the-art systems in terms of scalability and speed, while at the same time requiring only a Bash shell on a Unix system.", "meta": {}, "annotation_approver": null, "labels": [[555, 560, "Metric"], [329, 343, "Material"], [360, 369, "Material"], [252, 284, "Task"], [61, 74, "Task"], [81, 94, "Task"], [288, 295, "Material"]]}
{"id": 4462, "text": "Understanding the search tasks and search behavior of users is necessary for optimizing search engine results. While much work has been done on understanding the users in Web search, little knowledge is available about the search tasks and behavior of users in the E-Commerce (E-Com) search applications. In this paper, we share the first empirical study of the queries and search behavior of users in E-Com search by analyzing search log from a major E-Com search engine. The analysis results show that E-Com queries can be categorized into five categories, each with distinctive search behaviors: (1) Shallow Exploration Queries are short vague queries that a user may use initially in exploring the product space. (2) Targeted Purchase Queries are queries used by users to purchase items that they are generally familiar with, thus without much decision making. (3) Major-Item Shopping Queries are used by users to shop for a major item which is often relatively expensive and thus requires some serious exploration, but typically in a limited scope of choices. (4) Minor-Item Shopping Queries are used by users to shop for minor items that are generally not very expensive, but still require some exploration of choices. (5) Hard-Choice Shopping Queries are used by users who want to deeply explore all the candidate products before finalizing the choice often appropriate when multiple products must be carefully compared with each other. These five categories form a taxonomy for E-Com queries and can shed light on how we may develop customized search technologies for each type of search queries to improve search engine utility.", "meta": {}, "annotation_approver": null, "labels": [[77, 109, "Task"], [428, 438, "Material"], [1473, 1499, "Method"], [1589, 1636, "Task"]]}
{"id": 4463, "text": "With the increasing of multi-modal data on the internet, cross-modal retrieval has received a lot of attention in recent years. It aims to use one type of data as query and retrieve results of another type. For different modality data, how to reduce their heterogeneous property and preserve their local relationship are two main challenges. In this paper, we present a novel joint dictionary learning and semantic constrained latent subspace learning method for cross-modal retrieval~(JDSLC) to deal with above two issues. In this unified framework, samples from different modalities are encoded by their corresponding dictionaries to reduce the semantic gap. In the meantime, we learn modality-specific projection matrices to map the sparse coefficients into the shared latent subspace. Meanwhile, we impose a novel cross-modal similarity constraint to make the representations of samples that belong to same class but from different modalities as close as possible in the latent subspace. An efficient algorithm is proposed to jointly optimize the proposed model and learn the optimal dictionary, coefficients and projection matrix for each modality. Extensive experimental results on multiple benchmark datasets show that our method outperforms the state-of-the-art approaches.", "meta": {}, "annotation_approver": null, "labels": [[57, 78, "Task"], [463, 484, "Task"], [486, 491, "Method"], [240, 317, "Task"], [818, 840, "Method"], [376, 451, "Method"], [1197, 1215, "Material"]]}
{"id": 4464, "text": "Gradient-based learning methods such as stochastic gradient descent are widely used in matrix approximation-based collaborative filtering algorithms to train recommendation models based on observed user-item ratings. One major difficulty in existing gradient-based learning methods is determining proper learning rates, since model convergence would be inaccurate or very slow if the learning rate is too large or too small, respectively. This paper proposes AdaError, an adaptive learning rate method for matrix approximation-based collaborative filtering. AdaError eliminates the need of manually tuning the learning rates by adaptively adjusting the learning rates based on the noisiness level of user-item ratings, using smaller learning rates for noisy ratings so as to reduce their impact on the learned models. Our theoretical and empirical analysis shows that AdaError can improve the generalization performance of the learned models. Experimental studies on the MovieLens and Netflix datasets also demonstrate that AdaError outperforms state-of-the-art adaptive learning rate methods in matrix approximation-based collaborative filtering. Furthermore, by applying AdaError to the standard matrix approximation method, we can achieve statistically significant improvements over state-of-the-art collaborative filtering methods in both rating prediction accuracy and top-N recommendation accuracy.", "meta": {}, "annotation_approver": null, "labels": [[40, 67, "Method"], [87, 148, "Method"], [250, 281, "Method"], [285, 318, "Task"], [459, 467, "Method"], [506, 556, "Task"], [868, 876, "Method"], [971, 980, "Material"], [985, 992, "Material"], [1024, 1032, "Method"], [1096, 1146, "Task"], [1173, 1181, "Method"], [1198, 1225, "Method"], [558, 567, "Method"], [1395, 1403, "Metric"], [0, 23, "Method"], [472, 501, "Method"], [1361, 1369, "Metric"]]}
{"id": 4465, "text": "Relative Path Overwrite (RPO) is a recent technique to inject style directives into sites even when no style sink or markup injection vulnerability is present. It exploits differences in how browsers and web servers interpret relative paths (i.e., path confusion) to make a HTML page reference itself as a stylesheet; a simple text injection vulnerability alongwith browsers’ leniency in parsing CSS resources results in an attacker’s ability to inject style directives that will be interpreted by the browser. Even though style injection may appear less serious a threat than script injection, it has been shown that it enables a range of attacks, including secret exfiltration. In this paper, we present the first large-scale study of the Web to measure the prevalence and significance of style injection using RPO. Our work shows that around 9% of the sites in the Alexa Top 10,000 contain at least one vulnerable page, out of which more than one third can be exploited. We analyze in detail various impediments to successful exploitation, and make recommendations for remediation. In contrast to script injection, relatively simple countermeasures exist to mitigate style injection. However, there appears to be little awareness of this attack vector as evidenced by a range of popular Content Management Systems (CMSes) that we found to be exploitable.", "meta": {}, "annotation_approver": null, "labels": [[55, 89, "Task"], [523, 538, "Task"], [813, 816, "Method"], [0, 29, "Method"], [748, 806, "Task"], [868, 873, "Material"]]}
{"id": 4466, "text": "We introduce Texygen, a benchmarking platform to support research on open-domain text generation models. Texygen has not only implemented a majority of text generation models, but also covered a set of metrics that evaluate the diversity, the quality and the consistency of the generated texts. The Texygen platform could help standardize the research on text generation and improve the reproductivity and reliability of future research work in text generation.", "meta": {}, "annotation_approver": null, "labels": [[46, 103, "Task"], [355, 370, "Task"], [444, 460, "Task"]]}
{"id": 4467, "text": "Creating, populating, updating and maintaining a knowledge resource requires intense human effort. Automatic Information Extraction techniques play a crucial role for this task, but many ongoing production systems still require a large component of human annotation. In this work we investigate how to better take advantage of human annotations by performing active learning on multiple IE tasks concurrently, specifically Relation Extraction and Named Entity Recognition. Our proposed approach adaptively requests annotations for one task or the other depending on the current overall performance of the combined extraction. We show promising results on a small use case extracting relations expressing Adverse Drug Reactions from unannotated sentences.", "meta": {}, "annotation_approver": null, "labels": [[99, 143, "Method"], [0, 67, "Task"], [359, 374, "Method"], [423, 442, "Task"], [447, 471, "Task"], [704, 753, "Material"]]}
{"id": 4468, "text": "Nowadays, it is common for one natural person to join multiple social networks to enjoy different services. Linking identical users across different social networks, also known as the User Identity Linkage (UIL), is an important problem of great research challenges and practical value. Most existing UIL models are supervised or semi-supervised and a considerable number of manually matched user identity pairs are required, which is costly in terms of labor and time. In addition, existing methods generally rely heavily on some discriminative common user attributes, and thus are hard to be generalized. Motivated by the isomorphism across social networks, in this paper we consider all the users in a social network as a whole and perform UIL from the user space distribution level. The insight is that we convert the unsupervised UIL problem to the learning of a projection function to minimize the distance between the distributions of user identities in two social networks. We propose to use the earth mover's distance (EMD) as the measure of distribution closeness, and propose two models UUIL$_gan $ and UUIL$_omt $ to efficiently learn the distribution projection function. Empirically, we evaluate the proposed models over multiple social network datasets, and the results demonstrate that our proposal significantly outperforms state-of-the-art methods.", "meta": {}, "annotation_approver": null, "labels": [[108, 164, "Task"], [743, 746, "Task"], [1244, 1267, "Material"], [694, 720, "Material"], [184, 211, "Task"], [301, 304, "Task"], [835, 838, "Task"], [1004, 1032, "Metric"]]}
{"id": 4469, "text": "A valuable step towards news veracity assessment is to understand stance from different information sources, and the process is known as the stance detection. Specifically, the stance detection is to detect four kinds of stances (“agree”, “disagree”, “discuss” and “unrelated”) of the news towards a claim. Existing methods tried to tackle the stance detection problem by classification-based algorithms. However, classification-based algorithms make a strong assumption that there is clear distinction between any two stances, which may not be held in the context of stance detection. Accordingly, we frame the detection problem as a ranking problem and propose a rankingbased method to improve detection performance. Compared with the classification-based methods, the ranking-based method compare the true stance and false stances and maximize the difference between them. Experimental results demonstrate the effectiveness of our proposed method.", "meta": {}, "annotation_approver": null, "labels": [[24, 48, "Task"], [55, 107, "Task"], [141, 157, "Task"], [177, 193, "Task"], [344, 368, "Task"], [372, 403, "Method"], [414, 445, "Method"], [568, 584, "Task"], [737, 765, "Method"], [612, 629, "Task"], [635, 650, "Task"], [770, 791, "Method"]]}
{"id": 4470, "text": "Knowledge bases (KBs) such as DBpedia, Wikidata, and YAGO contain a huge number of entities and facts. Several recent works induce rules or calculate statistics on these KBs. Most of these methods are based on the assumption that the data is a representative sample of the studied universe. Unfortunately, KBs are biased because they are built from crowdsourcing and opportunistic agglomeration of available databases. This paper aims at approximating the representativeness of a relation within a knowledge base. For this, we use the generalized Benford’s law, which indicates the distribution expected by the facts of a relation. We then compute the minimum number of facts that have to be added in order to make the KB representative of the real world. Experiments show that our unsupervised method applies to a large number of relations. For numerical relations where ground truths exist, the estimated representativeness proves to be a reliable indicator.", "meta": {}, "annotation_approver": null, "labels": [[438, 512, "Task"], [719, 721, "Material"], [169, 173, "Material"], [306, 309, "Material"], [0, 21, "Material"]]}
{"id": 4471, "text": "Past decade witnessed an explosive growth in the amount of unstructured data, especially in the public domain, mainly due to Web 2.0 and social media. This led to the creation of applications, called information extractors, that extract structured information from unstruc- tured data. The extracted information is stored in a Knowledge Base (KB). KB stores facts about entities like name, type and other attributes. My PhD thesis entitled 'Named Entity Extraction for Knowledgebase Enhancement' deals with information extraction on named entities with the purpose of enhancing a KB. The enhanced KB is in turn used by the information extraction task to refine the extraction process. Thus, KB provides structure and guidance to the extraction task, and gets enhanced by the results of the extraction task. Here we see that the tasks of entity extraction and KB enhancement are mutually dependent and mutually beneficial. Hence in my research I propose methods to enhance both the tasks, in an effort to build a strong and sound named entity extraction system. Named Entity Extraction, also known as Entity Linking (EL) in scientific literature, is the task of determining the identity of entities mentioned in text. EL helps automatic extraction of structured information about entities from unstructured data, which is stored in the KB. EL consists of Mention Detection and Entity Disambiguation. In my research, I propose methods to enhance mention detection, entity disambiguation and KB enhancement.", "meta": {}, "annotation_approver": null, "labels": [[229, 284, "Task"], [348, 350, "Material"], [507, 547, "Task"], [568, 582, "Task"], [597, 599, "Material"], [691, 693, "Material"], [733, 748, "Task"], [790, 805, "Task"], [859, 873, "Task"], [1061, 1084, "Task"], [1161, 1215, "Task"], [1217, 1219, "Task"], [1226, 1310, "Task"], [1335, 1337, "Material"], [1339, 1341, "Task"], [1354, 1371, "Task"], [1376, 1397, "Task"], [1489, 1503, "Task"], [1463, 1484, "Task"], [623, 646, "Task"], [327, 346, "Material"], [837, 854, "Task"], [1100, 1119, "Task"], [1444, 1461, "Task"]]}
{"id": 4472, "text": "Rapid increase of misinformation online has emerged as one of the biggest challenges in this post-truth era. This has given rise to many fact-checking websites that manually assess doubtful claims. However, the speed and scale at which misinformation spreads in online media inherently limits manual verification. Hence, the problem of automatic credibility assessment has attracted great attention. In this work, we present CredEye, a system for automatic credibility assessment. It takes a natural language claim as input from the user and automatically analyzes its credibility by considering relevant articles from the Web. Our system captures joint interaction between language style of articles, their stance towards a claim and the trustworthiness of the sources. In addition, extraction of supporting evidence in the form of enriched snippets makes the verdicts of CredEye transparent and interpretable.", "meta": {}, "annotation_approver": null, "labels": [[137, 150, "Task"], [174, 196, "Task"], [623, 626, "Material"], [336, 368, "Task"], [447, 479, "Task"]]}
{"id": 4473, "text": "Motivated by the popularity of online ride and delivery services, we study natural variants of classical multi-vehicle minimum latency problems where the objective is to route a set of vehicles located at depots to serve requests located on a metric space so as to minimize the total latency. In this paper, we consider point-to-point requests that come with source-destination pairs and release-time constraints that restrict when each request can be served. The pointto-point requests and release-time constraints model taxi rides and deliveries. For all the variants considered, we show constant-factor approximation algorithms based on a linear programming framework. To the best of our knowledge, these are the first set of results for the aforementioned variants of the minimum latency problems. Furthermore, we provide an empirical study of heuristics based on our theoretical algorithms on a real data set of taxi rides.", "meta": {}, "annotation_approver": null, "labels": [[590, 630, "Method"], [642, 670, "Method"], [848, 858, "Method"], [900, 927, "Material"], [75, 143, "Task"], [776, 800, "Task"], [265, 291, "Task"]]}
{"id": 4474, "text": "Recently, deep neural network models have achieved promising results in image captioning task. Yet, “vanilla” sentences, only describing shallow appearances (e.g., types, colors), generated by current works are not satisfied netizen style resulting in lacking engagements, contexts, and user intentions. To tackle this problem, we propose Netizen Style Commenting (NSC), to automatically generate characteristic comments to a user-contributed fashion photo. We are devoted to modulating the comments in a vivid “netizen” style which reflects the culture in a designated social community and hopes to facilitate more engagement with users. In this work, we design a novel framework that consists of three major components: (1) We construct a large-scale clothing dataset named NetiLook, which contains 300K posts (photos) with 5M comments to discover netizen-style comments. (2) We propose three unique measures to estimate the diversity of comments. (3) We bring diversity by marrying topic models with neural networks to make up the insufficiency of conventional image captioning works. Experimenting over Flickr30k and our NetiLook datasets, we demonstrate our proposed approaches benefit fashion photo commenting and improve image captioning tasks both in accuracy and diversity.", "meta": {}, "annotation_approver": null, "labels": [[776, 784, "Material"], [985, 997, "Method"], [1003, 1018, "Method"], [1107, 1116, "Material"], [1125, 1133, "Material"], [10, 36, "Method"], [1259, 1268, "Metric"], [1272, 1281, "Metric"], [753, 769, "Material"], [72, 88, "Task"], [339, 369, "Method"], [373, 456, "Task"], [841, 872, "Task"]]}
{"id": 4475, "text": "In this paper, we consider the problem of anomaly detection. Previous studies mostly deal with this task in either supervised or unsupervised manner according to whether label information is available. However, there always exists settings which are different from the two standard manners. In this paper, we address the scenario when anomalies are partially observed, i.e., we are given a large amount of unlabeled instances as well as a handful labeled anomalies. We refer to this problem as anomaly detection with partially observed anomalies, and proposed a two-stage method ADOA to solve it. Firstly, by addressing the difference between the anomalies, the observed anomalies are clustered, while the unlabeled instances are filtered to get potential anomalies and reliable normal instances. Then, with the above instances, a weight is attached to each instance according to the confidence of its label, and a weightedmulti-class model is built, which will be further used to distinguish different anomalies to the normal instances. Experimental results show that in the aforementioned setting, existing methods behave unsatisfactorily and the proposed method performs significantly better than all these methods, which validates the effectiveness of the proposed approach.", "meta": {}, "annotation_approver": null, "labels": [[42, 59, "Task"], [915, 940, "Method"], [493, 511, "Task"], [561, 584, "Method"]]}
{"id": 4476, "text": "The neural networks have attracted great attention for sentence similarity modeling in recent years. Most neural networks focus on the representation of each sentence, while the common features of a sentence pair are not well studied. In this paper, we propose a Collaborative and Adversarial Network (CAN), which explicitly models the common features between two sentences for enhancing sentence similarity modeling. To be specific, a common feature extractor is presented and embedded into our CAN model, which includes a generator and a discriminator playing a collaborative and adversarial game for common feature extraction. Experiments on three benchmark datasets, namely TREC-QA and WikiQA for answer selection and MSRP for paraphrase identification, show that our proposed model is effective to boost the performance of sentence similarity modeling. In particular, our proposed model outperforms the state-of-the-art approaches on TREC-QA without using any external resources or pre-training. For the other two datasets, our model is also comparable to if not better than the recent neural network approaches.", "meta": {}, "annotation_approver": null, "labels": [[4, 19, "Method"], [54, 83, "Task"], [106, 121, "Method"], [378, 416, "Task"], [496, 505, "Method"], [678, 685, "Material"], [690, 696, "Material"], [722, 726, "Material"], [939, 946, "Material"], [263, 306, "Method"], [1091, 1105, "Method"]]}
{"id": 4477, "text": "Persuasiveness is a creative art which aims at inducing certain set of beliefs in the target audience. In an e-commerce seing, for a newly launched product, persuasive descriptions are oen composed to motivate an online buyer towards a successful purchase. Such descriptions can be catchy taglines, product-summaries, style-tips etc.. In this paper, we present PersuAIDE! a persuasive system based on linguistic creativity to generate various forms of persuasive sentences from the input product specication. To demonstrate the eectiveness of the proposed system, we have applied the technology to fashion domain, where, for a given fashion product like ”red collar shirt” we were able to generate descriptive sentences that not only explain the item but also garner positive aention, making it persuasive. PersuAIDE! identies fashion related keywords from input specications and intelligently expands the keywords to creative phrases. Once such compatible phrases are obtained, persuasive descriptions are synthesized from the set of phrases and input keywords with the help of a neural language model trained on a large domainspecic fashion corpus. We evaluate the system on a large fashion corpus collected from dierent sources using (a) automatic text generation metrics used for Machine Translation and Automatic Summarization evaluation and Readability measurement, and (b) human judgment scores evaluating the persuasiveness and uency of the generated text. Experimental results and qualitative analysis show that an unsupervised system like ours can produce more creative and beer constructed persuasive output than supervised generative counterparts based on neural sequence-to-sequence models and statistical machine translation.", "meta": {}, "annotation_approver": null, "labels": [[47, 101, "Task"], [1088, 1109, "Method"], [1193, 1207, "Material"], [1679, 1713, "Method"], [1718, 1749, "Method"], [428, 474, "Task"], [1260, 1275, "Task"], [1293, 1312, "Task"], [1317, 1340, "Task"]]}
{"id": 4478, "text": "As data streams become more prevalent, the necessity for online algorithms that mine this transient and dynamic data becomes clearer. Multi-label data stream classification is a supervised learning problem where each instance in the data stream is classified into one or more pre-defined sets of labels. Many methods have been proposed to tackle this problem, including but not limited to ensemble-based methods. Some of these ensemble-based methods are specifically designed to work with certain multi-label base classifiers; some others employ online bagging schemes to build their ensembles. In this study, we introduce a novel online and dynamically-weighted stacked ensemble for multi-label classification, called GOOWE-ML, that utilizes spatial modeling to assign optimal weights to its component classifiers. Our model can be used with any existing incremental multi-label classification algorithm as its base classifier. We conduct experiments with 4 GOOWE-ML-based multi-label ensembles and 7 baseline models on 7 real-world datasets from diverse areas of interest. Our experiments show that GOOWE-ML ensembles yield consistently better results in terms of predictive performance in almost all of the datasets, with respect to the other prominent ensemble models.", "meta": {}, "annotation_approver": null, "labels": [[134, 172, "Task"], [389, 411, "Method"], [427, 449, "Method"], [497, 526, "Method"], [546, 568, "Method"], [684, 710, "Task"], [743, 759, "Method"], [868, 904, "Method"], [1020, 1042, "Material"], [631, 679, "Method"]]}
{"id": 4479, "text": "Answer passage retrieval is an increasingly important information retrieval task as queries become more precise and mobile and audio interfaces more prevalent. In this task, the goal is to retrieve a contiguous series of sentences (a passage) that concisely addresses the information need expressed in the query. Recent work with deep learning has shown the efficacy of distributed text representations for retrieving sentences or tokens for question answering. However, determining the relevancy of answer passages remains a significant challenge, specifically when there exists a lexical and semantic gap between the text representation used for training and the collection’s vocabulary. In this paper, we demonstrate the flexibility of a character based approach on the task of answer passage retrieval, agnostic to the source of embeddings and with improved performance in P@1 and MRR metrics over a word based approach as the collections degrade in quality.", "meta": {}, "annotation_approver": null, "labels": [[0, 24, "Task"], [330, 343, "Method"], [370, 402, "Method"], [442, 460, "Task"], [741, 765, "Method"], [781, 805, "Task"], [904, 923, "Method"], [54, 76, "Task"], [885, 889, "Metric"], [406, 427, "Task"], [877, 880, "Metric"]]}
{"id": 4480, "text": "In the last years, job recommender systems have become popular since they successfully reduce information overload by generating personalized job suggestions. Although in the literature exists a variety of techniques and strategies used as part of job recommender systems, most of them fail to recommending job vacancies that fit properly to the job seekers profiles. Thus, the contributions of this work are threefold, we: i) made publicly available a new dataset formed by a set of job seekers profiles and a set of job vacancies collected from different job search engine sites; ii) put forward the proposal of a framework for job recommendation based on professional skills of job seekers; and iii) carried out an evaluation to quantify empirically the recommendation abilities of two state-of-the-art methods, considering different configurations, within the proposed framework. We thus present a general panorama of job recommendation task aiming to facilitate research and real-world application design regarding this important issue.", "meta": {}, "annotation_approver": null, "labels": [[477, 495, "Material"], [629, 648, "Task"], [630, 648, "Task"], [922, 940, "Task"], [511, 531, "Material"]]}
{"id": 4481, "text": "We present an Information Retrieval framework that leverages Heterogeneous Information Network (HIN) embeddings for contextual suggestion. Our method represents users, documents and other context-related documents as heterogeneous objects in a HIN. Using meta-paths, selected based on domain knowledge, we create graph embeddings from this network, thereby learning a representation of users and objects in the same semantic vector space. This allows inferences of user interest on unseen objects based on distance in the embedding space. These object distances are then incorporated as features in a well-established learning to rank (LTR) framework. We make use of the 2016 TREC Contextual Suggestion (TRECCS) dataset, which contains user profiles in the form of relevance-rated documents, and demonstrate the competitiveness of our approach by comparing our system to the best performing systems of the TRECCS task.", "meta": {}, "annotation_approver": null, "labels": [[116, 137, "Task"], [61, 100, "Method"], [244, 247, "Method"], [671, 711, "Material"], [906, 912, "Material"]]}
{"id": 4482, "text": "Entity Mixture refers to a phenomenon that the information on an entity is mistaken as attributes of another entity in information extraction during knowledge base (KB) construction and population. To improve the quality of knowledge-based services, data accuracy and validity in KBs should be enhanced. This paper presents a clustering analysis-based approach for detecting potentially mixed entities in a KB. Our approach aims at detecting the inconsistency of the attribute values of a KB instance as an indication of entity mixture occurrence. This paper also presents an experiment conducted on a data set of industrial applications to demonstrate the process of entity mixture detection. Experimental results show that our proposed methodology performs well in detecting mixed entities.", "meta": {}, "annotation_approver": null, "labels": [[668, 692, "Task"], [326, 360, "Method"], [365, 409, "Task"]]}
{"id": 4483, "text": "Starting with the earliest studies showing that the spread of new trends, information, and innovations is closely related to the social influence exerted on people by their social networks, the research on social influence theory took off, providing remarkable evidence on social influence induced viral phenomena. Fueled by the extreme popularity of online social networks and social media, computational social influence has emerged as a subfield of data mining whose goal is to analyze and optimize social influence using computational frameworks such as algorithm design and theoretical modeling. One of the fundamental problems in this field is the problem of influence maximization, primarily motivated by the application of viral marketing. The objective is to identify a small set of users in a social network who, when convinced to adopt a product, shall influence others in the network in a manner that leads to a large number of adoptions.\n In this tutorial, we extensively survey the research on social influence propagation and maximization, with a focus on the recent algorithmic and theoretical advances. To this end, we provide detailed reviews of the latest research effort devoted to (i) improving the efficiency and scalability of the influence maximization algorithms; (ii) context-aware modeling of the influence maximization problem to better capture real-world marketing scenarios; (iii) modeling and learning of real-world social influence; (iv) bridging the gap between social advertising and viral marketing.", "meta": {}, "annotation_approver": null, "labels": [[665, 687, "Task"], [1008, 1053, "Task"], [1324, 1346, "Task"], [1294, 1316, "Method"]]}
{"id": 4484, "text": "Problems involving multiple networks are prevalent in many scientific and other domains. In particular, network alignment, or the task of identifying corresponding nodes in different networks, has applications across the social and natural sciences. Motivated by recent advancements in node representation learning for single-graph tasks, we propose REGAL (REpresentation learning-based Graph ALignment), a framework that leverages the power of automatically-learned node representations to match nodes across different graphs. Within REGAL we devise xNetMF, an elegant and principled node embedding formulation that uniquely generalizes to multi-network problems. Our results demonstrate the utility and promise of unsupervised representation learning-based network alignment in terms of both speed and accuracy. REGAL runs up to 30x faster in the representation learning stage than comparable methods, outperforms existing network alignment methods by 20 to 30% accuracy on average, and scales to networks with millions of nodes each.", "meta": {}, "annotation_approver": null, "labels": [[350, 403, "Method"], [104, 121, "Task"], [138, 191, "Task"], [794, 799, "Metric"], [804, 812, "Metric"], [445, 487, "Method"]]}
{"id": 4485, "text": "Learning node representations for networks has attracted much attention recently due to its effectiveness in a variety of applications. This paper focuses on learning node representations for heterogeneous star networks, which have a center node type linked with multiple attribute node types through different types of edges. In heterogeneous star networks, we observe that the training order of different types of edges affects the learning performance significantly. Therefore we study learning curricula for node representation learning in heterogeneous star networks, i.e., learning an optimal sequence of edges of different types for the node representation learning process. We formulate the problem as a Markov decision process, with the action as selecting a specific type of edges for learning or terminating the training process, and the state as the sequence of edge types selected so far. The reward is calculated as the performance on external tasks with node representations as features, and the goal is to take a series of actions to maximize the cumulative rewards. We propose an approach based on deep reinforcement learning for this problem. Our approach leverages LSTM models to encode states and further estimate the expected cumulative reward of each state-action pair, which essentially measures the long-term performance of different actions at each state. Experimental results on real-world heterogeneous star networks demonstrate the effectiveness and efficiency of our approach over competitive baseline approaches.", "meta": {}, "annotation_approver": null, "labels": [[1115, 1142, "Method"], [158, 187, "Task"], [712, 735, "Method"], [1184, 1195, "Method"]]}
{"id": 4486, "text": "At its core, the Document Object Model (DOM) defines a tree-like data structure for representing documents in general and HTML documents in particular. It is the heart of any modern web browser. Formalizing the key concepts of the DOM is a prerequisite for the formal reasoning over client-side JavaScript programs and for the analysis of security concepts in modern web browsers. We present a formalization of the core DOM, with focus on the node-tree and the operations defined on node-trees, in Isabelle/HOL. We use the formalization to verify the functional correctness of the most important functions defined in the DOM standard. Moreover, our formalization is (1) extensible, i.e., can be extended without the need of re-proving already proven properties and (2) executable, i.e., we can generate executable code from our specification.", "meta": {}, "annotation_approver": null, "labels": [[84, 106, "Task"], [122, 136, "Material"], [540, 633, "Task"], [394, 423, "Method"]]}
{"id": 4487, "text": "The value of microblogging services (such as Twitter) and social networks (such as Facebook) in disseminating and discussing important events is currently under serious threat from automated or human contributors employed to distort information. While detecting coordinated attacks by their behaviour (e.g. different accounts posting the same images or links, fake profiles, etc.) has been already explored, here we look at detecting coordination in the content (words, phrases, sentences). We are proposing a metric capable of capturing the differences between organic and coordinated posts, which is based on the estimated probability of coincidentally repeating a word sequence. Our simulation results support our conjecture that only when the metric takes the context and the properties of the repeated sequence into consideration, it is capable of separating organic and coordinated content. We also demonstrate how those context-specific adjustments can be obtained using existing resources.", "meta": {}, "annotation_approver": null, "labels": [[252, 281, "Task"], [45, 52, "Material"], [83, 91, "Material"], [528, 591, "Task"], [615, 680, "Method"]]}
{"id": 4488, "text": "In this paper, we improve the low-rank matrix completion algorithm by assuming that the data points lie in a union of low dimensional subspaces. We applied the self-expressiveness, which is a property of a dataset when the data points lie in a union of low dimensional subspaces, to the low-rank matrix completion. By considering self-expressiveness of low dimensional subspaces, the proposed low-rank matrix completion may perform well even with little information, leading to the robust completion on a dataset with high missing rate. In our experiments on movie rating datasets, the proposed model outperforms state-of-the-art matrix completion models. In clustering experiments conducted on MNIST dataset, the result indicates that our method closely recovers the subspaces of original dataset even with the high missing rate.", "meta": {}, "annotation_approver": null, "labels": [[18, 66, "Task"], [160, 179, "Method"], [393, 419, "Method"], [559, 580, "Material"], [694, 708, "Material"]]}
{"id": 4489, "text": "Our usage of language is not solely reliant on cognition but is arguably determined by myriad external factors leading to a global variability of linguistic patterns. This issue, which lies at the core of sociolinguistics and is backed by many small-scale studies on faceto-face communication, is addressed here by constructing a dataset combining the largest French Twitter corpus to date with detailed socioeconomic maps obtained from national census in France. We show how key linguistic variables measured in individual Twitter streams depend on factors like socioeconomic status, location, time, and the social network of individuals. We found that (i) people of higher socioeconomic status, active to a greater degree during the daytime, use a more standard language; (ii) the southern part of the country is more prone to use more standard language than the northern one, while locally the used variety or dialect is determined by the spatial distribution of socioeconomic status; and (iii) individuals connected in the social network are closer linguistically than disconnected ones, even after the effects of status homophily have been removed. Our results inform sociolinguistic theory and may inspire novel learning methods for the inference of socioeconomic status of people from the way they tweet.", "meta": {}, "annotation_approver": null, "labels": [[360, 381, "Material"], [4, 21, "Task"]]}
{"id": 4490, "text": "We will demonstrate a reusable framework for developing knowledge graphs that supports general, open-ended development of knowledge curation, interaction, and inference. Knowledge graphs need to be easily maintainable and usable in sometimes complex application settings. Often, scaling knowledge graph updates can require developing a knowledge curation pipeline that either replaces the graph wholesale whenever updates are made, or requires detailed tracking of knowledge provenance across multiple data sources. Fig. 1 shows how Whyis provides a semantic analysis ecosystem: an environment that supports research and development of semantic analytics for which we previously had to build custom applications [3,4]. Users interact through a suite of knowledge graph views driven by the node type and view requested in the URL. Knowledge curation methods include Semantic ETL, external linked data mapping,and Natural Language Processing (NLP). Autonomous inference agents expand the available knowledge using traditional deductive reasoning as well as inductive methods that can include predictive models, statistical reasoners, and machine learning. Whyis is used in a number of areas today, including nanopolymers, spectrum policy, and health informatics. We demonstrate Whyis by creating and deploying an example Biological Knowledge Graph (BioKG), using data from DrugBank and Uniprot1, and briefly discuss benefits of using our approach over a conventional knowledge graph pipeline.", "meta": {}, "annotation_approver": null, "labels": [[830, 848, "Task"], [1371, 1379, "Material"], [1384, 1392, "Material"], [336, 354, "Task"], [1090, 1152, "Method"], [879, 907, "Method"], [912, 945, "Method"], [865, 877, "Method"]]}
{"id": 4491, "text": "Friend and item recommendation on a social media site is an important task, which not only brings conveniences to users but also benefits platform providers. However, recommendation for newly launched social media sites is challenging because they often lack user historical data and encounter data sparsity and cold-start problem. Thus, it is important to exploit auxiliary information to help improve recommendation performances on these sites. Existing approaches try to utilize the knowledge transferred from other mature sites, which often require overlapped users or similar items to ensure an effective knowledge transfer. However, these assumptions may not hold in practice because 1) Overlapped user set is often unavailable and costly to identify due to the heterogeneous user profile, content and network data, and 2) Different schemes to show item attributes across sites cause the attribute values inconsistent, incomplete, and noisy. Thus, how to transfer knowledge when no direct bridge is given between two social media sites remains a challenge. In addition, another auxiliary information we can exploit is the mutual benefit between social relationships and rating preferences within the platform. User-user relationships are widely used as side information to improve item recommendation, whereas how to exploit user-item interactions for friend recommendation is rather limited. To tackle these challenges, we propose aCross media jointF riend andI temRe commendation framework (CrossFire ), which can capture both 1) cross-platform knowledge transfer, and 2) within-platform correlations among user-user relations and user-item interactions. Empirical results on real-world datasets demonstrate the effectiveness of the proposed framework.", "meta": {}, "annotation_approver": null, "labels": [[1287, 1306, "Task"], [1358, 1379, "Task"], [1439, 1510, "Method"]]}
{"id": 4492, "text": "Intelligent personal assistant systems with either text-based or voice-based conversational interfaces are becoming increasingly popular around the world. Retrieval-based conversation models have the advantages of returning fluent and informative responses. Most existing studies in this area are on open domain ''chit-chat'' conversations or task / transaction oriented conversations. More research is needed for information-seeking conversations. There is also a lack of modeling external knowledge beyond the dialog utterances among current conversational models. In this paper, we propose a learning framework on the top of deep neural matching networks that leverages external knowledge for response ranking in information-seeking conversation systems. We incorporate external knowledge into deep neural models with pseudo-relevance feedback and QA correspondence knowledge distillation. Extensive experiments with three information-seeking conversation data sets including both open benchmarks and commercial data show that, our methods outperform various baseline methods including several deep text matching models and the state-of-the-art method on response selection in multi-turn conversations. We also perform analysis over different response types, model variations and ranking examples. Our models and research findings provide new insights on how to utilize external knowledge with deep neural models for response selection and have implications for the design of the next generation of information-seeking conversation systems.", "meta": {}, "annotation_approver": null, "labels": [[155, 190, "Method"], [414, 447, "Task"], [595, 613, "Method"], [628, 657, "Method"], [696, 712, "Task"], [797, 815, "Method"], [851, 891, "Method"], [716, 756, "Task"], [821, 846, "Method"]]}
{"id": 4493, "text": "Collaborative filtering techniques are a common approach for building recommendations, and have been widely applied in real recommender systems. However, collaborative filtering usually suffers from limited performance due to the sparsity of user-item interaction. To address this issue, auxiliary information is usually used to improve the performance. Transfer learning provides the key idea of using knowledge from auxiliary domains. An assumption of transfer learning in collaborative filtering is that the source domain is a full rating matrix, which may not hold in many real-world applications. In this paper, we investigate how to leverage rating patterns from multiple incomplete source domains to improve the quality of recommender systems. First, by exploiting the transferred learning, we compress the knowledge from the source domain into a cluster-level rating matrix. The rating patterns in the low-level matrix can be transferred to the target domain. Specifically, we design a knowledge extraction method to enrich rating patterns by relaxing the full rating restriction on the source domain. Finally, we propose a robust multiple-rating-pattern transfer learning model for cross-domain collaborative filtering, which is called MINDTL, to accurately predict missing values in the target domain. Extensive experiments on real-world datasets demonstrate that our proposed approach is effective and outperforms several alternative methods.", "meta": {}, "annotation_approver": null, "labels": [[61, 85, "Task"], [354, 371, "Method"], [1138, 1187, "Method"], [1191, 1227, "Task"], [1245, 1251, "Method"], [124, 143, "Task"], [776, 796, "Method"], [154, 177, "Method"]]}
{"id": 4494, "text": "Offering products in the forms of menu bundles is a common practice in marketing to attract customers and maximize revenues. In crowdfunding platforms such as Kickstarter, rewards also play an important part in influencing project success. Designing rewards consisting of the appropriate items is a challenging yet crucial task for the project creators. However, prior research has not considered the strategies project creators take to offer and bundle the rewards, making it hard to study the impact of reward designs on project success. In this paper, we raise a novel research question: understanding project creators' decisions of reward designs to level their chance to succeed. We approach this by modeling the design behavior of project creators, and identifying the behaviors that lead to project success. We propose a probabilistic generative model, Menu-Offering-Bundle (MOB) model, to capture the offering and bundling decisions of project creators based on collected data of 14K crowdfunding projects and their 149K reward bundles across a half-year period. Our proposed model is shown to capture the offering and bundling topics, outperform the baselines in predicting reward designs. We also find that the learned offering and bundling topics carry distinguishable meanings and provide insights of key factors on project success.", "meta": {}, "annotation_approver": null, "labels": [[240, 258, "Task"], [828, 886, "Method"], [1172, 1197, "Task"], [992, 1013, "Material"], [1029, 1043, "Material"]]}
{"id": 4495, "text": "This paper examines how legacies of thinking about ontology, logic, and how best to approach knowledge representation have become interwoven in the architecture of the technologies that enable a Semantic Web. As a cultural anthropologist, I approach this study with qualitative historical and ethnographic methodologies, positioning the community of researchers that have been involved in the design and implementation of Semantic Web protocols and technologies as my primary field site. Two concepts from Science and Technology Studies are introduced - thought styles and design logics. The paper demonstrates how diverse thinking about how to approach knowledge representation on the Web is rooted in debates that emerged in artificial intelligence in the 1970s and 1980s. It then goes on to discuss how the diverse approaches to Web semantics that emerged from these legacies have cultural and political implications. The paper concludes with a call for further research that positions Web architectures as objects of social and cultural study.", "meta": {}, "annotation_approver": null, "labels": [[93, 117, "Task"], [266, 319, "Method"]]}
{"id": 4496, "text": "Social media has quickly established itself as an important means that people, NGOs and governments use to spread information during natural or man-made disasters, mass emergencies and crisis situations. Given this important role, real-time analysis of social media contents to locate, organize and use valuable information for disaster management is crucial. In this paper, we propose self-learning algorithms that, with minimal supervision, construct a simple bag-of-words model of information expressed in the news about various natural disasters. Such a model is human-understandable, human-modifiable and usable in a realtime scenario. Since tweets are a diffferent category of documents than news, we next propose a model transfer algorithm, which essentially refines the model learned from news by analyzing a large unlabeled corpus of tweets. We show empirically that model transfer improves the predictive accuracy of the model. We demonstrate empirically that our model learning algorithm is better than several state of the art semi-supervised learning algorithms.", "meta": {}, "annotation_approver": null, "labels": [[231, 274, "Task"], [386, 410, "Method"], [462, 480, "Method"], [328, 347, "Task"], [915, 923, "Metric"]]}
{"id": 4497, "text": "Solving technical problems with complex systems and integrating the many technologies employed in these multifaceted structures has been a recurring theme in Smart Cities research. This paper presents an analysis of the reason this problem has been so well explored but persists with no solution widely available. The problem is viewed as a combination of Smart City needs, governance, and increasingly technically difficult decisions. The paper describes the requirements that must be met to develop a framework that can address this seeming intractable and expanding integration concern, identifies the governance processes that can be used to address this problem, and to manage integration in Smart Cities, The solution proposed is a formalized accepted and managed technology regulated environment introduced by governance groups composed of city planners/managers, citizen, stake holders, and technology delivery organizations. The solution requirements dictate the establishment of a standard that would guide the development and usage of automated, autonomous components, integrating dynamically with software agents. All of this working to rapidly optimize shared resources through error handling processes executing largely at no cost except those of processing time, meeting safety guidelines, satisfying operational monitoring needs, and meeting post issue liability guidelines. This technical standard would obligate developers and vendors to meet safety standards and accept liability for malfeasance. As initiating steps, Smart City managers must come together and establish a basic understanding of the goals and regulations, and the methodologies for implementing them.", "meta": {}, "annotation_approver": null, "labels": [[682, 709, "Task"]]}
{"id": 4498, "text": "Verifiability is one of the core editing principles in Wikipedia, where editors are encouraged to provide citations for the added statements. Statements can be any arbitrary piece of text, ranging from a sentence up to a paragraph. However, in many cases, citations are either outdated, missing, or link to non-existing references (e.g. dead URL, moved content etc.). In total, 20% of the cases such citations refer to news articles and represent the second most cited source. Even in cases where citations are provided, there are no explicit indicators for the span of a citation for a given piece of text. In addition to issues related with the verifiability principle, many Wikipedia entity pages are incomplete, with relevant information that is already available in online news sources missing. Even for the already existing citations, there is often a delay between the news publication time and the reference time. In this thesis, we address the aforementioned issues and propose automated approaches that enforce the verifiability principle in Wikipedia, and suggest relevant and missing news references for further enriching Wikipedia entity pages. To this end we make the following contributions as part of this thesis [1, 2, 3, 4]", "meta": {}, "annotation_approver": null, "labels": [[1124, 1156, "Task"], [55, 64, "Material"], [677, 686, "Material"], [1052, 1061, "Material"], [1013, 1038, "Task"]]}
{"id": 4499, "text": "Social media, as a major platform to disseminate information, has changed the way users and communities contribute content. In this paper, we aim to study content modifications on public Facebook pages operated by news media, community groups, and bloggers. We also study the possible reasons behind them, and their effects on user interaction. We conducted a detailed study of Content Censorship (CC) and Content Edit (CE) in Facebook using a detailed longitudinal dataset consisting of 57 public Facebook pages over 3 weeks covering 145,955 posts and 9,379,200 comments. We detected many CC and CE activities between 28% and 56% of these pages (in both Facebook Posts and Comments). Manual judgements on these post/comment removals and edits show that majority of the content censorship is related to negative reports on events and personal grouses, and content edit is mainly performed to improve content quality and correctness. Furthermore, recency effect is also observed as part of Facebook content modification behavior.", "meta": {}, "annotation_approver": null, "labels": [[378, 436, "Task"], [498, 512, "Material"], [543, 548, "Material"], [563, 571, "Material"]]}
{"id": 4500, "text": "We consider the problem of discovering local events on the web, where events are entities extracted from webpages. Examples of such local events include small venue concerts, farmers markets, sports activities, etc. Given an event entity, we propose a graph-based framework for retrieving a ranked list of related events that a user is likely to be interested in attending. Due to the difficulty of obtaining ground-truth labels for event entities, which are temporal and are constrained by location, our retrieval framework is unsupervised, and its graph-based formulation addresses (a) the challenge of feature sparseness and noisiness, and (b) the semantic mismatch problem in a self-contained and principled manner.\n To validate our methods, we collect human annotations and conduct a comprehensive empirical study, analyzing the performance of our methods with regard to relevance, recall, and diversity. This study shows that our graph-based framework is significantly better than any individual feature source, and can be further improved with minimal supervision.", "meta": {}, "annotation_approver": null, "labels": [[27, 62, "Task"], [252, 273, "Method"], [749, 774, "Method"], [936, 957, "Method"], [876, 885, "Metric"], [887, 893, "Metric"], [899, 908, "Metric"]]}
{"id": 4501, "text": "While search technology is widely used for learning-oriented information needs, the results provided by popular services such as Web search engines are optimized primarily for generic relevance, not effective learning outcomes. As a result, the typical information trail that a user must follow while searching to achieve a learning goal may be an inefficient one involving unnecessarily easy or difficult content, or material that is irrelevant to actual learning progress relative to a user's existing knowledge. We address this problem by introducing a novel theoretical framework, algorithms, and empirical analysis of an information retrieval model that is optimized for learning outcomes instead of generic relevance. We do this by formulating an optimization problem that incorporates a cognitive learning model into a retrieval objective, and then give an algorithm for an efficient approximate solution to find the search results that represent the best 'training set' for a human learner. Our model can personalize results for an individual user's learning goals, as well as account for the effort required to achieve those goals for a given set of retrieval results. We investigate the effectiveness and efficiency of our retrieval framework relative to a commercial search engine baseline ('Google') through a crowdsourced user study involving a vocabulary learning task, and demonstrate the effectiveness of personalized results from our model on word learning outcomes.", "meta": {}, "annotation_approver": null, "labels": [[43, 78, "Task"], [626, 653, "Method"], [301, 337, "Task"]]}
{"id": 4502, "text": "Current search and recommendation engines enable us to effectively retrieve a set of documents based on topical relevance. What is not taken into account is the knowledge a user may already have about a topic, e.g., whether information is redundant or whether he/she is able to understand the results. We propose a method to measure demonstrated potential domain knowledge (DPDK) as a proxy for knowledge and use this metric to analyse the query log of a user spanning over 10 years.", "meta": {}, "annotation_approver": null, "labels": [[440, 449, "Material"], [333, 378, "Task"]]}
{"id": 4503, "text": "In the current online Open Science context, scientific datasets and tools for deep text analysis, visualization and exploitation play a major role. We present a system for deep analysis and annotation of scientific text collections. We also introduce the first version of the SEPLN Anthology, a bi-lingual (Spanish and English) fully annotated text resource in the field of natural language processing that we created with our system. Moreover, a faceted-search and visualization system to explore the created resource is introduced. All resources created for this paper will be available to the research community.", "meta": {}, "annotation_approver": null, "labels": [[83, 128, "Task"], [276, 291, "Material"]]}
{"id": 4504, "text": "An automated-based system has been developed in order to gather stories from iCanQuit. Similarly, using Twitter Streaming API, geolocated tweets within UK region have been collected during a three months period, and those related to smoking cessation, according to the semantic text matching, are extracted and stored in a database. An automated classifier has been employed to identify the four most dominant categories in Health field for each document of blogs or Twitter dataset. A total of 880 stories from iCanQuit and 22155 relevant tweets have been collected and indexed in SQLite database. The automated classifier highlighted four categories: Weight Loss, Mental Health, Addiction, Support Group. The analysis surprisingly reveals that both blogs and Twitter datasets agree that the dominant source of smoking cessation is related to weight loss (shape body appearance), or body-look, while the support group, which includes any clinician supports, plays little impact on the smokers’ quit motivation, a result that maybe precious for health authorities.", "meta": {}, "annotation_approver": null, "labels": [[77, 85, "Material"], [104, 111, "Material"], [336, 356, "Method"], [377, 436, "Task"], [603, 623, "Method"], [512, 520, "Material"], [540, 546, "Material"], [751, 756, "Material"], [761, 768, "Material"], [458, 463, "Material"], [467, 474, "Material"]]}
{"id": 4505, "text": "We analyze nearly 20 million geocoded PubMed articles with author affiliations. Using K-means clustering for the lower 48 US states and mainland China, we find that the average published paper is within a relatively short distance of a few centroids. These centroids have shifted very little over the past 30 years, and the distribution of distances to these centroids has not changed much either. The overall country centroids have gradually shifted south (about 0.2° for the USA and 1.7° for China), while the longitude has not moved significantly. These findings indicate that there are few large scientific hubs in the USA and China and the typical investigator is within geographical reach of one such hub. This sets the stage to study centralization of biomedical research at national and regional levels across the globe, and over time.", "meta": {}, "annotation_approver": null, "labels": [[86, 104, "Method"], [38, 53, "Material"], [735, 778, "Task"]]}
{"id": 4506, "text": "As the Web evolves towards a service-oriented architecture, application program interfaces (APIs) are becoming an increasingly important way to provide access to data, services, and devices. We study the problem of natural language interface to APIs (NL2APIs), with a focus on web APIs for web services. Such NL2APIs have many potential benefits, for example, facilitating the integration of web services into virtual assistants.\n We propose the first end-to-end framework to build an NL2API for a given web API. A key challenge is to collect training data, i.e., NL command-API call pairs, from which an NL2API can learn the semantic mapping from ambiguous, informal NL commands to formal API calls. We propose a novel approach to collect training data for NL2API via crowdsourcing, where crowd workers are employed to generate diversified NL commands. We optimize the crowdsourcing process to further reduce the cost. More specifically, we propose a novel hierarchical probabilistic model for the crowdsourcing process, which guides us to allocate budget to those API calls that have a high value for training NL2APIs. We apply our framework to real-world APIs, and show that it can collect high-quality training data at a low cost, and build NL2APIs with good performance from scratch. We also show that our modeling of the crowdsourcing process can improve its effectiveness, such that the training data collected via our approach leads to better performance of NL2APIs than a strong baseline.", "meta": {}, "annotation_approver": null, "labels": [[215, 259, "Task"], [958, 1020, "Method"], [309, 316, "Task"], [485, 491, "Task"], [605, 611, "Task"], [758, 764, "Task"], [1112, 1119, "Task"], [1245, 1252, "Task"], [1466, 1473, "Task"]]}
{"id": 4507, "text": "Digital Twin models are computerized clones of physical assets that can be used for in-depth analysis. Industrial production lines tend to have multiple sensors to generate near real-time status information for production. Industrial Internet of Things datasets are difficult to analyze and infer valuable insights such as points of failure, estimated overhead. etc. In this paper we introduce a simple way of formalizing knowledge as digital twin models coming from sensors in industrial production lines. We present a way on to extract and infer knowledge from large scale production line data, and enhance manufacturing process management with reasoning capabilities, by introducing a semantic query mechanism. Our system primarily utilizes a graph-based query language equivalent to conjunctive queries and has been enriched with inference rules.", "meta": {}, "annotation_approver": null, "labels": [[0, 19, "Method"], [530, 557, "Task"], [688, 712, "Method"], [746, 772, "Method"]]}
{"id": 4508, "text": "In recent years the number of academic publication increased strongly. As this information flood grows, it becomes more difficult for researchers to find relevant literature effectively. To overcome this difficulty, recommendation systems can be used which often utilize text similarity to find related documents. To improve those systems we add scientometrics as a ranking measure for popularity into these algorithms. In this paper we analyse whether and how scientometrics are useful in a recommender system.", "meta": {}, "annotation_approver": null, "labels": [[492, 510, "Task"], [215, 238, "Task"]]}
{"id": 4509, "text": "Recent years have witnessed a proliferation of large-scale knowledge graphs, from purely academic projects such as YAGO to major commercial projects such as Google's Knowledge Graph and Microsoft's Satori. Whereas there is a large body of research on mining homogeneous graphs, this new generation of information networks are highly heterogeneous, with thousands of entity and relation types and billions of instances of those types (graph vertices and edges). In this tutorial, we present the state of the art in constructing, mining, and growing knowledge graphs. The purpose of the tutorial is to equip newcomers to this exciting field with an understanding of the basic concepts, tools and methodologies, open research challenges, as well as pointers to available datasets and relevant literature. Knowledge graphs have become an enabling resource for a plethora of new knowledge-rich applications. Consequently, the tutorial will also discuss the role of knowledge bases in empowering a range of web applications, from web search to social networks to digital assistants. A publicly available knowledge base (Freebase) will be used throughout the tutorial to exemplify the different techniques.", "meta": {}, "annotation_approver": null, "labels": [[59, 75, "Material"], [802, 818, "Material"], [960, 975, "Material"], [115, 119, "Material"], [157, 181, "Material"], [186, 204, "Material"], [514, 564, "Task"], [1114, 1122, "Material"]]}
{"id": 4510, "text": "The written medium through which we commonly learn about relevant news are news articles. Since there is an abundance of news articles that are written daily, the readers have a common problem of discovering the content of interest and still not be overwhelmed with the amount of it. In this paper we present a system called Event Registry which is able to group articles about an event across languages and extract from the articles core event information in a structured form. In this way, the amount of content that the reader has to check is significantly reduced while additionally providing the reader with a global coverage of each event. Since all event information is structured this also provides extensive and fine-grained options for information searching and filtering that are not available with current news aggregators.", "meta": {}, "annotation_approver": null, "labels": [[408, 477, "Task"]]}
{"id": 4511, "text": "The paper presents a new framework for discrimination of Latin and Italian languages. The first phase maps the text in the given language into a uniformly coded text. It is based on the position of each letter of the script in the text line and its height, derived from its energy profile. The second phase extracts run-length texture measures from the coded text given as 1-D image, by producing a feature vector of 11 values. The obtained feature vectors are adopted for language discrimination by using a clustering algorithm. As a result, the distinction between the two languages is perfectly realized with an accuracy of 100% on a complex database of documents in Latin and Italian languages.", "meta": {}, "annotation_approver": null, "labels": [[39, 84, "Task"], [547, 584, "Task"], [615, 623, "Metric"]]}
{"id": 4512, "text": "Traditional information retrieval (IR) models, in which a document is normally represented as a bag of words and their frequencies, capture the term-level and document-level information. Topic models, on the other hand, discover semantic topic-based information among words. In this paper, we consider term-based information and semantic information as two features of query terms and propose a simple enhancement for ad-hoc IR via topic modeling. In particular, three topic-based hybrid models, LDA-BM25, LDA-MATF and LDA-LM, are proposed. A series of experiments on eight standard datasets show that our proposed models can always outperform significantly the corresponding strong baselines over all datasets in terms of MAP and most of datasets in terms of P@5 and P@20. A direct comparison on eight standard datasets also indicates our proposed models are at least comparable to the state-of-the-art approaches.", "meta": {}, "annotation_approver": null, "labels": [[220, 261, "Task"], [496, 504, "Method"], [506, 514, "Method"], [519, 525, "Method"], [723, 726, "Metric"], [768, 772, "Metric"], [12, 38, "Task"], [432, 446, "Task"], [418, 427, "Task"], [760, 763, "Metric"]]}
{"id": 4513, "text": "Tourism industry has grown tremendously in the previous several decades. Despite its global impact, there still remain a number of open questions related to better understanding of tourists and their habits. In this work we analyze the largest data set of travel receipts considered thus far, and focus on exploring and modeling booking behavior of online customers. We extract useful, actionable insights into the booking behavior, and tackle the task of predicting the booking time. The presented results can be directly used to improve booking experience of customers and optimize targeting campaigns of travel operators.", "meta": {}, "annotation_approver": null, "labels": [[456, 483, "Task"], [306, 365, "Task"]]}
{"id": 4514, "text": "The large scale of scholarly publications poses a challenge for scholars in information-seeking and sensemaking. Bibliometric, information retrieval~(IR), text mining and NLP techniques could help in these activities, but are not yet widely used in digital libraries. This workshop is intended to stimulate IR researchers and digital library professionals to elaborate on new approaches in natural language processing, information retrieval, scientometric and recommendation techniques which can advance the state-of-the-art in scholarly document understanding, analysis and retrieval at scale.", "meta": {}, "annotation_approver": null, "labels": [[76, 95, "Task"], [155, 166, "Task"], [419, 440, "Task"], [442, 455, "Task"], [390, 417, "Task"], [538, 560, "Task"], [127, 153, "Task"]]}
{"id": 4515, "text": "In this shared task, we applied “Learning to Rank” algorithm with multiple features, including lexical features, topic features, knowledge-based features and sentence importance, to Task 1A by regarding reference span finding as an information retrieval problem. Task 1B, discourse facet identifying, is treated as a text classification problem by considering features of both citation contexts and cited spans.", "meta": {}, "annotation_approver": null, "labels": [[33, 49, "Method"], [232, 253, "Task"], [317, 336, "Task"]]}
{"id": 4516, "text": "In an entity classification task, topic or concept hierarchies are often incomplete. Previous work by Dalvi et al. [12] has showed that in non-hierarchical semi-supervised classification tasks, the presence of such unanticipated classes can cause semantic drift for seeded classes. The Exploratory learning [12] method was proposed to solve this problem; however it is limited to the flat classification task. This paper builds such exploratory learning methods for hierarchical classification tasks.\n We experimented with subsets of the NELL [8] ontology and text, and HTML table datasets derived from the ClueWeb09 corpus. Our method (OptDAC-ExploreEM) outperforms the existing Exploratory EM method, and its naive extension (DAC-ExploreEM), in terms of seed class F1 on average by 10% and 7% respectively.", "meta": {}, "annotation_approver": null, "labels": [[139, 192, "Task"], [286, 306, "Method"], [607, 623, "Material"], [767, 769, "Metric"], [680, 701, "Method"], [6, 27, "Task"], [384, 403, "Task"], [466, 493, "Task"], [433, 453, "Method"], [538, 542, "Material"], [570, 589, "Material"]]}
{"id": 4517, "text": "The hyperlink structure of Wikipedia forms a rich semantic network connecting entities and concepts, enabling it as a valuable source for knowledge harvesting. Wikipedia, as crowd-sourced data, faces various data quality issues which significantly impacts knowledge systems depending on it as the information source. One such issue occurs when an anchor text in a Wikipage links to a wrong Wikipage, causing the error link problem. While much of previous work has focused on leveraging Wikipedia for entity linking, little has been done to detect error links.\n In this paper, we address the error link problem, and propose algorithms to detect and correct error links. We introduce an efficient method to generate candidate error links based on iterative ranking in an Anchor Text Semantic Network. This greatly reduces the problem space. A more accurate pairwise learning model was used to detect error links from the reduced candidate error link set, while suggesting correct links in the same time. This approach is effective when data sparsity is a challenging issue. The experiments on both English and Chinese Wikipedia illustrate the effectiveness of our approach. We also provide a preliminary analysis on possible causes of error links in English and Chinese Wikipedia.", "meta": {}, "annotation_approver": null, "labels": [[138, 158, "Task"], [500, 514, "Task"], [540, 558, "Task"], [591, 609, "Task"], [636, 667, "Task"], [855, 879, "Method"], [769, 797, "Method"], [1096, 1125, "Material"], [1248, 1277, "Material"]]}
{"id": 4518, "text": "Spam is a widespread problem for many online services. The use case in this paper is the social bookmarking system Bib-Sonomy, which received over 150 times more registrations from spam users than from normal users over the last ten years.\n A common approach to fight spam is to use machine learning to classify the users into good or malicious users. Based on information the users provide to the service in form of profile information or posts, features are created from which a classifier can make its decision. However, this often means that the accounts of the spam users are already active and can post their spam. In this work we propose an approach for deciding at registration time whether a user is malicious or not. In order to achieve this goal, we extracted 177 features from the information the users provide during the registration process, their IP address, and registration time. With these features we used state-of-the-art classifiers to identify users as spammers or regular users. With the best classifier, we could reach an AUC of 0.912.", "meta": {}, "annotation_approver": null, "labels": [[114, 125, "Material"], [1046, 1049, "Metric"], [303, 350, "Task"], [957, 1000, "Task"]]}
{"id": 4519, "text": "The problem of deciding the overall sentiment of a user review is usually treated as a text classification problem. The simplest machine learning setup for text classification uses a unigram bag-of-words feature representation of documents, and this has been shown to work well for a number of tasks such as spam detection and topic classification. However, the problem of sentiment analysis is more complex and not as easily captured with unigram (single-word) features. Bigram and trigram features capture certain local context and short distance negations—thus outperforming unigram bag-of-words features for sentiment analysis. But higher order n-gram features are often overly specific and sparse, so they increase model complexity and do not generalize well. In this paper, we perform an empirical study of skip-gram features for large scale sentiment analysis. We demonstrate that skip-grams can be used to improve sentiment analysis performance in a model-efficient and scalable manner via regularized logistic regression. The feature sparsity problem associated with higher order n-grams can be alleviated by grouping similar n-grams into a single skip-gram: For example, “waste time” could match the n-gram variants “waste of time”, “waste my time”, “waste more time”, “waste too much time”, “waste a lot of time”, and so on. To promote model-efficiency and prevent overfitting, we demonstrate the utility of logistic regression incorporating both L1 regularization (for feature selection) and L2 regularization (for weight distribution).", "meta": {}, "annotation_approver": null, "labels": [[87, 106, "Task"], [156, 175, "Task"], [183, 226, "Method"], [308, 322, "Task"], [327, 347, "Task"], [373, 391, "Task"], [612, 630, "Task"], [848, 866, "Task"], [922, 940, "Task"], [1458, 1475, "Method"], [1504, 1521, "Method"], [14, 62, "Task"], [440, 470, "Method"], [472, 499, "Method"], [578, 607, "Method"], [636, 664, "Method"], [888, 898, "Method"], [813, 831, "Method"], [998, 1029, "Method"], [1035, 1059, "Task"], [1419, 1438, "Method"]]}
{"id": 4520, "text": "This paper evaluates the effectiveness of a massive open online course (MOOC) as a professional development tool in higher education. The transition from the MOOC’s initial intended use as a low cost way for students to access education and aid their studies has evolved to facilitate continuing professional development (CPD), particularly within the commercial sector [1]. Findings from this study indicate there is an increase in participation and satisfaction amongst higher education staff who undertook a MOOC compared to attending traditional staff development days. Recommendations from this study’s findings highlight that staff were keen to engage with the MOOC format, but felt they needed face-to-face meetings as well to reinforce, contextualize and discuss the key messages of the MOOC. In addition to this, time allocation within workloads should be considered for any future inclusion of MOOCs for staff development.", "meta": {}, "annotation_approver": null, "labels": [[11, 77, "Task"], [285, 326, "Method"]]}
{"id": 4521, "text": "In recent years artificial neural networks have become the method of choice for many pattern recognition tasks. Despite their overwhelming success, a rigorous and easy to interpret mathematical explanation of the influence of input variables on a output produced by a neural network is still missing.\n We propose a generic framework as well as a concrete method for quantifying the influence of individual input signals on the output computed by a deep neural network. Inspired by the variable weighting scheme in the log-linear combination of variables in logistic regression, the proposed method provides linear models for specific observations of the input variables. This linear model locally approximates the behaviour of the neural network and can be used to quantify the influence of input variables in a principled way. We demonstrate the effectiveness of the proposed method in experiments on various synthetic and real-world datasets.", "meta": {}, "annotation_approver": null, "labels": [[518, 576, "Method"], [366, 467, "Task"]]}
{"id": 4522, "text": "The World Wide Web is an infrastructure to publish and retrieve information through web resources. It evolved from a static Web 1.0 to a multimodal and interactive communication and information space which is used to collaboratively contribute and discuss web resources, which is better known as Web 2.0. The evolution into a Semantic Web (Web 3.0) proceeds. One of its remarkable advantages is the decentralized and interlinked data composition. Hence, in contrast to its data distribution, workflows and technologies for decentralized collaborative contribution are missing. In this paper we propose the Structured Feedback protocol as an interactive addition to the Web of Data. It offers support for users to contribute to the evolution of web resources, by providing structured data artifacts as patches for web resources, as well as simple plain text comments. Based on this approach it enables crowd-supported quality assessment and web data cleansing processes in an ad-hoc fashion most web users are familiar with.", "meta": {}, "annotation_approver": null, "labels": [[901, 935, "Task"]]}
{"id": 4523, "text": "When a message, such as a piece of news, spreads in social networks, how can we classify it into categories of interests, such as genuine or fake news? Classification of social media content is a fundamental task for social media mining, and most existing methods regard it as a text categorization problem and mainly focus on using content features, such as words and hashtags. However, for many emerging applications like fake news and rumor detection, it is very challenging, if not impossible, to identify useful features from content. For example, intentional spreaders of fake news may manipulate the content to make it look like real news. To address this problem, this paper concentrates on modeling the propagation of messages in a social network. Specifically, we propose a novel approach, TraceMiner, to (1) infer embeddings of social media users with social network structures; and (2) utilize an LSTM-RNN to represent and classify propagation pathways of a message. Since content information is sparse and noisy on social media, adopting TraceMiner allows to provide a high degree of classification accuracy even in the absence of content information. Experimental results on real-world datasets show the superiority over state-of-the-art approaches on the task of fake news detection and news categorization.", "meta": {}, "annotation_approver": null, "labels": [[152, 190, "Task"], [217, 236, "Task"], [279, 306, "Task"], [438, 453, "Task"], [800, 810, "Method"], [1278, 1297, "Task"], [1302, 1321, "Task"]]}
{"id": 4524, "text": "Social media platforms are increasingly being used to share and seek advice on mental health issues. In particular, Reddit users freely discuss such issues on various subreddits, whose structure and content can be leveraged to formally interpret and relate subreddits and their posts in terms of mental health diagnostic categories. There is prior research on the extraction of mental health-related information, including symptoms, diagnosis, and treatments from social media; however, our approach can additionally provide actionable information to clinicians about the mental health of a patient in diagnostic terms for web-based intervention. Specifically, we provide a detailed analysis of the nature of subreddit content from domain expert's perspective and introduce a novel approach to map each subreddit to the best matching DSM-5 (Diagnostic and Statistical Manual of Mental Disorders - 5th Edition) category using multi-class classifier. Our classification algorithm analyzes all the posts of a subreddit by adapting topic modeling and word-embedding techniques, and utilizing curated medical knowledge bases to quantify relationship to DSM-5 categories. Our semantic encoding-decoding optimization approach reduces the false-alarm-rate from 30% to 2.5% over a comparable heuristic baseline, and our mapping results have been verified by domain experts achieving a kappa score of 0.84.", "meta": {}, "annotation_approver": null, "labels": [[364, 411, "Task"], [794, 839, "Task"], [925, 947, "Method"], [1231, 1247, "Metric"], [1376, 1387, "Metric"]]}
{"id": 4525, "text": "Multi-view anomaly detection is a challenging issue due to diverse data generation mechanisms and inconsistent cluster structures of different views. Existing methods of point anomaly detection are ineffective for scenarios where individual instances are normal, but their collective behavior as a group is abnormal. In this paper, we formalize this group anomaly detection issue, and propose a novel non-parametric bayesian model, named Multi-view Group Anomaly Detection (MGAD). By representing the multi-view data with different latent group and topic structures, MGAD first discovers the distribution of groups or topics in each view, then detects group anomalies effectively. In order to solve the proposed model, we conduct the collapsed Gibbs sampling algorithm for model inference. We evaluate our model on both synthetic and real-world datasets with different anomaly settings. The experimental results demonstrate the effectiveness of the proposed approach on detecting multi-view group anomalies.", "meta": {}, "annotation_approver": null, "labels": [[0, 28, "Task"], [356, 373, "Task"], [401, 430, "Method"], [438, 479, "Method"], [734, 768, "Method"]]}
{"id": 4526, "text": "Open Information Extraction (OpenIE)methods extract (noun phrase, relation phrase, noun phrase) triples from text, resulting in the construction of large Open Knowledge Bases (Open KBs). The noun phrases (NPs) and relation phrases in such Open KBs are not canonicalized, leading to the storage of redundant and ambiguous facts. Recent research has posed canonicalization of Open KBs as clustering over manually-defined feature spaces. Manual feature engineering is expensive and often sub-optimal. In order to overcome this challenge, we propose Canonicalization using Embeddings and Side Information (CESI) – a novel approach which performs canonicalization over learned embeddings of Open KBs. CESI extends recent advances in KB embedding by incorporating relevant NP and relation phrase side information in a principled manner. Through extensive experiments on multiple real-world datasets, we demonstrate CESI’s effectiveness.", "meta": {}, "annotation_approver": null, "labels": [[0, 36, "Task"], [154, 185, "Material"], [239, 247, "Material"], [374, 382, "Material"], [546, 607, "Method"], [696, 700, "Method"]]}
{"id": 4527, "text": "We study the problem of allocating impressions to sellers in ecommerce websites, such as Amazon, eBay or Taobao, aiming to maximize the total revenue generated by the platform. We employ a general framework of reinforcement mechanism design, which uses deep reinforcement learning to design efficient algorithms, taking the strategic behaviour of the sellers into account. Specifically, we model the impression allocation problem as a Markov decision process, where the states encode the history of impressions, prices, transactions and generated revenue and the actions are the possible impression allocations in each round. To tackle the problem of continuity and high-dimensionality of states and actions, we adopt the ideas of the DDPG algorithm to design an actor-critic policy gradient algorithm which takes advantage of the problem domain in order to achieve convergence and stability. We evaluate our proposed algorithm, coined IA(GRU), by comparing it against DDPG, as well as several natural heuristics, under different rationality models for the sellers we assume that sellers follow well-known no-regret type strategies which may vary in their degree of sophistication. We find that IA(GRU) outperforms all algorithms in terms of the total revenue.", "meta": {}, "annotation_approver": null, "labels": [[24, 79, "Task"], [253, 280, "Method"], [783, 801, "Method"]]}
{"id": 4528, "text": "This paper presents a Kernel Entity Salience Model (KESM) that improves text understanding and retrieval by better estimating entity salience (importance) in documents. KESM represents entities by knowledge enriched distributed representations, models the interactions between entities and words by kernels, and combines the kernel scores to estimate entity salience. The whole model is learned end-to-end using entity salience labels. The salience model also improves ad hoc search accuracy, providing effective ranking features by modeling the salience of query entities in candidate documents. Our experiments on two entity salience corpora and two TREC ad hoc search datasets demonstrate the effectiveness of KESM over frequency-based and feature-based methods. We also provide examples showing how KESM conveys its text understanding ability learned from entity salience to search.", "meta": {}, "annotation_approver": null, "labels": [[22, 57, "Method"], [72, 104, "Task"], [169, 173, "Method"], [652, 679, "Material"], [713, 717, "Method"], [803, 807, "Method"]]}
{"id": 4529, "text": "In the 2017 German Federal elections, the “Alternative for Deutschland”, or AfD, party was able to take control of many seats in German parliament. Their success was credited, in part, to their large online presence. Like other “alt-right” organizations worldwide, this party is tech savvy, generating a large social media footprint, especially on Twitter, which provides an ample opportunity to understand their online behavior. In this work we present an analysis of Twitter data related to the aforementioned election. We show how users self-organize into communities, and identify the themes that define those communities. Next we analyze the content generated by those communities, and the extent to which these communities interact. Despite these elections being held in Germany, we note a substantial impact from the English-speaking Twittersphere. Specifically, we note that many of these accounts appear to be from the American alt-right movement, and support the German alt-right movement.", "meta": {}, "annotation_approver": null, "labels": [[457, 481, "Task"]]}
{"id": 4530, "text": "This demo presents SmartPub, a novel web-based platform that supports the exploration and visualization of shallow meta-data (e.g., author list, keywords) and deep meta-data – long tail named entities which are rare, and often relevant only in specific knowledge domain – from scientific publications. The platform collects documents from different sources (e.g. DBLP and Arxiv), and extracts the domain-specific named entities from the text of the publications using Named Entity Recognizers (NERs) which we can train with minimal human supervision even for rare entity types. The platform further enables the interaction with the Crowd for filtering purposes or training data generation, and provides extended visualization and exploration capabilities. SmartPubwill be demonstrated using sample collection of scientific publications focusing on the computer science domain and will address the entity types Dataset (i.e. dataset presented or used in a publication), and Methods (i.e. algorithms used to create/enrich/analyse a data set).", "meta": {}, "annotation_approver": null, "labels": [[384, 461, "Task"], [468, 499, "Method"]]}
{"id": 4531, "text": "Missing values in real world datasets are a common issue. Handling missing values is one of the most key aspects in data mining, as it can seriously impact the performance of predictive models. In this paper we proposed a unified Boosting framework that consolidates model construction and missing value handling. At each Boosting iteration, weights are assigned to both the samples and features. The sample weights make difficult samples become the learning focus, while the feature weights enable critical features to be compensated by less critical features when they are unavailable. A weak classifier that abstains (i.e, produce no prediction when required feature value is missing) is learned on a data subset determined by the feature weights. Experimental results demonstrate the efficacy and robustness of the proposed method over existing Boosting algorithms.", "meta": {}, "annotation_approver": null, "labels": [[222, 248, "Method"]]}
{"id": 4532, "text": "Let me begin by saying how honored I am to receive the Gerard Salton Award from SIGIR. I only had the chance to meet Professor Salton once in person, at SIGIR’91 in Chicago, where he invited me as a visiting scholar to Cornell. I could not do that then but had been inspired by his great pioneering work on automatic information retrieval (IR) and the SMART system. He could well have pushed me to a more system-oriented path within IR than the one I actually followed. I’m also humbled by the honour of having been chosen to stand among the previous 11 recipients of this award whose contributions to IR are of greatest value. Honestly, I never believed that I would be giving this talk. As the tradition is, based on several examples by prior Salton Award winners, I’d like to present today a personal reflection on IR. I will begin with some personal history in Information Science and IR and continue by discussing my personal view on the goals and scope of IR. Then I’d like to present some studies seeking to approach those goals and discuss their methodological challenges. I came to the field of IR through Information Science. I started at the University of Tampere aiming at a degree in librarianship. However, I was soon introduced to IR, information behaviour and Information Science through inspiring books by Wilfrid Lancaster, Gerard Salton, and Manfred Kochen. My professor also advised me to study Computer Science, which led me to research on database management systems. Within Information Science, the subfield of user and information needs studies was seen to offer a scientific foundation for the development of libraries. The vast majority of the studies were user studies, or institution-centred studies, focusing on the role of users in the life of libraries. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the Owner/Author. SIGIR '18, July 8–12, 2018, Ann Arbor, MI, USA © 2018 Copyright is held by the owner/author(s). ACM ISBN 978-1-4503-5657-2/18/07. https://doi.org/10.1145/3209978.3210230 I was exposed to ideas by Gernot Wersig (Informationssoziologie), Nick Belkin (the ASK hypothesis), Brenda Dervin (sense-making theory), and D. C. Engelbart (human augmentation research). These led me to view Information Science as a discipline studying people as actors in their work/life – not as users of some information delivery institutions per se, nor as information seekers per se, but as subjects perceiving their own information environment and interacting with it. At the University of Tampere, we pursued this line of research and contributed to the international debate on the transformation of Information Science from institution focus to person / information practises focus. Alongside Carol C. Kuhlthau’s work on the Information Seeking Process in learning tasks, theoretical reflections led me to think that task complexity affects information seeking. In particular, tasks of different complexity require different types of knowledge to be solved, which may be constructed from information available in different types of sources. Quite self-evident once stated, but not that simple to study. Task complexity as a factor in information interaction got quite popular, however. Others have extended its application in IR and seeking studies, e.g. by proposing taxonomies of work tasks and search tasks. Following the rise of test-collection based IR research, we established our IR Lab and the research group FIRE in the beginning of 1990’s. The IR community focused then on developing ranked retrieval systems that scale up to handling large collections. That research was without doubt very important and successful. Still we thought that a broader user-oriented scope would be beneficial to the field. To advance this, we thought, one needs credibility earned through becoming a full member of the community – through carrying out successful research with similar goals. In addition to making use of the TReC test collections, we decided to build a Finnish language test collection. This had two important benefits: one in the development of NLP methods for IR, and the other in the comeback of graded relevance assessments in IR evaluation. The former led us to quite successful in research into applications of NLP methods in IR across a range of languages; crosslanguage IR using a range of approaches across a range of language pairs; and approximate string matching methods for searching with words not covered by other approaches. The latter inspired the development of evaluation metrics based on graded relevance assessments – the Cumulated Gain metrics family, foremost the nDCG metric. We proposed it as a user-oriented metric because it was based on graded relevance, allowed weighing of relevance levels and discounting the gain incurring from documents found late on the result list. There are quite a few proKeynote SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA", "meta": {}, "annotation_approver": null, "labels": [[307, 343, "Task"], [602, 604, "Task"], [962, 964, "Task"], [1104, 1106, "Task"], [889, 891, "Task"], [1246, 1248, "Task"]]}
{"id": 4533, "text": "The observation of social media provides an important complementing source of information about an unfolding event such as a crisis situation. For this purpose we have developed and demonstrate Sover!, a system to monitor real-time dynamic events via Twitter targeting the needs of aid organizations. At its core it builds upon an effective adaptive crawler, which combines two social media streams in a Bayesian inference framework and after each time-window updates the probabilities of whether given keywords are relevant for an event. Sover! also exposes the crawling functionality so a user can actively influence the evolving selection of keywords. The crawling activity feeds a rich dashboard, which enables the user to get a better understanding of a crisis situation as it unfolds in real-time.", "meta": {}, "annotation_approver": null, "labels": [[214, 258, "Task"], [404, 432, "Method"]]}
{"id": 4534, "text": "Speeding up the development process of Web Services, while adhering to high quality software standards is a typical requirement in the software industry. This is why industry specialists usually suggest \"driven by\" development approaches to tackle this problem. In this paper, we propose such a methodology that employs Specification Driven Development and Behavior Driven Development in order to facilitate the phases of Web Service requirements elicitation and specification. Furthermore, we introduce gherkin2OAS, a software tool that aspires to bridge the aforementioned development approaches. Through the suggested methodology and tool, one may design and build RESTful services fast, while ensuring proper functionality.", "meta": {}, "annotation_approver": null, "labels": [[320, 352, "Method"], [357, 384, "Method"], [422, 476, "Task"]]}
{"id": 4535, "text": "In coal-fired power plants, it is critical to improve the operational efficiency of boilers for sustainability. In this work, we formulate real-time boiler control as an optimization problem that looks for the best distribution of temperature in different zones and oxygen content from the flue to improve the boiler's stability and energy efficiency. We employ an efficient algorithm by integrating appropriate machine learning and optimization techniques. We obtain a large dataset collected from a real boiler for more than two months from our industry partner, and conduct extensive experiments to demonstrate the effectiveness and efficiency of the proposed algorithm.", "meta": {}, "annotation_approver": null, "labels": [[476, 563, "Material"]]}
{"id": 4536, "text": "We address the automatic extraction from publications of two key concepts for representing research processes: the concept of research activity and the sequence relation between successive activities. These representations are driven by the Scholarly Ontology, specifically conceived for documenting research processes. Unlike usual named entity recognition and relation extraction tasks, we are facing textual descriptions of activities of widely variable length, while pairs of successive activities often span multiple sentences. We developed and experimented with several sliding window classifiers using Logistic Regression, SVMs, and Random Forests, as well as a two-stage pipeline classifier. Our classifiers employ task-specific features, as well as word, part-of-speech and dependency embeddings, engineered to exploit distinctive traits of research publications written in English. The extracted activities and sequences are associated with other relevant information from publication metadata and stored as RDF triples in a knowledge base. Evaluation on datasets from three disciplines, Digital Humanities, Bioinformatics, and Medicine, shows very promising performance.", "meta": {}, "annotation_approver": null, "labels": [[15, 53, "Task"], [333, 357, "Task"], [362, 381, "Task"], [576, 602, "Method"], [609, 628, "Method"], [630, 634, "Method"], [640, 654, "Method"]]}
{"id": 4537, "text": "Completing knowledge bases (KBs) with missing facts is of great importance, since most existing KBs are far from complete. To this end, many knowledge base completion (KBC) methods have been proposed. However, most existing methods embed each relation into a vector separately, while ignoring the correlations among different relations. Actually, in large-scale KBs, there always exist some relations that are semantically related, and we believe this can help to facilitate the knowledge sharing when learning the embedding of related relations simultaneously. Along this line, we propose a novel KBC model by Multi -Task E mbedding, named MultiE. In this model, semantically related relations are first clustered into the same group, and then learning the embedding of each relation can leverage the knowledge among different relations. Moreover, we propose a three-layer network to predict the missing values of incomplete knowledge triples. Finally, experiments on three popular benchmarks FB15k, FB15k-237 and WN18 are conducted to demonstrate the effectiveness of MultiE against some state-of-the-art baseline competitors.", "meta": {}, "annotation_approver": null, "labels": [[141, 172, "Task"], [885, 943, "Task"], [994, 999, "Material"], [1001, 1010, "Material"], [1015, 1019, "Material"], [0, 26, "Task"], [598, 601, "Task"], [611, 633, "Method"]]}
{"id": 4538, "text": "Brain-Computer Interface (BCI) enables human to communicate with and intuitively control an external device through brain signals. Movement intention recognition paves the path for developing BCI applications. The current state-of-the-art in EEG based BCI usually involves subject-specific adaptation before ready to use. However, the subject-independent scenario, in which a well-trained model is directly applied to new subjects without any pre-calibration, is particularly desired yet rarely explored. In order to fill the gap, we present a Convolutional Attention Model (CAM) for EEG-based human movement intention recognition in the subject-independent scenario. The convolutional network is designed to capture the spatio-temporal features of EEG signals, while the integrated attention mechanism is utilized to focus on the most discriminative information of EEG signals during the period of movement imagination while omitting other less relative parts. Experiments conducted on a real-world EEG dataset containing 55 subjects show that our model is capable of mining the underlying invariant EEG patterns across different subjects and generalizing to unseen subjects. Our model achieves better performance than a series of state-of-the-art and baseline approaches.", "meta": {}, "annotation_approver": null, "labels": [[544, 579, "Method"], [584, 630, "Task"]]}
{"id": 4539, "text": "Recent years have witnessed a widespread increase of rumor news generated by humans and machines. Therefore, tools for investigating rumor news have become an urgent necessity. One useful function of such tools is to see ways a specific topic or event is represented by presenting different points of view from multiple sources. In this paper, we propose Maester, a novel agreement-aware search framework for investigating rumor news. Given an investigative question, Maester will retrieve related articles to that question, assign and display top articles from agree, disagree, and discuss categories to users. Splitting the results into these three categories provides the user a holistic view towards the investigative question. We build Maester based on the following two key observations: (1) relatedness can commonly be determined by keywords and entities occurring in both questions and articles, and (2) the level of agreement between the investigative question and the related news article can often be decided by a few key sentences. Accordingly, we use gradient boosting tree models with keyword/entity matching features for relatedness detection, and leverage recurrent neural network to infer the level of agreement. Our experiments on the Fake News Challenge (FNC) dataset demonstrate up to an order of magnitude improvement of Maester over the original FNC winning solution, for agreement-aware search.", "meta": {}, "annotation_approver": null, "labels": [[119, 143, "Task"], [409, 433, "Task"], [1064, 1093, "Method"], [1136, 1157, "Task"], [1172, 1196, "Method"], [1253, 1286, "Material"]]}
{"id": 4540, "text": "We propose a location prediction method for tweets based on the geographical probability distribution of their terms over a region. In our method, the probabilities are calculated using Kernel Density Estimation (KDE), where the bandwidth of the kernel function for each term is determined separately according to the location indicativeness of the term. Prediction for a new tweet is performed by combining the probability distributions of its terms weighted by their information gain ratio. The method we propose relies on statistical approaches without requiring any parameter tuning. Experiments conducted on three tweet sets from different regions of the world indicate significant improvement in prediction accuracy compared to the state-of-the-art methods.", "meta": {}, "annotation_approver": null, "labels": [[186, 217, "Method"]]}
{"id": 4541, "text": "User engagement plays a central role in companies operating online services, such as search engines, news portals, e-commerce sites, and social networks. A main challenge is to leverage collected knowledge about the daily online behavior of millions of users to understand what engage them short-term and more importantly long-term. The most common way that engagement is measured is through various online metrics, acting as proxy measures of user engagement. This tutorial will review these metrics, their advantages and drawbacks, and their appropriateness to various types of online services. As case studies, we will focus on three types of services, news, search and e-commerce. We will also briefly discuss how to develop better machine learning models to optimize online metrics, and design experiments to test these models.", "meta": {}, "annotation_approver": null, "labels": [[763, 786, "Task"], [736, 752, "Method"]]}
{"id": 4542, "text": "Modern web search engines exploit users' search history to personalize search results, with a goal of improving their service utility on a per-user basis. But it is this very dimension that leads to the risk of privacy infringement and raises serious public concerns. In this work, we propose a client-centered intent-aware query obfuscation solution for protecting user privacy in a personalized web search scenario. In our solution, each user query is submitted with l additional cover queries and corresponding clicks, which act as decoys to mask users' genuine search intent from a search engine. The cover queries are sequentially sampled from a set of hierarchically organized language models to ensure the coherency of fake search intents in a cover search task. Our approach emphasizes the plausibility of generated cover queries, not only to the current genuine query but also to previous queries in the same task, to increase the complexity for a search engine to identify a user's true intent. We also develop two new metrics from an information theoretic perspective to evaluate the effectiveness of provided privacy protection. Comprehensive experiment comparisons with state-of-the-art query obfuscation techniques are performed on the public AOL search log, and the propitious results substantiate the effectiveness of our solution.", "meta": {}, "annotation_approver": null, "labels": [[295, 341, "Task"], [355, 407, "Task"], [1250, 1271, "Material"]]}
{"id": 4543, "text": "We propose a research that aims at improving the effectiveness of case-based retrieval systems through the use of automatically created document-level semantic networks. The proposed research leverages the recent advancements in information extraction and relational learning to revisit and advance the core ideas of concept-centered hypertext models. The automatic extraction of semantic relations from documents --- and their centrality in the creation and exploitation of the documents' semantic networks --- represents our attempt to go one step further than previous approaches.", "meta": {}, "annotation_approver": null, "labels": [[66, 86, "Task"], [114, 168, "Method"], [229, 251, "Task"], [356, 413, "Task"], [446, 507, "Task"]]}
{"id": 4544, "text": "Determining the similarity between two objects is pertinent to many applications. When the basis for similarity is a set of object-to-object relationships, it is natural to rely on graph-theoretic measures. One seminal technique for measuring the structural-context similarity between a pair of graph vertices is SimRank, whose underlying intuition is that two objects are similar if they are connected by similar objects. However, by design, SimRank as well as its variants capture only a single view or perspective of similarity. Meanwhile, in many real-world scenarios, there emerge multiple perspectives of similarity, i.e., two objects may be similar from one perspective, but dissimilar from another. For instance, human subjects may generate varied, yet valid, clusterings of objects. In this work, we propose a graph-theoretic similarity measure that is natively multiperspective. In our approach, the observed object-to-object relationships due to various perspectives are integrated into a unified graph-based representation, stylised as a hypergraph to retain the distinct perspectives. We then introduce a novel model for learning and reflecting diverse similarity perceptions given the hypergraph, yielding the similarity score between any pair of objects from any perspective. In addition to proposing an algorithm for computing the similarity scores, we also provide theoretical guarantees on the convergence of the algorithm. Experiments on public datasets show that the proposed model deals better with multiperspectivity than the baselines.", "meta": {}, "annotation_approver": null, "labels": [[313, 320, "Method"], [443, 450, "Method"], [819, 853, "Method"], [181, 205, "Method"], [233, 276, "Task"], [1457, 1472, "Material"], [16, 46, "Task"]]}
{"id": 4545, "text": "Public bibliographic databases hold invaluable data about the academic environment. However, researcher affiliation information is frequently missing or outdated. We propose a statistical data extraction method to acquire affiliation information directly from university websites and solve the name extraction task in general. Previous approaches to web data extraction either lack in flexibility, because wrappers do not generalize well across websites, or they lack in precision, because domain agnostic methods neglect useful properties of this particular application domain. Our statistical approach solves the name extraction task with a good tradeoff between generality and precision. We conducted experiments over a collection of 152 faculty web pages in multiple languages from universities in 49 countries and obtained 94.37 % precision, 97.61% recall, and 0.9596 F-measure.", "meta": {}, "annotation_approver": null, "labels": [[176, 210, "Method"], [214, 279, "Task"], [836, 845, "Metric"], [854, 860, "Metric"], [741, 758, "Material"], [0, 30, "Material"], [350, 369, "Task"], [471, 480, "Metric"], [680, 689, "Metric"], [294, 309, "Task"], [615, 630, "Task"], [873, 882, "Metric"]]}
{"id": 4546, "text": "Generative adversarial nets (GANs) have been widely studied during the recent development of deep learning and unsupervised learning. With an adversarial training mechanism, GAN manages to train a generative model to fit the underlying unknown real data distribution under the guidance of the discriminative model estimating whether a data instance is real or generated. Such a framework is originally proposed for fitting continuous data distribution such as images, thus it is not straightforward to be directly applied to information retrieval scenarios where the data is mostly discrete, such as IDs, text and graphs. In this tutorial, we focus on discussing the GAN techniques and the variants on discrete data fitting in various information retrieval scenarios. (i) We introduce the fundamentals of GAN framework and its theoretic properties; (ii) we carefully study the promising solutions to extend GAN onto discrete data generation; (iii) we introduce IRGAN, the fundamental GAN framework of fitting single ID data distribution and the direct application on information retrieval; (iv) we further discuss the task of sequential discrete data generation tasks, e.g., text generation, and the corresponding GAN solutions; (v) we present the most recent work on graph/network data fitting with node embedding techniques by GANs. Meanwhile, we also introduce the relevant open-source platforms such as IRGAN and Texygen to help audience conduct research experiments on GANs in information retrieval. Finally, we conclude this tutorial with a comprehensive summarization and a prospect of further research directions for GANs in information retrieval.", "meta": {}, "annotation_approver": null, "labels": [[0, 34, "Method"], [93, 106, "Method"], [525, 546, "Task"], [735, 756, "Task"], [1067, 1088, "Task"], [1126, 1161, "Task"], [1175, 1190, "Task"], [1300, 1314, "Method"], [1482, 1503, "Task"], [1633, 1654, "Task"], [110, 132, "Method"]]}
{"id": 4547, "text": "Multi-task Multi-view (MTMV) learning has recently undergone noticeable development for dealing with heterogeneous data. To exploit information from both related tasks and related views, a common strategy is to model task relatedness and view consistency separately. The drawback of this strategy is that it did not consider the interactions between tasks and views. To remedy this, we propose a novel method, racBFA, by adding rank constraints to asymmetric bilinear factor analyzers (aBFA). We then adapt racBFA to our MTMV learning problem and design a new MTMV learning algorithm, racMTMV. We evaluated racMTMV on 3 real-world data sets. The experimental results demonstrated the effectiveness of our proposed method.", "meta": {}, "annotation_approver": null, "labels": [[0, 37, "Task"], [421, 491, "Method"]]}
{"id": 4548, "text": "The Bloomberg Terminal is the leading source of information and news in the finance industry. Through hundreds of functions that provide access to a vast wealth of structured and semi-structured data, the terminal is able to satisfy a wide range of information needs. Users can find what they need by constructing queries, plotting charts, creating alerts, and so on. Until recently, most queries to the terminal were constructed through dedicated GUIs. For instance, if users wanted to screen for technology companies that met certain criteria, they would specify the criteria by filling out a form via a sequence of interactions with GUI elements such as drop-down lists, checkboxes, radio and toggle buttons, etc. To facilitate information retrieval in the terminal, we are equipping it with the ability to understand and answer queries expressed in natural language. Our QA (question answering) systems map structurally complex questions like the above to a logical meaning representation which can then be translated to an executable query language (such as SQL or SPARQL). At that point we can execute the queries against a suitable back end, obtain the results, and present them to the users. Adding a natural-language interface to a data repository introduces usability challenges of its own, chief amongst them being this: How can the user know what the system can and cannot understand and answer (without needing to undergo extensive training)? We can unpack this question into two separate parts: 1) How can we convey the full range of the system's abilities? 2) How can we convey its limitations? We use auto-complete as a tool to help meet both challenges. Specifically, the first question pertains to the general issue of discoverability: We want at least some of the suggested completions to act as vehicles for discovering data and functionality of which users may have not been previously aware. The second question pertains to expectation management. Naturally, no QA system can attain perfect performance; limiting factors include representational shortcomings and various kinds of incompleteness of the underlying data sources, as well as NLP technology limitations. We want to stop generating completions as a signal indicating that we are not able to understand and/or answer what is being typed.", "meta": {}, "annotation_approver": null, "labels": [[731, 752, "Task"], [924, 941, "Material"], [164, 199, "Material"], [879, 897, "Task"]]}
{"id": 4549, "text": "This paper presents a new method for constructing an optimal feature set from sequential data. It creates a dictionary of n-grams of variable length (we call them v-grams), based on the minimum description length principle. The proposed method is a dictionary coder and works simultaneously as both a compression algorithm and as unsupervised feature extraction. The length of constructed v-grams is not limited by any bound and exceeds 100 characters in provided experiments. Constructed v-grams can be used for any sequential data analysis and allows transfer bag-of-word techniques to non-text data types. The method demonstrates a high compression rate on various real-life datasets. Extracted features generate a practical basis for text classification, that shows competitive results on standard text classification collections without using the text structure. Combining extracted character v-grams with the words from the original text we achieved substantially better classification quality than on words or v-grams alone.", "meta": {}, "annotation_approver": null, "labels": [[37, 93, "Task"], [249, 265, "Method"], [301, 322, "Method"], [330, 361, "Method"], [517, 541, "Task"], [668, 686, "Material"], [738, 757, "Task"]]}
{"id": 4550, "text": "Learning with pairwise ranking methods for implicit feedback datasets has shown promising results as compared to pointwise ranking methods for recommendation tasks. However, there is limited effort in scaling the pairwise ranking methods in a large scale distributed setting. In this paper we address the scalability aspect of a pairwise ranking method using Factorization Machines in distributed settings. Our proposed method is based on a block partitioning of the model parameters so that each distributed worker runs stochastic gradient updates on an independent block. We developed a dynamic block creation and exchange strategy by utilizing the frequency of occurrence of a feature in the local training data of a worker. Empirical evidence on publicly available benchmark datasets indicates that the proposed method scales better than the static block based methods and outperforms competing state-of-the-art methods.", "meta": {}, "annotation_approver": null, "labels": [[14, 30, "Method"], [143, 157, "Task"], [329, 345, "Method"], [769, 787, "Material"], [213, 229, "Method"], [113, 130, "Method"]]}
{"id": 4551, "text": "Mobile devices (e.g., smartphones) play a crucial role in our daily lives nowadays. People rely heavily on mobile devices for searching online, sending emails, chatting with friends, etc. As a result, input efficiency becomes increasingly important for real-time communication on mobile devices. Due to the small size of the screen on mobile devices, however, it is oftentimes frustrating for users to correct or update the input sequences on an even smaller input area on the screen. This often causes poor user experience. In this paper, we focus on improving the input efficiency on mobile devices to offer better user experience. In order to achieve efficient input, there are multiple challenges: 1) how to employ a single, unified representation of the keyboard layouts for different input languages; 2) how to build a framework to correct a mistouch immediately and predict the coming input texts (words or phrases) effectively; 3) how to deploy and evaluate the model on mobile devices with limited computational power. To address these challenges, we introduce \\em FastInput to improve the user input efficiency on mobile devices. Three key techniques are developed in FastInput -- layout modeling, instant mistouch correction and user input text prediction. We also design solutions for efficient deployment and evaluation of FastInput on mobile devices. The proposed FastInput achieves higher efficiency compared to the traditional input system over millions of user input sequences in different languages.", "meta": {}, "annotation_approver": null, "labels": [[552, 600, "Task"], [1191, 1206, "Method"], [1208, 1235, "Method"], [1087, 1138, "Task"], [1245, 1266, "Method"], [1473, 1493, "Material"]]}
{"id": 4552, "text": "Job recommendation is an important task for the modern recruitment industry. An excellent job recommender system not only enables to recommend a higher paying job which is maximally aligned with the skill-set of the current job, but also suggests to acquire few additional skills which are required to assume the new position. In this work, we created three types of information net- works from the historical job data: (i) job transition network, (ii) job-skill network, and (iii) skill co-occurrence network. We provide a representation learning model which can utilize the information from all three networks to jointly learn the representation of the jobs and skills in the shared k-dimensional latent space. In our experiments, we show that by jointly learning the representation for the jobs and skills, our model provides better recommendation for both jobs and skills. Additionally, we also show some case studies which validate our claims.", "meta": {}, "annotation_approver": null, "labels": [[0, 18, "Task"], [424, 446, "Method"], [453, 470, "Method"], [482, 509, "Method"], [524, 553, "Method"]]}
{"id": 4553, "text": "Recommender systems are a key component of music sharing platforms, which suggest musical recordings a user might like. People often have implicit preferences while listening to music, though these preferences might not always be the same while they listen to music at different times. For example, a user might be interested in listening to songs of only a particular artist at some time, and the same user might be interested in the top-rated songs of a genre at another time. In this paper we try to explicitly model the short term preferences of the user with the help of Last.fm tags of the songs the user has listened to. With a session defined as a period of activity surrounded by periods of inactivity, we introduce the concept of a subsession, which is that part of the session wherein the preference of the user does not change much. We assume the user preference might change within a session and a session might have multiple subsessions. We use our modelling of the user preferences to generate recommendations for the next song the user might listen to. Experiments on the user listening histories taken from Last.fm indicate that this approach beats the present methodologies in predicting the next recording a user might listen to.", "meta": {}, "annotation_approver": null, "labels": [[0, 18, "Task"], [1124, 1131, "Material"], [74, 100, "Task"], [576, 588, "Material"], [503, 558, "Method"]]}
{"id": 4554, "text": "We address the problem of constructing a knowledge base of entity-oriented search intents. Search intents are defined on the level of entity types, each comprising of a high-level intent category (property, website, service, or other), along with a cluster of query terms used to express that intent. These machine-readable statements can be leveraged in various applications, e.g., for generating entity cards or query recommendations. By structuring service-oriented search intents, we take one step towards making entities actionable. The main contribution of this paper is a pipeline of components we develop to construct a knowledge base of entity intents. We evaluate performance both component-wise and end-to-end, and demonstrate that our approach is able to generate high-quality data.", "meta": {}, "annotation_approver": null, "labels": [[440, 483, "Method"], [26, 89, "Task"]]}
{"id": 4555, "text": "Crowdsourcing has become a popular paradigm in data curation, annotation and evaluation for many artificial intelligence and information retrieval applications. Considerable efforts have gone into devising effective quality control mechanisms that identify or discourage cheat submissions in an attempt to improve the quality of noisy crowd judgments. Besides purposeful cheating, there is another source of noise that is often alluded to but insufficiently studied: Cognitive biases. \n This paper investigates the prevalence and effect size of a range of common cognitive biases on a standard relevance judgment task. Our experiments are based on three sizable publicly available document collections and note significant detrimental effects on annotation quality, system ranking and the performance of derived rankers when task design does not account for such biases.", "meta": {}, "annotation_approver": null, "labels": [[0, 13, "Method"], [125, 146, "Task"], [47, 60, "Task"], [97, 120, "Method"], [594, 612, "Task"]]}
{"id": 4556, "text": "In this paper, we introduce a novel multimodal fashion search paradigm where e-commerce data is searched with a multimodal query composed of both an image and text. In this setting, the query image shows a fashion product that the user likes and the query text allows to change certain product attributes to fit the product to the user's desire. Multimodal search gives users the means to clearly express what they are looking for. This is in contrast to current e-commerce search mechanisms, which are cumbersome and often fail to grasp the customer's needs. Multimodal search requires intermodal representations of visual and textual fashion attributes which can be mixed and matched to form the user's desired product, and which have a mechanism to indicate when a visual and textual fashion attribute represent the same concept. With a neural network, we induce a common, multimodal space for visual and textual fashion attributes where their inner product measures their semantic similarity. We build a multimodal retrieval model which operates on the obtained intermodal representations and which ranks images based on their relevance to a multimodal query. We demonstrate that our model is able to retrieve images that both exhibit the necessary query image attributes and satisfy the query texts. Moreover, we show that our model substantially outperforms two state-of-the-art retrieval models adapted to multimodal fashion search.", "meta": {}, "annotation_approver": null, "labels": [[77, 92, "Material"], [346, 363, "Task"], [560, 577, "Task"], [840, 854, "Method"], [976, 995, "Metric"], [1008, 1034, "Method"], [1385, 1401, "Method"], [1413, 1438, "Task"]]}
{"id": 4557, "text": "We investigate the alignment of international attention of news media organizations within 193 countries with the expressed international interests of the public within those same countries from March 7, 2016 to April 14, 2017. We collect fourteen months of longitudinal data of online news from Unfiltered News and web search volume data from Google Trends and build a multiplex network of media attention and public attention in order to study its structural and dynamic properties. Structurally, the media attention and the public attention are both similar and different depending on the resolution of the analysis. For example, we find that 63.2% of the country-specific media and the public pay attention to different countries, but local attention flow patterns, which are measured by network motifs, are very similar. We also show that there are strong regional similarities with both media and public attention that is only disrupted by significantly major worldwide incidents (e.g., Brexit). Using Granger causality, we show that there are a substantial number of countries where media attention and public attention are dissimilar by topical interest. Our findings show that the media and public attention toward specific countries are often at odds, indicating that the public within these countries may be ignoring their country-specific news outlets and seeking other online sources to address their media needs and desires.", "meta": {}, "annotation_approver": null, "labels": [[19, 83, "Task"], [370, 427, "Method"], [296, 311, "Material"], [344, 357, "Material"]]}
{"id": 4558, "text": "Multi-armed bandit algorithms have become a reference solution for handling the explore/exploit dilemma in recommender systems, and many other important real-world problems, such as display advertisement. However, such algorithms usually assume a stationary reward distribution, which hardly holds in practice as users' preferences are dynamic. This inevitably costs a recommender system consistent suboptimal performance. In this paper, we consider the situation where the underlying distribution of reward remains unchanged over (possibly short) epochs and shifts at unknown time instants. In accordance, we propose a contextual bandit algorithm that detects possible changes of environment based on its reward estimation confidence and updates its arm selection strategy respectively. Rigorous upper regret bound analysis of the proposed algorithm demonstrates its learning effectiveness in such a non-trivial environment. Extensive empirical evaluations on both synthetic and real-world datasets for recommendation confirm its practical utility in a changing environment.", "meta": {}, "annotation_approver": null, "labels": [[0, 29, "Method"], [182, 203, "Task"], [67, 126, "Task"], [620, 647, "Method"], [1004, 1018, "Task"], [369, 387, "Task"]]}
{"id": 4559, "text": "In probabilistic BM25, term frequency normalization is one of the key components. It is often controlled by parameters $k_1$ and b , which need to be optimized for each given data set. In this paper, we assume and show empirically that term frequency normalization should be specific with query length in order to optimize retrieval performance. Following this intuition, we first propose a new term frequency normalization with query length for probabilistic information retrieval, namely \\textttBM25\\tiny QL . Then \\textttBM25\\tiny QL is incorporated into the state-of-the-art models CRTER riptsize 2 and LDA-BM25, denoted as $\\textttCRTER riptsize 2 ^\\texttt\\tiny QL $ and \\textttLDA-BM25\\tiny QL respectively. A series of experiments show that our proposed approaches \\textttBM25\\tiny QL , $\\textttCRTER riptsize 2 ^\\texttt\\tiny QL $ and \\textttLDA-BM25\\tiny QL are comparable to BM25, CRTER riptsize 2 and LDA-BM25 with the optimal b setting in terms of MAP on all the data sets.", "meta": {}, "annotation_approver": null, "labels": [[23, 51, "Method"], [236, 264, "Method"], [314, 344, "Task"], [395, 441, "Method"], [446, 481, "Task"], [959, 962, "Metric"]]}
{"id": 4560, "text": "The proliferation of online social networks and the spread of smart mobile devices enable the collection of information related to a multitude of users’ activities. These networks, where every node is associated with a type of action and a frequency, are usually referred to as activity networks. Examples of such networks include road networks, where the nodes are intersections and the edges are road segments. Each node is associated with a number of geolocated actions that users of an online platform took in its vicinity. In these networks, we define a prize-collecting subgraph to be a connected set of nodes, which exhibits high levels of activity, and is compact, i.e., the nodes are close to each other. The k-PCSubgraphs problem we address in this paper is defined as follows: given an activity network and an integer k , identify k non-overlapping and connected subgraphs of the network such that the nodes of each subgraph are close to each other, and the total number of actions they are associated with is high. Here, we define and study two new variants of the k-PCSubgraphs problem, where the subgraphs of interest are tours and paths. Since both these problems are NP-hard, we provide approximate and heuristic algorithms that run in time nearly-linear to the number of edges. In our experiments, we use real activity networks obtained by combining road networks and projecting on them user activity from Twitter and Flickr. Our experimental results demonstrate both the efficiency and the practical utility of our methods. ACM Reference Format: SofiaMaria Nikolakaki, CharalamposMavroforakis, Alina Ene, and Evimaria Terzi. 2018. Mining Tours and Paths in Activity Networks. InWWW 2018: The 2018 Web Conference, April 23–27, 2018, Lyon, France. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3178876.3186112", "meta": {}, "annotation_approver": null, "labels": [[1203, 1239, "Method"], [1423, 1430, "Material"], [1435, 1441, "Material"], [1077, 1098, "Task"], [718, 739, "Task"], [832, 898, "Task"]]}
{"id": 4561, "text": "Open data refers to data that everyone can freely use, reuse and redistribute. A number of open data is released by various organisations, governments or communities. However, it is limited to discover datasets that users want, since most of data portals allow to search their datasets based on simple keywords using file names or descriptions, etc. This paper proposes a novel way for discovering disclosed government datasets by using linked data technologies. For achieving this objective, a set of datasets is collected from the public data portal in Korea, and all of data fields are extracted and transformed into linked data using an ontology model. We also provide a simple evaluation, which compares a search performance between the portal and the proposed method.", "meta": {}, "annotation_approver": null, "labels": [[386, 427, "Task"], [641, 655, "Method"], [533, 560, "Material"], [437, 461, "Method"]]}
{"id": 4562, "text": "As smart cities infrastructures mature, data becomes a valuable asset which can radically improve city services and tools. Registration, acquisition and utilization of data, which will be transformed into smart services, are becoming more necessary than ever. Online social networks with their enormous momentum are one of the main sources of urban data offering heterogeneous real-time data at a minimal cost. However, various types of attacks often appear on them, which risk users' privacy and affect their online trust. The purpose of this article is to investigate how risks on online social networks affect smart cities and study the differences between privacy and security threats with regard to smart people and smart living dimensions.", "meta": {}, "annotation_approver": null, "labels": [[558, 625, "Task"], [260, 282, "Material"], [630, 688, "Task"]]}
{"id": 4563, "text": "The problem of ad-hoc structured document retrieval arises in many information access scenarios, from Web to product search. Yet neither deep neural networks, which have been successfully applied to ad-hoc information retrieval and Web search, nor the attention mechanism, which has been shown to significantly improve the performance of deep neural networks on natural language processing tasks, have been explored in the context of this problem. In this paper, we propose a deep neural architecture for ad-hoc structured document retrieval, which utilizes attention mechanism to determine important phrases in keyword queries as well as the relative importance of matching those phrases in different fields of structured documents. Experimental evaluation on publicly available collections for Web document, product and entity retrieval from knowledge graphs indicates superior retrieval accuracy of the proposed neural architecture relative to both state-of-the-art neural architectures for ad-hoc document retrieval and probabilistic models for ad-hoc structured document retrieval.", "meta": {}, "annotation_approver": null, "labels": [[15, 51, "Task"], [137, 157, "Method"], [199, 227, "Task"], [232, 242, "Task"], [252, 271, "Method"], [338, 358, "Method"], [109, 123, "Task"], [476, 500, "Method"], [505, 541, "Task"], [558, 577, "Method"], [810, 838, "Task"], [780, 808, "Material"], [844, 860, "Material"], [890, 898, "Metric"], [994, 1019, "Task"], [1024, 1044, "Method"], [362, 389, "Task"], [581, 619, "Task"], [1049, 1085, "Task"]]}
{"id": 4564, "text": "Signed directed networks with positive or negative links convey rich information such as like or dislike, trust or distrust. Existing work of sign prediction mainly focuses on triangles (triadic nodes) motivated by balance theory to predict positive and negative links. However, real-world signed directed networks can contain a good number of \"bridge'' edges which, by definition, are not included in any triangles. Such edges are ignored in previous work, but may play an important role in signed directed network analysis.%Such edges serve as fundamental building blocks and may play an important role in signed network analysis.\n In this paper, we investigate the problem of learning representations for signed directed networks. We present a novel deep learning approach to incorporating two social-psychologic theories, balance and status theories, to model both triangles and \"bridge'' edges in a complementary manner. The proposed framework learns effective embeddings for nodes and edges which can be applied to diverse tasks such as sign prediction and node ranking. Experimental results on three real-world datasets of signed directed social networks verify the essential role of \"bridge\" edges in signed directed network analysis by achieving the state-of-the-art performance.", "meta": {}, "annotation_approver": null, "labels": [[608, 631, "Task"], [679, 732, "Task"], [753, 766, "Method"], [1043, 1058, "Task"], [1063, 1075, "Task"], [1209, 1241, "Task"], [142, 157, "Task"], [290, 314, "Method"], [233, 268, "Task"], [1107, 1126, "Material"]]}
{"id": 4565, "text": "In an increasingly digital world, identifying signs of online extremism sits at the top of the priority list for counter-extremist agencies. Researchers and governments are investing in the creation of advanced information technologies to identify and counter extremism through intelligent large-scale analysis of online data. However, to the best of our knowledge, these technologies are neither based on, nor do they take advantage of, the existing theories and studies of radicalisation. In this paper we propose a computational approach for detecting and predicting the radicalisation influence a user is exposed to, grounded on the notion of 'roots of radicalisation' from social science models. This approach has been applied to analyse and compare the radicalisation level of 112 pro-ISIS vs.112 \"general\" Twitter users. Our results show the effectiveness of our proposed algorithms in detecting and predicting radicalisation influence, obtaining up to 0.9 F-1 measure for detection and between 0.7 and 0.8 precision for prediction. While this is an initial attempt towards the effective combination of social and computational perspectives, more work is needed to bridge these disciplines, and to build on their strengths to target the problem of online radicalisation.", "meta": {}, "annotation_approver": null, "labels": [[964, 967, "Metric"], [34, 71, "Task"], [893, 942, "Task"], [545, 598, "Task"], [1014, 1023, "Metric"]]}
{"id": 4566, "text": "We present CoNEREL, a system for collective named entity recognition and entity linking focusing on news articles and readers' comments. Different from other systems, CoNEREL processes articles and comments in batch mode, to make the best use of the shared contexts of multiple news stories and their comments. Particularly, a news article provides context for all its comments. To improve named entity recognition, CoNEREL utilizes co-reference of mentions to refine their class labels ( e.g. , person, location). To link the recognized entities to Wikipedia, our system implements Pair-Linking, a state-of-the-art entity linking algorithm. Furthermore, CoNEREL provides an interactive visualization of the Pair-Linking process. From the visualization, one can understand how Pair-Linking achieves decent linking performance through iterative evidence building, while being extremely fast and efficient. The graph formed by the Pair-Linking process naturally becomes a good summary of entity relations, making CoNEREL a useful tool to study the relationships between the entities mentioned in an article, as well as the ones that are discussed in its comments.", "meta": {}, "annotation_approver": null, "labels": [[433, 457, "Method"], [550, 559, "Material"], [100, 114, "Material"], [118, 135, "Material"], [185, 193, "Material"], [198, 206, "Material"], [278, 290, "Material"], [301, 309, "Material"], [583, 595, "Method"], [616, 630, "Task"], [708, 720, "Method"], [777, 789, "Method"], [929, 941, "Method"], [327, 339, "Material"], [369, 377, "Material"], [1097, 1104, "Material"], [33, 68, "Task"], [73, 87, "Task"], [390, 414, "Task"], [1152, 1160, "Material"]]}
{"id": 4567, "text": "When a message, such as a piece of news, spreads in social networks, how can we classify it into categories of interests, such as genuine or fake news? Classification of social media content is a fundamental task for social media mining, and most existing methods regard it as a text categorization problem and mainly focus on using content features, such as words and hashtags. However, for many emerging applications like fake news and rumor detection, it is very challenging, if not impossible, to identify useful features from content. For example, intentional spreaders of fake news may manipulate the content to make it look like real news. To address this problem, this paper concentrates on modeling the propagation of messages in a social network. Specifically, we propose a novel approach, TraceMiner, to (1) infer embeddings of social media users with social network structures; and (2) utilize an LSTM-RNN to represent and classify propagation pathways of a message. Since content information is sparse and noisy on social media, adopting TraceMiner allows to provide a high degree of classification accuracy even in the absence of content information. Experimental results on real-world datasets show the superiority over state-of-the-art approaches on the task of fake news detection and news categorization.", "meta": {}, "annotation_approver": null, "labels": [[152, 190, "Task"], [217, 236, "Task"], [800, 810, "Method"], [819, 888, "Method"], [898, 977, "Method"], [1051, 1061, "Method"], [1112, 1120, "Metric"], [1278, 1297, "Task"], [279, 298, "Task"], [359, 364, "Material"], [369, 377, "Material"], [438, 453, "Task"], [699, 756, "Task"], [1302, 1321, "Task"]]}
{"id": 4568, "text": "Web APIs are a prominent source of machine-readable information. We hypothesize that harnessing the Semantic Web standards to enable automatic combination of Linked Data and non-RDF Web APIs data could trigger novel cross-fertilization scenarios. To achieve this goal, we define the SPARQLMicro-Service architecture. A SPARQL micro-service is a lightweight, task-specific SPARQL endpoint that provides access to a small, resource-centric, virtual graph, while dynamically assigning dereferenceable URIs to Web API resources that do not have URIs beforehand. The graph is delineated by the Web API service being wrapped, the arguments passed to this service, and the restricted types of RDF triples that this SPARQLmicro-service is designed to spawn. In this context, we argue that full SPARQL expressiveness can be supported efficiently without jeopardizing servers availability. Eventually, we believe that an ecosystem of SPARQL micro-services could emerge from independent service providers, enabling Linked Data-based applications to glean pieces of data from a wealth of distributed, scalable and reliable services. We describe an experimentation where we dynamically augment biodiversity-related Linked Data with data from Flickr, MusicBrainz and the Macauley scientific media library.", "meta": {}, "annotation_approver": null, "labels": [[283, 315, "Method"], [1229, 1235, "Material"], [1237, 1248, "Material"], [133, 195, "Task"], [1257, 1290, "Material"]]}
{"id": 4569, "text": "Helpdesk is a key component of any large IT organization, where users can log a ticket about any issue they face related to IT infrastructure, administrative services, human resource services, etc. Normally, users have to assign appropriate set of labels to a ticket so that it could be routed to right domain expert who can help resolve the issue. In practice, the number of labels are very large and organized in form of a tree. It is non-trivial to describe the issue completely and attach appropriate labels unless one knows the cause of the problem and the related labels. Sometimes domain experts discuss the issue with the users and change the ticket labels accordingly, without modifying the ticket description. This results in inconsistent and badly labeled data, making it hard for supervised algorithms to learn from. In this paper, we propose a novel approach of creating a conversational helpdesk system, which will ask relevant questions to the user, for identification of the right category and will then raise a ticket on users' behalf. We use attention based seq2seq model to assign the hierarchical categories to tickets. We use a slot filling model to help us decide what questions to ask to the user, if the top-k model predictions are not consistent. We also present a novel approach to generate training data for the slot filling model automatically based on attention in the hierarchical classification model. We demonstrate via a simulated user that the proposed approach can give us a significant gain in accuracy on ticket-data without asking too many questions to users. Finally, we also show that our seq2seq model is as versatile as other approaches on publicly available datasets, as state of the art approaches.", "meta": {}, "annotation_approver": null, "labels": [[792, 813, "Method"], [875, 916, "Task"], [1060, 1089, "Method"], [1093, 1138, "Task"], [1149, 1167, "Method"], [1339, 1357, "Method"], [1530, 1538, "Metric"], [1629, 1642, "Method"], [1308, 1330, "Task"], [1381, 1431, "Method"]]}
{"id": 4570, "text": "1 KE4WOT CHALLENGE OVERVIEW The Web of Things (WoT)1 is an extension of the Internet of Things (IoT) to ease the access to data generated by things/devices using the benefits of Web technologies [6]. Data is exploited by WoT applications to monitor healthcare or even control home automation devices. The purpose of the Knowledge Extraction for the Web of Things (KE4WoT) challenge2 is to automatically extract the relevant knowledge from already designed smart WoT applications in various applicative domains. Those applications design and release Knowledge Bases (e.g., datasets and/or models) on the web. Standardizations have a growing interest in designing models to represent devices and generated data.W3C Semantic Sensor Networks (SSN) [3] is the first initiative to address interoperability issues to describe sensor networks through an ontology. Sensors and devices are required to build WoT applications. The last release of the SSN ontology3 became a W3C recommendation in October 2017. It is a joint contribution with the Open Geospatial Consortium (OGC) standard, extending and improving the SSN ontology published in 2011. W3CWeb of Things (WoT) Interest Group is designing a vocabulary to describe interactions between objects through the Web, a potential implementation is the WoT ontology4. A \"Remote health monitoring system\"5 scenario has been designed among several use cases.OneM2M, an international standard for Machine-to-Machine (M2M) designed the OneM2M ontology6. OneM2M extends the European ETSI M2M standard. At the current date of writing, neither WoT ontology nor OneM2M ontology are aligned with W3C ontologies.", "meta": {}, "annotation_approver": null, "labels": [[241, 259, "Task"], [268, 299, "Task"], [320, 340, "Task"], [389, 478, "Task"]]}
{"id": 4571, "text": "The first International Workshop on Heterogeneous Networks Analysis and Mining is held in Los Angeles, California, USA on February 9th, 2018 and is co-located with the 11th ACM International Conference on Web Search and Data Mining. The goal of this workshop is to bring together computing researchers and practitioners to address challenges in the mining and analysis of real-world heterogeneous networks. This workshop has an exciting program that spans a number of subareas including: graph mining, learning from structured data, statistical relational learning, and network science in general. The program includes six invited speakers, lively discussion on emerging topics, and presentations of several original research papers.", "meta": {}, "annotation_approver": null, "labels": [[349, 405, "Task"], [488, 500, "Task"], [502, 531, "Task"], [533, 564, "Task"], [35, 78, "Task"]]}
{"id": 4572, "text": "Adverse drug reactions (ADRs) are one of the leading causes of mortality in health care. Current ADR surveillance systems are often associated with a substantial time lag before such events are officially published. On the other hand, online social media such as Twitter contain information about ADR events in real-time, much before any official reporting. Current state-of-the-art methods in ADR mention extraction use Recurrent Neural Networks (RNN), which typically need large labeled corpora. Towards this end, we propose a semi-supervised method based on co-training which can exploit a large pool of unlabeled tweets to augment the limited supervised training data, and as a result enhance the performance. Experiments with ∼0.1M tweets show that the proposed approach outperforms the state-of-the-art methods for the ADR mention extraction task by ∼5% in terms of F1 score.", "meta": {}, "annotation_approver": null, "labels": [[263, 270, "Material"], [394, 416, "Task"], [421, 452, "Method"], [737, 743, "Material"], [824, 847, "Task"], [872, 880, "Metric"], [529, 551, "Method"]]}
{"id": 4573, "text": "Echo chambers, i.e., situations where one is exposed only to opinions that agree with their own, are an increasing concern for the political discourse in many democratic countries. This paper studies the phenomenon of political echo chambers on social media. We identify the two components in the phenomenon: the opinion that is shared (“echo”), and the place that allows its exposure (“chamber” — the social network), and examine closely at how these two components interact. We de ne a production and consumption measure for social-media users, which captures the political leaning of the content shared and received by them. By comparing the two, we nd that Twitter users are, to a large degree, exposed to political opinions that agree with their own. We also nd that users who try to bridge the echo chambers, by sharing content with diverse leaning, have to pay a “price of bipartisanship” in terms of their network centrality and content appreciation. In addition, we study the role of “gatekeepers,” users who consume content with diverse leaning but produce partisan content (with a single-sided leaning), in the formation of echo chambers. Finally, we apply these ndings to the task of predicting partisans and gatekeepers from social and content features. While partisan users turn out relatively easy to identify, gatekeepers prove to be more challenging. ACM Reference format: Kiran Garimella, Gianmarco De Francisci Morales, Aristides Gionis, and Michael Mathioudakis. 2018. Political Discourse on Social Media: Echo Chambers, Gatekeepers, and the Price of Bipartisanship. In Proceedings of WWW ’18, Lyon, France, April 23–27, 2018, 10 pages. DOI: 10.1145/nnnnnnn.nnnnnnn", "meta": {}, "annotation_approver": null, "labels": [[975, 1007, "Task"], [192, 257, "Task"], [914, 932, "Method"], [661, 668, "Material"], [527, 545, "Material"], [1196, 1265, "Task"]]}
{"id": 4574, "text": "Systematic reviews, in particular medical systematic reviews, are time consuming and costly to produce but are of value for clinical decision making, policy, and regulations. The largest contributing factors to the time and monetary costs are the searching (including the formulation of queries) and screening processes. These initial processes involve researchers reading the abstracts of thousands and sometimes hundreds of thousands of research articles to determine if the retrieved articles should be included or excluded from the systematic review. This research explores automatic methodologies to reduce the workload relating to the searching and initial screening processes. The objective of this research is to use Information Retrieval techniques to improve the retrieval of literature for medical systematic reviews.", "meta": {}, "annotation_approver": null, "labels": [[0, 18, "Task"], [124, 173, "Task"], [725, 746, "Task"], [605, 682, "Task"], [761, 827, "Task"]]}
{"id": 4575, "text": "A recently-introduced product of Comcast, a large cable company in the United States, is a \"voice remote\" that accepts spoken queries from viewers. We present an analysis of a large query log from this service to answer the question: \"What do viewers say to their TVs?\" In addition to a descriptive characterization of queries and sessions, we describe two complementary types of analyses to support query understanding. First, we propose a domain-specific intent taxonomy to characterize viewer behavior: as expected, most intents revolve around watching programs---both direct navigation as well as browsing---but there is a non-trivial fraction of non-viewing intents as well. Second, we propose a domain-specific tagging scheme for labeling query tokens, that when combined with intent and program prediction, provides a multi-faceted approach to understand voice queries directed at entertainment systems.", "meta": {}, "annotation_approver": null, "labels": [[235, 267, "Task"], [441, 472, "Method"], [701, 731, "Method"], [182, 191, "Material"], [400, 419, "Task"]]}
{"id": 4576, "text": "We present the first version of the complex benchmark of the Ontology Alignment Evaluation Initiative campaigns. This benchmark is composed of four datasets from different domains (conference, hydrology, geoscience and agronomy) and covers different evaluation strategies.", "meta": {}, "annotation_approver": null, "labels": [[61, 101, "Task"], [181, 191, "Material"], [193, 202, "Material"], [204, 214, "Material"], [219, 227, "Material"]]}
{"id": 4577, "text": "Neural word embedding approaches, due to their ability to capture semantic meanings of vocabulary terms, have recently gained attention of the information retrieval (IR) community and have shown promising results in improving ad hoc retrieval performance. It has been observed that these approaches are sensitive to various choices made during the learning of word embeddings and their usage, often leading to poor reproducibility. We study the effect of varying following two parameters, viz., i) the term normalization and ii) the choice of training collection, on ad hoc retrieval performance with word2vec and fastText embeddings. We present quantitative estimates of similarity of word vectors obtained under different settings, and use embeddings based query expansion task to understand the effects of these parameters on IR effectiveness.", "meta": {}, "annotation_approver": null, "labels": [[435, 487, "Task"], [143, 169, "Task"], [0, 21, "Method"], [226, 242, "Task"], [567, 583, "Task"], [601, 609, "Method"], [614, 633, "Method"], [742, 774, "Task"]]}
{"id": 4578, "text": "Following the rising prominence of online social networks, we observe an emerging trend for brands to adopt influencer marketing, embracing key opinion leaders (KOLs) to reach potential customers (PCs) online. Owing to the growing strategic importance of these brand key assets, this paper presents a novel feature extraction method named Multi-modal Asset-aware Projection (M2A2P) to learn a discriminative subspace from the high-dimensional multi-modal social media data for effective brand key asset discovery. By formulating a new asset-aware discriminative information preserving criterion, M2A2P differentiates with the existing multi-model feature extraction algorithms in two pivotal aspects: 1) We consider brand's highly imbalanced class interest steering towards the KOLs and PCs over the irrelevant users; 2) We consider a common observation that a user is not exclusive to a single class (e.g. a KOL can also be a PC). Experiments on a real-world apparel brand key asset dataset validate the effectiveness of the proposed method.", "meta": {}, "annotation_approver": null, "labels": [[339, 381, "Method"], [477, 512, "Task"], [455, 472, "Material"], [635, 677, "Method"], [949, 991, "Material"]]}
{"id": 4579, "text": "Most existing knowledge graphs (KGs) in academic domains suffer from problems of insufficient multi-relational information, name ambiguity and improper data format for large-scale machine processing. In this paper, we present AceKG, a new large-scale KG in academic domain. AceKG not only provides clean academic information, but also offers a large-scale benchmark dataset for researchers to conduct challenging data mining projects including link prediction, community detection and scholar classification. Specifically, AceKG describes 3.13 billion triples of academic facts based on a consistent ontology, including necessary properties of papers, authors, fields of study, venues and institutes, as well as the relations among them. To enrich the proposed knowledge graph, we also perform entity alignment with existing databases and rule-based inference. Based on AceKG, we conduct experiments of three typical academic data mining tasks and evaluate several state-of-the-art knowledge embedding and network representation learning approaches on the benchmark datasets built from AceKG. Finally, we discuss promising research directions that benefit from AceKG.", "meta": {}, "annotation_approver": null, "labels": [[233, 272, "Task"], [168, 198, "Task"], [444, 459, "Task"], [461, 480, "Task"], [485, 507, "Task"], [794, 810, "Task"], [982, 1001, "Method"], [1056, 1074, "Material"], [1006, 1037, "Method"]]}
{"id": 4580, "text": "This paper targets a general popularity prediction problem for event sequence, which has recently gained great attention due to its extensive applications in various domains. Feature driven method and point process method are two basic thinking paradigms to tackle the prediction problem, but both of them suffer from limitations. In this paper, we propose PreNets unifying the two thinking paradigms in an adversarial manner. On one side, feature driven model acts like a 'critic' who aims to discriminate the predicted popularity from the real one based on a set of temporal features from the sequence. On the other side, point process model acts like an 'interpreter' who recognizes the dynamic patterns in sequence to generate a predicted popularity that can fool the 'critic'. Through a Wasserstein learning based two-player game, the training loss of the 'critic' guides the 'interpreter' to better exploit the sequence patterns and enhance prediction, while the 'interpreter' pushes the 'critic' to select effective early features that helps discrimination. This mechanism enables the framework to absorb the advantages of both feature driven and point process methods. Empirical results show that PreNets achieves significant MAPE improvement for both Twitter cascade and Amazon review prediction.", "meta": {}, "annotation_approver": null, "labels": [[29, 50, "Task"], [1234, 1238, "Metric"], [1280, 1293, "Material"], [1260, 1267, "Material"], [357, 425, "Method"], [269, 287, "Task"], [440, 460, "Method"]]}
{"id": 4581, "text": "Understanding conversations is crucial to enabling conversational search in technologies such as chatbots, digital assistants, and smart home devices that are becoming increasingly popular. Conventional search engines are powerful at answering open domain queries but are mostly capable of stateless search. In this paper, we define a conversational query as a query that depends on the context of the current conversation, and we formulate the conversational query understanding problem as context-aware query reformulation, where the goal is to reformulate the conversational query into a search engine friendly query in order to satisfy users’ information needs in conversational settings. Such context-aware query reformulation problem lends itself to sequence to sequence modeling. We present a large scale open domain dataset of conversational queries and various sequence to sequence models that are learned from this dataset. The best model correctly reformulates over half of all conversational queries, showing the potential of sequence to sequence modeling for this task.", "meta": {}, "annotation_approver": null, "labels": [[0, 27, "Task"], [460, 479, "Task"], [698, 731, "Task"], [491, 524, "Task"], [870, 898, "Method"], [1038, 1067, "Method"], [756, 785, "Method"]]}
{"id": 4582, "text": "Web pages are a valuable source of information for many natural language processing and information retrieval tasks. Extracting the main content from those documents is essential for the performance of derived applications. To address this issue, we introduce a novel model that performs sequence labeling to collectively classify all text blocks in an HTML page as either boilerplate or main content. Our method uses a hidden Markov model on top of potentials derived from DOM tree features using convolutional neural networks. The proposed method sets a new state-of-the-art performance for boilerplate removal on the CleanEval benchmark. As a component of information retrieval pipelines, it improves retrieval performance on the ClueWeb12 collection.", "meta": {}, "annotation_approver": null, "labels": [[0, 9, "Material"], [420, 439, "Method"], [117, 144, "Task"], [56, 83, "Task"], [88, 109, "Task"], [288, 305, "Method"], [498, 527, "Method"], [733, 742, "Material"], [620, 639, "Material"], [659, 681, "Task"]]}
{"id": 4583, "text": "Link prediction in dynamic networks is an important task with many real-life applications in different domains, such as social networks, cyber-physical systems, and bioinformatics. There are two key processes in dynamic networks: network structural evolution and network temporal evolution, where the former represents interdependency between entities and their neighbors in the network at each timestamp, while the latter captures the evolving behavior of the entire network from the current timestamp to the next. Structural evolution generally assumes that a node is more likely to co-evolve with its neighbors in the near future. Temporal evolution focuses on the trend of network evolution as a whole, based on the accumulation of historical data. It is thus essential to use characteristics of both structural and temporal evolutions to emulate complex behaviors of dynamic networks. However, very few existing work considered both processes. In addition, real-life networks are often very sparse with limited observed links. A missing link between two nodes does not always imply that the two nodes do not have a relation in reality, especially when they share many common neighbors. Most existing methods only focus on the first-order proximity of networks, which is usually insufficient to capture the relationships among nodes. \n In this work, we propose a novel framework named STEP, to simultaneously integrate both structural and temporal information in link prediction in dynamic networks. STEP first constructs a sequence of higher-order proximity matrices to better capture the implicit relationships among nodes. A regularized optimization problem is then formulated to model those higher-order proximity matrices along with additional structural and temporal constraints. Given the large scale of modern networks, we also develop an efficient block coordinate gradient descent approach to solve the optimization problem efficiently. STEP can be used to solve the link prediction problem in directed or undirected, weighted or unweighted dynamic networks. Extensive experiments on several real world datasets demonstrate that STEP can effectively model link propagation over entire time-varying networks and its superiority over some state-of-the-art algorithms.", "meta": {}, "annotation_approver": null, "labels": [[1467, 1482, "Task"], [1861, 1894, "Method"], [1917, 1929, "Task"], [1981, 1996, "Task"], [0, 15, "Task"]]}
{"id": 4584, "text": "Multidimensional data appear frequently in many web-related applications, e.g., product ratings, the bag-of-words representation of web pages, etc. Principal Component Analysis (PCA) has been widely used for discovering patterns in relationships among entities in multidimensional data. However, existing algorithms for PCA have limited scalability since they explicitly materialize intermediate data, whose size rapidly grows as the dimension increases. To avoid scalability issues, we propose sSketch, a scalable sketching technique for PCA that employs several optimization ideas, such as mean propagation, efficient sparse matrix operations, and effective job consolidation to minimize intermediate data. Using sSketch, we also provide two other scalable methods for deriving singular value and 2-norm of reconstruction error, both of which are used for data analysis purpose. We provide our implementation on popular Spark framework for distributed platform. We compare our method against state-of-the-art library functions available for distributed settings, namely MLlib-PCA and Mahout-PCA with real big datasets. Our experiments show that our method outperforms both of them by a wide margin. To encourage reproducibility, the source code of sSketch is made publicly available at \\hrefhttps://github.com/DataMiningResearch/sSketch https://github.com/DataMiningResearch/sSketch.", "meta": {}, "annotation_approver": null, "labels": [[148, 182, "Task"], [858, 871, "Task"]]}
{"id": 4585, "text": "Memory networks have shown expressive performance on aspect based sentiment analysis. However, ordinary memory networks only capture word-level information and lack the capacity for modeling complicated expressions which consist of multiple words. Targeting this problem, we propose a novel convolutional memory network which incorporates an attention mechanism. This model sequentially computes the weights of multiple memory units corresponding to multi-words. This model may capture both words and multi-words expressions in sentences for aspect-based sentiment analysis. Experimental results show that the proposed model outperforms the state-of-the-art baselines.", "meta": {}, "annotation_approver": null, "labels": [[291, 361, "Method"], [53, 84, "Task"], [0, 15, "Method"], [104, 119, "Method"], [542, 573, "Task"]]}
{"id": 4586, "text": "Text segmentation plays an important role in various Natural Language Processing (NLP) tasks like summarization, context understanding, document indexing and document noise removal. Previous methods for this task require manual feature engineering, huge memory requirements and large execution times. To the best of our knowledge, this paper is the first one to present a novel supervised neural approach for text segmentation. Specifically, we propose an attention-based bidirectional LSTM model where sentence embeddings are learned using CNNs and the segments are predicted based on contextual information. This model can automatically handle variable sized context information. Compared to the existing competitive baselines, the proposed model shows a performance improvement of ∼7% in WinDiff score on three benchmark datasets.", "meta": {}, "annotation_approver": null, "labels": [[0, 17, "Task"], [53, 86, "Task"], [98, 111, "Task"], [113, 134, "Task"], [136, 153, "Task"], [158, 180, "Task"], [409, 426, "Task"], [456, 496, "Method"], [378, 404, "Method"]]}
{"id": 4587, "text": "Word evolution refers to the changing meanings and associations of words throughout time, as a byproduct of human language evolution. By studying word evolution, we can infer social trends and language constructs over different periods of human history. However, traditional techniques such as word representation learning do not adequately capture the evolving language structure and vocabulary. In this paper, we develop a dynamic statistical model to learn time-aware word vector representation. We propose a model that simultaneously learns time-aware embeddings and solves the resulting alignment problem. This model is trained on a crawled NYTimes dataset. Additionally, we develop multiple intuitive evaluation strategies of temporal word embeddings. Our qualitative and quantitative tests indicate that our method not only reliably captures this evolution over time, but also consistently outperforms state-of-the-art temporal embedding approaches on both semantic accuracy and alignment quality.", "meta": {}, "annotation_approver": null, "labels": [[0, 14, "Task"], [425, 497, "Method"], [582, 601, "Task"], [538, 566, "Task"], [646, 661, "Material"], [926, 944, "Method"], [732, 755, "Method"], [964, 981, "Metric"]]}
{"id": 4588, "text": "Many applications from various disciplines are now required to analyze fast evolving big data in real time. Various approaches for incremental processing of queries have been proposed over the years. Traditional approaches rely on updating the results of a query when updates are streamed rather than re-computing these queries, and therefore, higher execution performance is expected. However, they do not perform well for large databases that are updated at high frequencies. Therefore, new algorithms and approaches have been proposed in the literature to address these challenges by, for instance, reducing the complexity of processing updates. Moreover, many of these algorithms are now leveraging distributed streaming platforms such as Spark Streaming and Flink. In this tutorial, we briefly discuss legacy approaches for incremental query processing, and then give an overview of the new challenges introduced due to processing big data streams. We then discuss in detail the recently proposed algorithms that address some of these challenges. We emphasize the characteristics and algorithmic analysis of various proposed approaches and conclude by discussing future research directions.", "meta": {}, "annotation_approver": null, "labels": [[62, 106, "Task"], [131, 165, "Task"], [829, 857, "Task"]]}
{"id": 4589, "text": "In social networks, dense relationships among users contribute to stable networks. Breakdowns of some relationships may cause users to leave the network hence decrease the network stability. A popular metric to measure the stability of a network is k-core, i.e., the maximal subgraph of a social network in which each node has at least k neighbors. In this paper, we propose a novel problem, called k-core minimization. Given a graph G, an integer k and a budget b, we aim to identify a set B of edges with size b, so that we can get the minimum k-core by deleting B from G. We first formally define the problem and prove its NP-hardness. Then a baseline greedy algorithm is proposed. To handle large graphs, an optimized algorithm, named KC-Edge, is developed by adopting novel pruning rules. Finally, comprehensive experiments on 6 real social networks are conducted to demonstrate the efficiency and effectiveness of our proposed methods.", "meta": {}, "annotation_approver": null, "labels": [[399, 418, "Task"], [654, 671, "Method"], [839, 854, "Material"], [3, 18, "Material"], [249, 347, "Metric"]]}
{"id": 4590, "text": "Recently, dataless text classification has attracted increasing attention. It trains a classifier using seed words of categories, rather than labeled documents that are expensive to obtain. However, a small set of seed words may provide very limited and noisy supervision information, because many documents contain no seed words or only irrelevant seed words. In this paper, we address these issues using document manifold, assuming that neighboring documents tend to be assigned to a same category label. Following this idea, we propose a novel Laplacian seed word topic model (LapSWTM). In LapSWTM, we model each document as a mixture of hidden category topics, each of which corresponds to a distinctive category. Also, we assume that neighboring documents tend to have similar category topic distributions. This is achieved by incorporating a manifold regularizer into the log-likelihood function of the model, and then maximizing this regularized objective. Experimental results show that our LapSWTM significantly outperforms the existing dataless text classification algorithms and is even competitive with supervised algorithms to some extent. More importantly, it performs extremely well when the seed words are scarce.", "meta": {}, "annotation_approver": null, "labels": [[10, 38, "Task"], [547, 588, "Method"], [593, 600, "Method"], [999, 1006, "Method"], [1046, 1074, "Task"], [1115, 1136, "Method"]]}
{"id": 4591, "text": "RDF stream reasoning is gaining more and more attention but current research mainly focuses on logical frameworks which aim to formalize the query semantics and enhance the complexity of reasoning ability. These frameworks are evaluated on prototype systems based on a centralized design and suffer from limited scalability. A common way to enhance system scalability is to adopt a distributed approach. Moreover, the study of applying distributed solution for expressive RDF stream reasoning is still missing. In this paper, we explore the ability of modern Big Data platform to handle highly expressive temporal Datalog/Answer Set Programming(ASP) over RDF data streams. In order to achieve our goal, we first discuss some key features to parallelize Datalog/ASP program, and we associate these features to the two well known distributed stream processing models, namely Bulk Synchronous Processing (BSP) and Record-at-A-Time (RAT). We build a technical demonstrator called BigSR on top of Spark(BSP) and Flink(RAT) to support our evaluations, and identify the pros and cons of each model. Our experiments show that, BigSR achieves high throughput beyond million-triples per second using a rather small cluster of machines.", "meta": {}, "annotation_approver": null, "labels": [[0, 20, "Task"], [472, 492, "Task"], [828, 864, "Method"], [873, 906, "Method"], [911, 933, "Method"]]}
{"id": 4592, "text": "To explore the relationships between entities in RDF graphs, property path queries were introduced in SPARQL 1.1. However, existing RDF engines return only reachability of the entities ignoring the intermediate nodes in the path. If the paths are output, they are too many, which makes it difficult for users to find the most relevant paths. To address this issue, we propose a generalized topk ranking technique that balances the trade-off between relevance and diversity. We propose a shortest path based relevance scoring in combination with several path similarity measures for diversification. With preliminary experiments and examples, we show that our diversification strategies provide more novel paths as our technique prioritizes diversity over path length.", "meta": {}, "annotation_approver": null, "labels": [[378, 412, "Method"], [487, 524, "Method"]]}
{"id": 4593, "text": "Ranking question answer pairs has attracted increasing attention recently due to its broad applications such as information retrieval and question answering (QA). Significant progresses have been made by deep neural networks. However, background information and hidden relations beyond the context, which play crucial roles in human text comprehension, have received little attention in recent deep neural networks that achieve the state of the art in ranking QA pairs. In the paper, we propose KABLSTM, a Knowledge-aware Attentive Bidirectional Long Short-Term Memory, which leverages external knowledge from knowledge graphs (KG) to enrich the representational learning of QA sentences. Specifically, we develop a context-knowledge interactive learning architecture, in which a context-guided attentive convolutional neural network (CNN) is designed to integrate knowledge embeddings into sentence representations. Besides, a knowledge-aware attention mechanism is presented to attend interrelations between each segments of QA pairs. KABLSTM is evaluated on two widely-used benchmark QA datasets: WikiQA and TREC QA. Experiment results demonstrate that KABLSTM has robust superiority over competitors and sets state-of-the-art.", "meta": {}, "annotation_approver": null, "labels": [[0, 29, "Task"], [112, 133, "Task"], [138, 161, "Task"], [204, 224, "Method"], [394, 414, "Method"], [452, 468, "Task"], [576, 604, "Task"], [663, 677, "Task"], [805, 839, "Method"], [1100, 1106, "Material"], [1111, 1118, "Material"], [1087, 1089, "Task"], [1027, 1029, "Task"], [1037, 1044, "Method"], [1156, 1163, "Method"], [495, 568, "Method"]]}
{"id": 4594, "text": "Learning to Rank has traditionally considered settings where given the relevance information of objects, the desired order in which to rank the objects is clear. However, with today's large variety of users and layouts this is not always the case. In this paper, we consider so-called complex ranking settings where it is not clear what should be displayed, that is, what the relevant items are, and how they should be displayed, that is, where the most relevant items should be placed. These ranking settings are complex as they involve both traditional ranking and inferring the best display order. Existing learning to rank methods cannot handle such complex ranking settings as they assume that the display order is known beforehand. To address this gap we introduce a novel Deep Reinforcement Learning method that is capable of learning complex rankings, both the layout and the best ranking given the layout, from weak reward signals. Our proposed method does so by selecting documents and positions sequentially, hence it ranks both the documents and positions, which is why we call it the Double Rank Model (DRM). Our experiments show that DRM outperforms all existing methods in complex ranking settings, thus it leads to substantial ranking improvements in cases where the display order is not known a priori.", "meta": {}, "annotation_approver": null, "labels": [[0, 16, "Task"], [1097, 1120, "Method"], [779, 806, "Method"], [833, 858, "Task"]]}
{"id": 4595, "text": "Recommending a ranked list of interesting venues to users based on their preferences has become a key functionality in Location-Based Social Networks (LBSNs) such as Yelp and Gowalla. Bayesian Personalised Ranking (BPR) is a popular pairwise recommendation technique that is used to generate the ranked list of venues of interest to a user, by leveraging the user's implicit feedback such as their check-ins as instances of positive feedback, while randomly sampling other venues as negative instances. To alleviate the sparsity that affects the usefulness of recommendations by BPR for users with few check-ins, various approaches have been proposed in the literature to incorporate additional sources of information such as the social links between users, the textual content of comments, as well as the geographical location of the venues. However, such approaches can only readily leverage one source of additional information for negative sampling. Instead, we propose a novel Personalised Ranking Framework with Multiple sampling Criteria (PRFMC) that leverages both geographical influence and social correlation to enhance the effectiveness of BPR. In particular, we apply a multi-centre Gaussian model and a power-law distribution method, to capture geographical influence and social correlation when sampling negative venues, respectively. Finally, we conduct comprehensive experiments using three large-scale datasets from the Yelp, Gowalla and Brightkite LBSNs. The experimental results demonstrate the effectiveness of fusing both geographical influence and social correlation in our proposed PRFMC framework and its superiority in comparison to BPR-based and other similar ranking approaches. Indeed, our PRFMC approach attains a 37% improvement in MRR over a recently proposed approach that identifies negative venues only from social links.", "meta": {}, "annotation_approver": null, "labels": [[0, 84, "Task"], [184, 219, "Method"], [283, 339, "Task"], [1182, 1209, "Method"], [1216, 1245, "Method"], [1605, 1620, "Method"], [1762, 1765, "Metric"], [579, 582, "Method"], [1151, 1154, "Method"], [1718, 1723, "Method"], [982, 1052, "Method"]]}
{"id": 4596, "text": "Evidence-based medicine (EBM) is the practice of making clinical decisions based on rigorous scientific evidence. EBM relies on effective access to peer-reviewed literature - a task hampered by both the exponential growth of medical literature and a lack of efficient and effective means of searching and presenting this literature. This paper describes a search engine specifically designed for searching medical literature for the purpose of EBM and in a clinical decision support setting.", "meta": {}, "annotation_approver": null, "labels": [[0, 29, "Task"], [114, 117, "Task"], [444, 447, "Task"], [457, 482, "Task"]]}
{"id": 4597, "text": "Deep Generative Models (DGMs) are able to extract high-level representations from massive unlabeled data and are explainable from a probabilistic perspective. Such characteristics favor sequence modeling tasks. However, it still remains a huge challenge to model sequences with DGMs. Unlike real-valued data that can be directly fed into models, sequence data consist of discrete elements and require being transformed into certain representations first. This leads to the following two challenges. First, high-level features are sensitive to small variations of inputs as well as the way of representing data. Second, the models are more likely to lose long-term information during multiple transformations. In this paper, we propose a Hierarchical Deep Generative Model With Dual Memory to address the two challenges. Furthermore, we provide a method to efficiently perform inference and learning on the model. The proposed model extends basic DGMs with an improved hierarchically organized multi-layer architecture. Besides, our model incorporates memories along dual directions, respectively denoted as broad memory and deep memory. The model is trained end-to-end by optimizing a variational lower bound on data log-likelihood using the improved stochastic variational method. We perform experiments on several tasks with various datasets and obtain excellent results. The results of language modeling show our method significantly outperforms state-of-the-art results in terms of generative performance. Extended experiments including document modeling and sentiment analysis, prove the high-effectiveness of dual memory mechanism and latent representations. Text random generation provides a straightforward perception for advantages of our model.", "meta": {}, "annotation_approver": null, "labels": [[0, 29, "Method"], [278, 282, "Method"], [946, 950, "Method"], [968, 1017, "Method"], [1541, 1558, "Task"], [1563, 1581, "Task"], [736, 788, "Method"]]}
{"id": 4598, "text": "Sentence-level sentiment classification is important to understand users' fine-grained opinions. Existing methods for sentence-level sentiment classification are mainly based on supervised learning. However, it is difficult to obtain sentiment labels of sentences since manual annotation is expensive and time-consuming. In this paper, we propose an approach for sentence-level sentiment classification without the need of sentence labels. More specifically, we propose a unified framework to incorporate two types of weak supervision, i.e., document-level and word-level sentiment labels, to learn the sentence-level sentiment classifier. In addition, the contextual information of sentences and words extracted from unlabeled sentences is incorporated into our approach to enhance the learning of sentiment classifier. Experiments on benchmark datasets show that our approach can effectively improve the performance of sentence-level sentiment classification.", "meta": {}, "annotation_approver": null, "labels": [[0, 39, "Task"], [118, 157, "Task"], [178, 197, "Method"], [363, 402, "Task"], [921, 960, "Task"]]}
{"id": 4599, "text": "The majority of online display ads are served through real-time bidding (RTB) --- each ad display impression is auctioned off in real-time when it is just being generated from a user visit. To place an ad automatically and optimally, it is critical for advertisers to devise a learning algorithm to cleverly bid an ad impression in real-time. Most previous works consider the bid decision as a static optimization problem of either treating the value of each impression independently or setting a bid price to each segment of ad volume. However, the bidding for a given ad campaign would repeatedly happen during its life span before the budget runs out. As such, each bid is strategically correlated by the constrained budget and the overall effectiveness of the campaign (e.g., the rewards from generated clicks), which is only observed after the campaign has completed. Thus, it is of great interest to devise an optimal bidding strategy sequentially so that the campaign budget can be dynamically allocated across all the available impressions on the basis of both the immediate and future rewards. In this paper, we formulate the bid decision process as a reinforcement learning problem, where the state space is represented by the auction information and the campaign's real-time parameters, while an action is the bid price to set. By modeling the state transition via auction competition, we build a Markov Decision Process framework for learning the optimal bidding policy to optimize the advertising performance in the dynamic real-time bidding environment. Furthermore, the scalability problem from the large real-world auction volume and campaign budget is well handled by state value approximation using neural networks. The empirical study on two large-scale real-world datasets and the live A/B testing on a commercial platform have demonstrated the superior performance and high efficiency compared to state-of-the-art methods.", "meta": {}, "annotation_approver": null, "labels": [[1161, 1183, "Method"], [1717, 1732, "Method"], [1408, 1431, "Method"]]}
{"id": 4600, "text": "Wikidata is a collaboratively-edited knowledge graph; it expresses knowledge in the form of subject-property-value triples, which can be enhanced with references to add provenance information. Understanding the quality of Wikidata is key to its widespread adoption as a knowledge resource. We analyse one aspect of Wikidata quality, provenance, in terms of relevance and authoritativeness of its external references. We follow a two-staged approach. First, we perform a crowdsourced evaluation of references. Second, we use the judgements collected in the first stage to train a machine learning model to predict reference quality on a large-scale. The features chosen for the models were related to reference editing and the semantics of the triples they referred to. 61% of the references evaluated were relevant and authoritative. Bad references were often links that changed and either stopped working or pointed to other pages. The machine learning models outperformed the baseline and were able to accurately predict non-relevant and nonauthoritative references. Further work should focus on implementing our approach in Wikidata to help editors find bad references.", "meta": {}, "annotation_approver": null, "labels": [[605, 630, "Task"], [1015, 1067, "Task"]]}
{"id": 4601, "text": "We present work on building a global long-tailed ranking of entities across multiple languages using Wikipedia and Freebase knowledge bases. We identify multiple features and build a model to rank entities using a ground-truth dataset of more than 10 thousand labels. The final system ranks 27 million entities with 75% precision and 48% F1 score. We provide performance evaluation and empirical evidence of the quality of ranking across languages, and open the final ranked lists for future research.", "meta": {}, "annotation_approver": null, "labels": [[30, 94, "Task"], [320, 329, "Metric"], [338, 346, "Metric"], [101, 110, "Material"], [115, 123, "Material"]]}
{"id": 4602, "text": "We describe our experience of implementing a news content organization system at Tencent that discovers events from vast streams of breaking news and evolves news story structures in an online fashion. Our real-world system has distinct requirements in contrast to previous studies on topic detection and tracking (TDT) and event timeline or graph generation, in that we 1) need to accurately and quickly extract distinguishable events from massive streams of long text documents that cover diverse topics and contain highly redundant information, and 2) must develop the structures of event stories in an online manner, without repeatedly restructuring previously formed stories, in order to guarantee a consistent user viewing experience. In solving these challenges, we propose Story Forest, a set of online schemes that automatically clusters streaming documents into events, while connecting related events in growing trees to tell evolving stories. We conducted extensive evaluation based on 60 GB of real-world Chinese news data, although our ideas are not language-dependent and can easily be extended to other languages, through detailed pilot user experience studies. The results demonstrate the superior capability of Story Forest to accurately identify events and organize news text into a logical structure that is appealing to human readers, compared to multiple existing algorithm frameworks.", "meta": {}, "annotation_approver": null, "labels": [[285, 319, "Task"], [1006, 1035, "Material"]]}
{"id": 4603, "text": "Social media has become one of the most credible sources for delivering messages, breaking news, as well as events. Predicting the future dynamics of an event at a very early stage is significantly valuable, e.g, helping company anticipate marketing trends before the event becomes mature. However, this prediction is non-trivial because a) social events always stay with \"noise'' under the same topic and b) the information obtained at its early stage is too sparse and limited to support an accurate prediction. In order to overcome these two problems, in this paper, we design an event early embedding model (EEEM) that can 1) extract social events from noise, 2) find the previous similar events, and 3) predict future dynamics of a new event. Extensive experiments conducted on a large-scale dataset of Twitter data demonstrate the capacity of our model on extract events and the promising performance of prediction by considering both volume information as well as content information.", "meta": {}, "annotation_approver": null, "labels": [[116, 180, "Task"], [583, 617, "Method"], [630, 662, "Task"], [667, 699, "Task"], [708, 746, "Task"]]}
{"id": 4604, "text": "Topic models such as Latent Dirichlet Allocation (LDA) have been widely used in information retrieval for tasks ranging from smoothing and feedback methods to tools for exploratory search and discovery. However, classical methods for inferring topic models do not scale up to the massive size of today's publicly available Web-scale data sets. The state-of-the-art approaches rely on custom strategies, implementations and hardware to facilitate their asynchronous, communication-intensive workloads. We present APS-LDA, which integrates state-of-the-art topic modeling with cluster computing frameworks such as Spark using a novel asynchronous parameter server. Advantages of this integration include convenient usage of existing data processing pipelines and eliminating the need for disk writes as data can be kept in memory from start to finish. Our goal is not to outperform highly customized implementations, but to propose a general high-performance topic modeling framework that can easily be used in today's data processing pipelines. We compare APS-LDA to the existing Spark LDA implementations and show that our system can, on a 480-core cluster, process up to 135× more data and 10× more topics without sacricing model quality.", "meta": {}, "annotation_approver": null, "labels": [[21, 54, "Method"], [80, 101, "Task"], [555, 569, "Task"], [516, 519, "Method"], [1059, 1062, "Method"], [1085, 1088, "Method"]]}
{"id": 4605, "text": "Online learning has become very popular over the last decade. However, there are still many details that remain unknown about the strategies that students follow while studying online. In this study, we focus on the direction of detecting ‘invisible’ collaboration ties between students in online learning environments. Specifically, the paper presents a method developed to detect student ties based on temporal proximity of their assignment submissions. The paper reports on findings of a study that made use of the proposed method to investigate the presence of close submitters in two different massive open online courses. The results show that most of the students (i.e., student user accounts) were grouped as couples, though some bigger communities were also detected. The study also compared the population detected by the algorithm with the rest of user accounts and found that close submitters needed a statistically significant lower amount of activity with the platform to achieve a certificate of completion in a MOOC. These results confirm that the detected close submitters were performing some collaboration or even engaged in unethical behaviors, which facilitates their way into a certificate. However, more work is required in the future to specify various strategies adopted by close submitters and possible associations between the user accounts.", "meta": {}, "annotation_approver": null, "labels": [[228, 318, "Task"], [375, 394, "Task"], [607, 626, "Material"]]}
{"id": 4606, "text": "We introduce random directed acyclic graph and use it to model the information diffusion network. Subsequently, we analyze the cascade generation model (CGM) introduced by Leskovec et al. [19]. Until now only empirical studies of this model were done. In this paper, we present the first theoretical proof that the sizes of cascades generated by the CGM follow the power-law distribution, which is consistent with multiple empirical analysis of the large social networks. We compared the assumptions of our model with the Twitter social network and tested the goodness of approximation.", "meta": {}, "annotation_approver": null, "labels": [[13, 42, "Method"], [127, 157, "Method"], [350, 353, "Method"], [522, 530, "Material"]]}
{"id": 4607, "text": "Recent neural network approaches to sentence matching compute the probability of two sentences being similar by minimizing a logistic loss. In this paper, we learn sentence representations by means of a siamese network, which: (i) uses encoders that share parameters; and (ii) enables the comparison between two sentences in terms of their euclidean distance, by minimizing a contrastive loss. Moreover, we add a multilayer perceptron in the architecture to simultaneously optimize the contrastive and the logistic losses. This way, our network can exploit a more informative feedback, given by the logistic loss, which is also quantified by the distance that the two sentences have according to their representation in the euclidean space. We show that jointly minimizing the two losses yields higher accuracy than minimizing them independently. We verify this finding by evaluating several baseline architectures in two sentence matching tasks: question paraphrasing and textual entailment recognition. Our network approaches the state of the art, while being much simpler and faster to train, and with less parameters than its competitors.", "meta": {}, "annotation_approver": null, "labels": [[36, 53, "Task"], [802, 810, "Metric"], [947, 968, "Task"], [973, 1003, "Task"], [158, 188, "Task"], [203, 218, "Method"], [236, 245, "Method"], [413, 434, "Method"], [7, 21, "Method"], [922, 939, "Task"]]}
{"id": 4608, "text": "Recommendation plays an increasingly important role in our daily lives. Recommender systems automatically suggest items to users that might be interesting for them. Recent studies illustrate that incorporating social trust in Matrix Factorization methods demonstrably improves accuracy of rating prediction. Such approaches mainly use the trust scores explicitly expressed by users. However, it is often challenging to have users provide explicit trust scores of each other. There exist quite a few works, which propose Trust Metrics to compute and predict trust scores between users based on their interactions. In this paper, first we present how social relation can be extracted from users’ ratings to items by describing Hellinger distance between users in recommender systems. Then, we propose to incorporate the predicted trust scores into social matrix factorization models. By analyzing social relation extraction from three wellknown real-world datasets, which both: trust and recommendation data available, we conclude that using the implicit social relation in social recommendation techniques has almost the same performance compared to the actual trust scores explicitly expressed by users. Hence, we build our method, called Hell-TrustSVD, on top of the state-ofthe-art social recommendation technique to incorporate both the extracted implicit social relations and ratings given by users on the prediction of items for an active user. To the best of our knowledge, this is the first work to extend TrustSVD with extracted social trust information. The experimental results support the idea of employing implicit trust into matrix factorization whenever explicit trust is not available, can perform much better than the state-of-the-art approaches in user rating prediction. c © 2017 International World Wide Web Conference Committee (IW3C2), published under Creative Commons CC BY 4.0 License. WWW’17 Companion, April 3–7, 2017, Perth, Australia. ACM 978-1-4503-4914-7/17/04. http://dx.doi.org/10.1145/3041021.3051153", "meta": {}, "annotation_approver": null, "labels": [[1638, 1658, "Method"], [106, 163, "Task"], [226, 246, "Method"], [549, 569, "Task"], [645, 710, "Task"], [725, 743, "Method"], [846, 880, "Method"], [943, 962, "Material"], [1044, 1068, "Method"], [277, 285, "Metric"], [289, 306, "Task"], [1770, 1787, "Task"]]}
{"id": 4609, "text": "This paper explores the provision of adaptive hints based on attainment levels in the context of supporting the development of young adults’ ICT information processing skills. We describe the design of the LIBE VLE, particularly its personalisation and adaptation features, and a User Study undertaken with young adults at a vocational education centre. Using data collected through the LIBE VLE, we analyse the relationships between learners’ accessing of hints, motivation, and performance. Results point to a positive effect of accessing of hints on students’ perception of the LIBE VLE and their likelihood of using it again for further learning; and also a positive effect of students’ interest in the course subject on their engagement and performance in course activities. These findings have important implications for supporting young adults in developing key competences necessary for integration into the workforce and for fostering self-regulated lifelong learning.", "meta": {}, "annotation_approver": null, "labels": [[24, 174, "Task"], [280, 290, "Method"], [307, 352, "Material"], [360, 374, "Material"]]}
{"id": 4610, "text": "Cascades on social and information networks have been a tremendously popular subject of study in the past decade, and there is a considerable literature on phenomena such as diffusion mechanisms, virality, cascade prediction, and peer network effects. Against the backdrop of this research, a basic question has received comparatively little attention: how desirable are cascades on a social media platform from the point of view of users? While versions of this question have been considered from the perspective of the producers of cascades, any answer to this question must also take into account the effect of cascades on their audience — the viewers of the cascade who do not directly participate in generating the content that launched it. In this work, we seek to fill this gap by providing a consumer perspective of information cascades. Users on social and information networks play the dual role of producers and consumers, and our work focuses on how users perceive cascades as consumers. Starting from this perspective, we perform an empirical study of the interaction of Twitter users with retweet cascades. We measure how often users observe retweets in their home timeline, and observe a phenomenon that we term the Impressions Paradox: the share of impressions for cascades of size k decays much more slowly than frequency of cascades of size k. Thus, the audience for cascades can be quite large even for rare large cascades. We also measure audience engagement with retweet cascades in comparison to non-retweeted or organic content. Our results show that cascades often rival or exceed organic content in engagement received per impression. This result is perhaps surprising in that consumers didn’t opt in to see tweets from these authors. Furthermore, although cascading content is widely popular, one would expect it to eventually reach parts of the audience that may not be interested in the content. Motivated by the tension in these empirical findings, we posit a simple theoretical model that focuses on the effect of cascades on the audience (rather than the cascade producers). Our results on this model highlight c ©2017 International World Wide Web Conference Committee (IW3C2), published under Creative Commons CC BY 4.0 License. WWW 2017, April 3–7, 2017, Perth, Australia. ACM 978-1-4503-4913-0/17/04. http://dx.doi.org/10.1145/3038912.3052647 . the balance between retweeting as a high-quality content selection mechanism and the role of network users in filtering irrelevant content. In particular, the results suggest that together these two effects enable the audience to consume a high quality stream of content in the presence of cascades.", "meta": {}, "annotation_approver": null, "labels": [[353, 439, "Task"], [0, 18, "Method"], [23, 43, "Task"], [206, 224, "Task"], [958, 998, "Task"], [1084, 1092, "Material"]]}
{"id": 4611, "text": "This short report describes the participation of the Università della Svizzera italiana (USI) at the SMERP Workshop Data Challenge Track for the task text summarization of Level 1. Our participation is based on a linear interpolation for combining relevance and novelty scores of the retrieved tweets. Our method is fully automatic. For the relevance score we used the results from our runs at the text retrieval task whereas for the novelty we used a method based on Word2Vec. In total, we submitted four different runs and we used two different weight parameters. The results showed that when relevance and novelty have an equal contribution in selecting the tweets to use for the summary, the performance is better compared to favoring only the novelty. Additionally, information from POS tags improves the performance of the summarization task.", "meta": {}, "annotation_approver": null, "labels": [[468, 476, "Method"], [829, 847, "Task"], [150, 168, "Task"], [294, 300, "Material"], [398, 417, "Task"], [661, 668, "Material"], [788, 796, "Method"]]}
{"id": 4612, "text": "A graph database D is a collection of graphs. To speed up subgraph query answering on graph databases, indexes are commonly used. State-of-the-art graph database indexes do not adapt or scale well to dynamic graph database use; they are static, and their ability to prune possible search responses to meet user needs worsens over time as databases change and grow. Users can re-mine indexes to gain some improvement, but it is time consuming. Users must also tune numerous parameters on an ongoing basis to optimize performance and can inadvertently worsen the query response time if they do not choose parameters wisely. Recently, a one-pass algorithm has been developed to enhance the performance of frequent subgraphs based indexes by using the algorithm to update them regularly. However, there are some drawbacks, most notably the need to make updates as the query workload changes.\n In this paper, we propose a new index based on graph-coarsening to speed up subgraph query answering time in dynamic graph databases. Our index is parameter-free, query-independent, scalable,small enough to store in the main memory, and is simpler and less costly to maintain for database updates. Experimental results show that our index outperforms hybrid-indexes (i.e. indexes updated with one-pass) for query answering time in the case of social network databases, and is comparable with these indexes for frequent and infrequent queries on chemical databases. Our index can be updated up to 60 times faster in comparison to one-pass on dynamic graph databases. Moreover, our index is independent of the query workload for index update and is up to 15 times faster after hybrid-indexes are attuned to query workload.", "meta": {}, "annotation_approver": null, "labels": [[49, 101, "Task"], [675, 720, "Task"], [936, 952, "Method"], [956, 994, "Task"], [998, 1021, "Material"], [1331, 1356, "Material"], [1434, 1452, "Material"], [1530, 1553, "Material"]]}
{"id": 4613, "text": "A large amount of information exists in reviews written by users. This source of information has been ignored by most of the current recommender systems while it can potentially alleviate the sparsity problem and improve the quality of recommendations. In this paper, we present a deep model to learn item properties and user behaviors jointly from review text. The proposed model, named Deep Cooperative Neural Networks (DeepCoNN), consists of two parallel neural networks coupled in the last layers. One of the networks focuses on learning user behaviors exploiting reviews written by the user, and the other one learns item properties from the reviews written for the item. A shared layer is introduced on the top to couple these two networks together. The shared layer enables latent factors learned for users and items to interact with each other in a manner similar to factorization machine techniques. Experimental results demonstrate that DeepCoNN significantly outperforms all baseline recommender systems on a variety of datasets.", "meta": {}, "annotation_approver": null, "labels": [[178, 208, "Task"], [213, 251, "Task"], [281, 291, "Method"], [295, 360, "Task"], [458, 473, "Method"], [947, 956, "Method"], [388, 431, "Method"]]}
{"id": 4614, "text": "The application of neural embedding algorithms (based on architectures like skip-grams) to large knowledge bases like Wikipedia and the Google News Corpus has tremendously benefited multiple communities in applications as diverse as sentiment analysis, named entity recognition and text classification. In this paper, we present a similar resource for geospatial applications. We systematically construct a weighted network that spans all populated places in Geonames. Using a network embedding algorithm that was recently found to achieve excellent results and is based on the skip-gram model, we embed each populated place into a 100-dimensional vector space, in a similar vein as the GloVe embeddings released for Wikipedia. We demonstrate potential applications of this dataset resource, which we release under a public license. Resource Type. Datasets generated using novel methods/algorithms. Github. https://github.com/mayankkejriwal/Geonames-embeddings Figshare/DOI. https://doi.org/10.6084/m9.figshare.5248120 License. MIT License", "meta": {}, "annotation_approver": null, "labels": [[407, 423, "Method"], [477, 504, "Method"], [578, 593, "Method"], [19, 46, "Method"], [136, 154, "Material"], [233, 251, "Task"], [253, 277, "Task"], [282, 301, "Task"], [687, 703, "Method"]]}
{"id": 4615, "text": "Today's video streaming market is crowded with various content providers (CPs). For individual CPs, understanding user behavior, in particular how users migrate among different CPs, is crucial for improving users' on-site experience and the CP's chance of success. In this paper, we take a data-driven approach to analyze and model user migration behavior in video streaming, i.e., users switching content provider during active sessions. Based on a large ISP dataset over two months (6 major content providers, 3.8 million users, and 315 million video requests), we study common migration patterns and reasons of migration. We find that migratory behavior is prevalent: 66% of users switch CPs with an average switching frequency of 13%. In addition, migration behaviors are highly diverse: regardless large or small CPs, they all have dedicated groups of users who like to switch to them for certain types of videos. Regarding reasons of migration, we find CP service quality rarely causes migration, while a few popular videos play a bigger role. Nearly 60% of cross-site migrations are landed to 0.14% top videos. Finally, we validate our findings by building an accurate regression model to predict user migration frequency, and discuss the implications of our results to CPs.", "meta": {}, "annotation_approver": null, "labels": [[100, 127, "Task"], [143, 180, "Task"], [290, 310, "Method"], [456, 467, "Material"], [1176, 1192, "Method"], [1196, 1228, "Task"], [314, 374, "Task"]]}
{"id": 4616, "text": "In this paper, we propose a novel deep coherence model (DCM) using a convolutional neural network architecture to capture the text coherence. The text coherence problem is investigated with a new perspective of learning sentence distributional representation and text coherence modeling simultaneously. In particular, the model captures the interactions between sentences by computing the similarities of their distributional representations. Further, it can be easily trained in an end-to-end fashion. The proposed model is evaluated on a standard Sentence Ordering task. The experimental results demonstrate its effectiveness and promise in coherence assessment showing a significant improvement over the state-of-the-art by a wide margin.", "meta": {}, "annotation_approver": null, "labels": [[69, 110, "Method"], [146, 168, "Task"], [549, 571, "Task"], [34, 60, "Method"], [114, 140, "Task"]]}
{"id": 4617, "text": "Bartering is a timeless practice that is becoming increasingly popular on the Web. Recommending trades for an online bartering platform shares many similarities with traditional approaches to recommendation, in particular the need to model the preferences of users and the properties of the items they consume. However, there are several aspects that make bartering problems interesting and challenging, specifically the fact that users are both suppliers and consumers, and that the trading environment is highly dynamic. Thus, a successful model of bartering requires us to understand not just users' preferences, but also the social dynamics of who trades with whom, and the temporal dynamics of when trades occur.\n We propose new models for bartering-based recommendation, for which we introduce three novel datasets from online bartering platforms. Surprisingly, we find that existing methods (based on matching algorithms) perform poorly on real-world platforms, as they rely on idealized assumptions that are not supported by real bartering data. We develop approaches based on Matrix Factorization in order to model the reciprocal interest between users and each other's items. We also find that the social ties between members have a strong influence, as does the time at which they trade, therefore we extend our model to be socially- and temporally-aware. We evaluate our approach on trades covering books, video games, and beers, where we obtain promising empirical performance compared to existing techniques.", "meta": {}, "annotation_approver": null, "labels": [[83, 135, "Task"], [745, 775, "Task"], [1033, 1052, "Material"], [1085, 1105, "Method"], [1395, 1416, "Material"], [1418, 1429, "Material"], [1435, 1440, "Material"]]}
{"id": 4618, "text": "A great quantity of information is required to support urban planning. Usually there are many (not integrated) data sources, originating from different government bodies, in distinct formats and variable properties (e.g. reliability, completeness). The effort to handle these data, integrate and analyze them is high, taking to much time for the information to be available to help decision making. We argue that data from location-based social networks (LBSN) could be used to provide useful information in reasonable time, despite several limitations they have. To asses this, as a case study, we used data from different LBSN to calculate the Local Availability Index (IOL) for a Brazilian city. This index is part of a methodology to estimate quality of urban life inside cities and is used to support urban planning. The results suggest that data from LBSN are useful and could be used to provide insights for local governments.", "meta": {}, "annotation_approver": null, "labels": [[624, 629, "Material"], [44, 69, "Task"], [646, 676, "Metric"], [423, 460, "Material"], [857, 861, "Material"]]}
{"id": 4619, "text": "To reduce the cost and delay caused by transferring data to the remote cloud, the trend is to design an intelligent Edge Device (ED) for preliminary data processing, i.e., edge computing. Some EDs usually form a group where they wirelessly communicate with each other. Different ED groups are interconnected by optical fiber cables. Through coordinating the use of ED groups, we can perform a collaborative edge computing in a hybrid network where a cost-efficient opticalwireless convergence is achieved by virtualization. In this paper, we use the virtual network to describe one computingapplication’s requirement for the substrate resource, and we investigate how embed multiple virtual networks onto the common network infrastructure. In our approach, a graphcutting algorithm is firstly utilized to embed as many virtual networks as possible onto the specified EDs within the same group. However, a single ED group cannot handle all computing applications competing for limited wireless and computing resources. To solve this challenging problem, we transform the virtual networks—impossibly embedded onto the same ED group—into new ones processed by ED groups. Simulations results demonstrate the green feature of our solution: 1) the total transmitting power assigned for EDs is effectively reduced using the graph cutting algorithm provided that all of computing applications can be solved by a single ED group; 2) our method accepts more virtual networks with the improvement ratio of 77%, through the coordination of ED groups. In addition, there is a good match between the algorithm result and the optimal number of consumed wavelengths per optical fiber cable.", "meta": {}, "annotation_approver": null, "labels": [[3, 56, "Task"], [193, 197, "Method"], [279, 282, "Method"], [365, 368, "Method"], [383, 441, "Task"], [550, 565, "Method"], [759, 781, "Method"], [867, 871, "Method"], [912, 915, "Method"], [1121, 1124, "Method"], [1157, 1160, "Method"], [1280, 1284, "Method"], [1317, 1340, "Method"], [1411, 1413, "Method"], [1528, 1531, "Method"], [172, 186, "Task"], [104, 132, "Method"]]}
{"id": 4620, "text": "IR methods are increasingly being applied over microblogs to extract real-time information, such as during disaster events. In such sites, most of the user-generated content is written informally – the same word is often spelled differently by different users, and words are shortened arbitrarily due to the length limitations on microblogs. Stemming is a common step for improving retrieval performance by unifying different morphological variants of a word. In this study, we show that rule-based stemming meant for formal text often cannot capture the arbitrary variations of words in microblogs. We propose a context-specific stemming algorithm, based on word embeddings, which can capture many more variations of words than what can be detected by conventional stemmers. Experiments on a large set of English microblogs posted during a recent disaster event shows that, the proposed stemming gives considerably better retrieval performance compared to Porter stemming.", "meta": {}, "annotation_approver": null, "labels": [[588, 598, "Material"], [613, 648, "Method"], [659, 674, "Method"], [806, 824, "Material"], [342, 351, "Task"], [61, 90, "Task"], [488, 507, "Method"]]}
{"id": 4621, "text": "Previous research has shown that format design has an impact on the usability of listing pages. This study investigated the effects of specific list presentation format on visual search performance and subjective satisfaction in e-commerce listing pages. At first, we found seven important commodity features for consumers through pre-study. Then, an eye tracking study was conducted to record the visual search for target items and cognitive workload based on three different list formats (Vertical Format/T Format/Block format) in e-commerce websites. The results suggested that list format could significantly influence the visual search performance and satisfaction. The efficiency of Vertical format and T format is higher than block format. Designers could get some valid references from this result when they are designing listing pages.", "meta": {}, "annotation_approver": null, "labels": [[68, 94, "Task"], [172, 197, "Task"], [201, 253, "Task"], [351, 369, "Method"], [533, 552, "Material"], [627, 669, "Task"]]}
{"id": 4622, "text": "Can online trackers and network adversaries de-anonymize web browsing data readily available to them? We show— theoretically, via simulation, and through experiments on real user data—that de-identified web browsing histories can be linked to social media profiles using only publicly available data. Our approach is based on a simple observation: each person has a distinctive social network, and thus the set of links appearing in one’s feed is unique. Assuming users visit links in their feed with higher probability than a random user, browsing histories contain tell-tale marks of identity. We formalize this intuition by specifying a model of web browsing behavior and then deriving the maximum likelihood estimate of a user’s social profile. We evaluate this strategy on simulated browsing histories, and show that given a history with 30 links originating from Twitter, we can deduce the corresponding Twitter profile more than 50% of the time. To gauge the real-world e↵ectiveness of this approach, we recruited nearly 400 people to donate their web browsing histories, and we were able to correctly identify more than 70% of them. We further show that several online trackers are embedded on su ciently many websites to carry out this attack with high accuracy. Our theoretical contribution applies to any type of transactional data and is robust to noisy observations, generalizing a wide range of previous de-anonymization attacks. Finally, since our attack attempts to find the correct Twitter profile out of over 300 million candidates, it is—to our knowledge—the largestscale demonstrated de-anonymization to date. CCS Concepts •Security and privacy ! Pseudonymity, anonymity and untraceability; •Information systems ! Online advertising; Social networks; Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. c 2017 ACM. ISBN TDB. DOI: TBD", "meta": {}, "annotation_approver": null, "labels": [[0, 100, "Task"], [169, 183, "Material"], [869, 876, "Material"], [885, 925, "Task"], [1055, 1077, "Material"]]}
{"id": 4623, "text": "Code reuse, code querying and computer aided programming are some of the main research challenges in software engineering. Therefore, we have introduced CodeOntology as an approach to leverage recent advances in the Semantic Web area and the impressive amount of open source code freely available online, to provide a semantic view of software systems by extracting structured information from source code and by performing named entity disambiguation on the comments provided within the code, in order to link the corresponding entities to pertinent DBpedia resources. In this paper, we focus on the expressiveness of this framework by showing how CodeOntology can be used for static code analysis, semantic component search and code reuse.", "meta": {}, "annotation_approver": null, "labels": [[153, 165, "Method"], [424, 451, "Task"], [318, 351, "Task"], [551, 568, "Material"], [649, 662, "Method"], [678, 698, "Task"], [700, 725, "Task"], [30, 56, "Task"], [730, 740, "Task"], [0, 10, "Task"], [12, 25, "Task"]]}
{"id": 4624, "text": "Social recommendation has been an active research topic over the last decade, based on the assumption that social information from friendship networks is beneficial for improving recommendation accuracy, especially when dealing with cold-start users who lack sufficient past behavior information for accurate recommendation. However, it is nontrivial to use such information, since some of a person's friends may share similar preferences in certain aspects, but others may be totally irrelevant for recommendations. Thus one challenge is to explore and exploit the extend to which a user trusts his/her friends when utilizing social information to improve recommendations. On the other hand, most existing social recommendation models are non-interactive in that their algorithmic strategies are based on batch learning methodology, which learns to train the model in an offline manner from a collection of training data which are accumulated from users? historical interactions with the recommender systems. In the real world, new users may leave the systems for the reason of being recommended with boring items before enough data is collected for training a good model, which results in an inefficient customer retention. To tackle these challenges, we propose a novel method for interactive social recommendation, which not only simultaneously explores user preferences and exploits the effectiveness of personalization in an interactive way, but also adaptively learns different weights for different friends. In addition, we also give analyses on the complexity and regret of the proposed model. Extensive experiments on three real-world datasets illustrate the improvement of our proposed method against the state-of-the-art algorithms.", "meta": {}, "annotation_approver": null, "labels": [[0, 21, "Task"], [1349, 1374, "Method"], [1379, 1424, "Method"], [1468, 1514, "Method"], [1284, 1317, "Task"]]}
{"id": 4625, "text": "It is laborious for researchers to find proper collaborators who can provide researching guidance besides collaborating. Beneficial Collaborators (BCs), researchers who have a high academic level and relevant topics, can genuinely help researchers to enrich their research. Though many efforts have made to develop collaborator recommendation, most of existing works have mainly focused on recommending most possible collaborators with no intention to recommend specifically the BCs. In this paper, we propose the Beneficial Collaborator Recommendation (BCR) model that considers the dynamic research interest of researcher’s and academic level of collaborators to recommend the BCs. First, we run the LDA model on the abstract of researchers’ publications in each year for topic clustering. Second, we fix generated topic distribution matrix by a time function to fit interest dynamic transformation. Third, we compute the similarity between the collaboration candidate's feature matrix and the target researcher. Finally, we combine the similarity and influence in collaborators network to fix rank score and mine the candidates with high academic level and academic social impact. BCR generates the topN BCs recommendation. Extensive experiments on a dataset with citation network demonstrate that BCR performs better in terms of precision, recall, F1 score and the recommendation quality compared to baseline methods.", "meta": {}, "annotation_approver": null, "labels": [[32, 97, "Task"], [315, 342, "Task"], [661, 682, "Task"], [702, 711, "Method"], [774, 790, "Method"], [1184, 1187, "Method"], [1254, 1283, "Material"], [1301, 1304, "Method"], [1333, 1342, "Metric"], [1344, 1350, "Metric"], [1352, 1360, "Metric"], [514, 558, "Method"]]}
{"id": 4626, "text": "In web search, data mining, and machine learning, two popular measures of data similarity are the cosine and the resemblance (the latter is for binary data). In this study, we develop theoretical results for both the cosine and the GMM (generalized min-max) kernel [26], which is a generalization of the resemblance. GMM has direct applications in machine learning as a positive definite kernel and can be efficiently linearized via probabilistic hashing to handle big data. Owing to its discrete nature, the hashed values can also be used to build hash tables for efficient near neighbor search. We prove the theoretical limit of GMM and the consistency result, assuming that the data follow an elliptical distribution, which is a general family of distributions and includes the multivariate normal and t-distribution as special cases. The consistency result holds as long as the data have bounded first moment (an assumption which typically holds for data commonly encountered in practice). Furthermore, we establish the asymptotic normality of GMM. We also prove the limit of cosine under elliptical distributions. In comparison, the consistency of GMM requires much weaker conditions. For example, when data follow a t-distribution with ν degrees of freedom, GMM typically provides a better estimate of similarity than cosine when ν < 8 (ν = 8 means the distribution is very close to normal). These theoretical results help explain the recent success of GMM and lay the foundation for further research.", "meta": {}, "annotation_approver": null, "labels": [[176, 264, "Task"], [317, 320, "Method"], [433, 454, "Method"], [631, 634, "Method"], [1048, 1051, "Method"], [1153, 1156, "Method"], [1264, 1267, "Method"], [1459, 1462, "Method"], [3, 13, "Task"]]}
{"id": 4627, "text": "This work studies the effectiveness of query expansion for email search. Three state-of-the-art expansion methods are examined: 1) a global translation-based expansion model; 2) a personalized-based word embedding model; 3) the classical pseudo-relevance-feedback model. Experiments were conducted with two mail datasets extracted from a large query log of a Web mail service. Our results demonstrate the significant contribution of query expansion for measuring the similarity between the query and email messages. On the other hand, the contribution of expansion methods for a well trained learning-to-rank scoring function that exploits many relevance signals, was found to be modest.", "meta": {}, "annotation_approver": null, "labels": [[133, 173, "Method"], [180, 219, "Method"], [228, 269, "Method"], [307, 320, "Material"], [359, 375, "Material"], [592, 625, "Method"], [39, 54, "Task"], [59, 71, "Task"], [433, 448, "Task"]]}
{"id": 4628, "text": "As an effective way of learning node representations in networks, network embedding has attracted increasing research interests recently. Most existing approaches use shallow models and only work on static networks by extracting local or global topology information of each node as the algorithm input. It is challenging for such approaches to learn a desirable node representation on incomplete graphs with a large number of missing links or on dynamic graphs with new nodes joining in. It is even challenging for them to deeply fuse other types of data such as node properties into the learning process to help better represent the nodes with insufficient links. In this paper, we for the first time study the problem of network embedding on incomplete networks. We propose a Multi-View Correlation-learning based Deep Network Embedding method named MVC-DNE to incorporate both the network structure and the node properties for more effectively and efficiently perform network embedding on incomplete networks. Specifically, we consider the topology structure of the network and the node properties as two correlated views. The insight is that the learned representation vector of a node should reflect its characteristics in both views. Under a multi-view correlation learning based deep autoencoder framework, the structure view and property view embeddings are integrated and mutually reinforced through both self-view and cross-view learning. As MVC-DNE can learn a representation mapping function, it can directly generate the representation vectors for the new nodes without retraining the model. Thus it is especially more efficient than previous methods. Empirically, we evaluate MVC-DNE over three real network datasets on two data mining applications, and the results demonstrate that MVC-DNE significantly outperforms state-of-the-art methods.", "meta": {}, "annotation_approver": null, "labels": [[23, 64, "Task"], [66, 83, "Task"], [1248, 1279, "Method"], [1286, 1312, "Method"], [1414, 1447, "Method"], [1452, 1459, "Method"], [1690, 1697, "Method"], [1797, 1804, "Method"], [723, 763, "Task"], [778, 859, "Method"]]}
{"id": 4629, "text": "Understanding human migration is of great interest to demographers and social scientists. User generated digital data has made it easier to study such patterns at a global scale. Geo coded Twitter data, in particular, has been shown to be a promising source to analyse large scale human migration. But given the scale of these datasets, a lot of manual effort has to be put into processing and getting actionable insights from this data. In this paper, we explore feasibility of using a new tool, tensor decomposition, to understand trends in global human migration. We model human migration as a three mode tensor, consisting of (origin country, destination country, time of migration) and apply CP decomposition to get meaningful low dimensional factors. Our experiments on a large Twitter dataset spanning 5 years and over 100M tweets show that we can extract meaningful migration patterns.", "meta": {}, "annotation_approver": null, "labels": [[189, 196, "Material"], [258, 296, "Task"], [497, 517, "Method"], [784, 799, "Material"], [522, 565, "Task"]]}
{"id": 4630, "text": "In this paper, we propose a novel k-anonymization scheme to counter deanonymization queries on social networks. With this scheme, all entities are protected by k-anonymization, which means the attackers cannot re-identify a target with confidence higher than 1/k. The proposed scheme minimizes the modification on original networks, and accordingly maximizes the utility preservation of published data while achieving k-anonymization privacy protection. Extensive experiments on real data sets demonstrate the effectiveness of the proposed scheme, where the efficacy of the k-anonymized networks is verified with the distributions of pagerank, betweenness, and their Kolmogorov-Smirnov (K-S) test.", "meta": {}, "annotation_approver": null, "labels": [[34, 56, "Method"], [160, 175, "Method"], [574, 595, "Method"], [634, 642, "Method"], [60, 110, "Task"], [667, 691, "Method"]]}
{"id": 4631, "text": "How can we reliably infer web users’ interest and evaluate the content relevance when lacking active user interaction such as click behavior? In this paper, we investigate the relationship between mobile users’ implicit interest inferred from attention metrics, such as eye gaze or viewport time, and explicit interest expressed by users. We present the first quantitative gaze tracking study using front-facing camera of mobile devices instead of specialized, expensive eye-tracking devices. We focus on multi-column digital media pages in Google Play Store that display 30+ items per page belonging to diverse categories. In such pages, we find significantly different distribution of gaze metrics on items that users rate as interesting vs. not. We leverage this insight by building a prediction model that is able to infer a user’s interest ratings from the the non-click actions of the user. Our model is able to attain AUC of 90.32% in predicting user interest at an individual item level. In addition, our experiments on collection item re-ranking show how user gaze and viewport signals can be used to personalize item ranking on the collection page. Beyond understanding users’ attention behavior in novel contexts such as multi-column digital media pages in Google Play Store, the findings in this study have implications for the design of a novel personalization and recommendation mechanism by (1) prioritizing items that are most likely of interest to users based on historical attention signals, and (2) prioritizing positions receiving significant portion of gaze attention.", "meta": {}, "annotation_approver": null, "labels": [[20, 80, "Task"], [270, 278, "Metric"], [282, 295, "Metric"], [399, 436, "Method"], [541, 558, "Material"], [788, 804, "Method"], [925, 928, "Metric"], [1268, 1285, "Material"], [942, 966, "Task"], [1166, 1205, "Task"]]}
{"id": 4632, "text": "Word embedding promises a quantification of the similarity between terms. However, it is not clear to what extent this similarity value can be of practical use for subsequent information access tasks. In particular, which range of similarity values is indicative of the actual term relatedness? We first observe and quantify the uncertainty of word embedding models with respect to the similarity values they generate. Based on this, we introduce a general threshold which effectively filters related terms. We explore the effect of dimensionality on this general threshold by conducting the experiments in different vector dimensions. Our evaluation on four test collections with four relevance scoring models supports the effectiveness of our approach, as the results of the proposed threshold are significantly better than the baseline while being equal to, or statistically indistinguishable from, the optimal results.", "meta": {}, "annotation_approver": null, "labels": [[216, 293, "Task"], [0, 14, "Method"], [344, 365, "Method"]]}
{"id": 4633, "text": "Twitter as an information dissemination tool has proved to be instrumental in generating user curated content in short spans of time. Tweeting usually occurs when reacting to events, speeches, about a service or product. This in some cases comes with its fair share of blame on varied aspects in reference to say an event. Our work in progress details how we plan to collect the informal texts, clean them and extract features for blame detection. We are interested in augmenting Recurrent Neural Networks (RNN) with selfdeveloped association rules in getting the most out of the data for training and evaluation. We aim to test the performance of our approach using human-induced terror-related tweets corpus. It is possible tailoring the model to fit natural disaster scenarios.", "meta": {}, "annotation_approver": null, "labels": [[0, 7, "Material"], [431, 446, "Task"], [517, 548, "Method"], [667, 709, "Material"], [480, 511, "Method"]]}
{"id": 4634, "text": "This paper investigates research specialisation of top ranked universities around the world. The revealed comparative advantage in different research fields are determined according to the number of research articles published. Subsequently, measures of research ubiquity and diversity, and research complexity index of each university, are obtained and discussed. The study is conducted on top-ranked universities according to Shanghai Jiao Tong Academic Ranking of World Universities, with bibliographical details extracted Microsoft Academic Graph data set and research fields of journals labelled with SCImago Journal Classification. Diversity-ubiquity distributions, relevance of RCI and university ranks, and geographical RCI distributions are examined in this paper.", "meta": {}, "annotation_approver": null, "labels": [[526, 559, "Material"], [607, 636, "Material"], [11, 91, "Task"], [189, 226, "Method"]]}
{"id": 4635, "text": "The content of today’s social media is becoming more and more rich, increasingly mixing text, images, videos, and audio. It is an intriguing research question to model the interplay between these different modes in attracting user attention and engagement. But in order to pursue this study of multimodal content, we must also account for context: timing effects, community preferences, and social factors (e.g., which authors are already popular) also affect the amount of feedback and reaction that social-media posts receive. In this work, we separate out the influence of these noncontent factors in several ways. First, we focus on ranking pairs of submissions posted to the same community in quick succession, e.g., within 30 seconds; this framing encourages models to focus on time-agnostic and community-specific content features. Within that setting, we determine the relative performance of author vs. content features. We find that victory usually belongs to “cats and captions,” as visual and textual features together tend to outperform identity-based features. Moreover, our experiments show that when considered in isolation, simple unigram text features and deep neural network visual features yield the highest accuracy individually, and that the combination of the two modalities generally leads to the best accuracies overall.", "meta": {}, "annotation_approver": null, "labels": [[1228, 1236, "Metric"], [1148, 1169, "Method"], [1174, 1209, "Method"], [162, 255, "Task"]]}
{"id": 4636, "text": "The mission of LinkedIn is to connect the world's professionals to make them more productive and successful. LinkedIn operates the world's largest professional network on the Internet with more than 500 Million members in over 200 countries. Core to realizing the mission is to help people find jobs. In this paper, we describe how the jobs recommendations is powered by a search index and some practical challenges involved in scaling such a system.", "meta": {}, "annotation_approver": null, "labels": [[328, 385, "Task"], [109, 117, "Material"], [15, 23, "Material"]]}
{"id": 4637, "text": "Link prediction on knowledge graphs is useful in numerous application areas such as semantic search, question answering, entity disambiguation, enterprise decision support, recommender systems and so on. While many of these applications require a reasonably quick response and may operate on data that is constantly changing, existing methods often lack speed and adaptability to cope with these requirements. This is aggravated by the fact that knowledge graphs are often extremely large and may easily contain millions of entities rendering many of these methods impractical. In this paper, we address the weaknesses of current methods by proposing Random Semantic Tensor Ensemble (RSTE), a scalable ensemble-enabled framework based on tensor factorization. Our proposed approach samples a knowledge graph tensor in its graph representation and performs link prediction via ensembles of tensor factorization. Our experiments on both publicly available datasets and real world enterprise/sales knowledge bases have shown that our approach is not only highly scalable, parallelizable and memory efficient, but also able to increase the prediction accuracy significantly across all datasets.", "meta": {}, "annotation_approver": null, "labels": [[0, 35, "Task"], [978, 1010, "Material"], [935, 962, "Material"], [1147, 1155, "Metric"], [84, 99, "Task"], [101, 119, "Task"], [121, 142, "Task"], [651, 689, "Method"], [856, 871, "Task"]]}
{"id": 4638, "text": "Time series (TS) occur in many scientific and commercial applications, ranging from earth surveillance to industry automation to the smart grids. An important type of TS analysis is classification, which can, for instance, improve energy load forecasting in smart grids by detecting the types of electronic devices based on their energy consumption profiles recorded by automatic sensors. Such sensor-driven applications are very often characterized by (a) very long TS and (b) very large TS datasets needing classification. However, current methods to time series classification (TSC) cannot cope with such data volumes at acceptable accuracy; they are either scalable but offer only inferior classification quality, or they achieve state-of-the-art classification quality but cannot scale to large data volumes. In this paper, we present WEASEL (Word ExtrAction for time SEries cLassification), a novel TSC method which is both fast and accurate. Like other state-of-the-art TSC methods, WEASEL transforms time series into feature vectors, using a sliding-window approach, which are then analyzed through a machine learning classifier. The novelty of WEASEL lies in its specific method for deriving features, resulting in a much smaller yet much more discriminative feature set. On the popular UCR benchmark of 85 TS datasets, WEASEL is more accurate than the best current non-ensemble algorithms at orders-of-magnitude lower classification and training times, and it is almost as accurate as ensemble classifiers, whose computational complexity makes them inapplicable even for mid-size datasets. The outstanding robustness of WEASEL is also confirmed by experiments on two real smart grid datasets, where it out-of-the-box achieves almost the same accuracy as highly tuned, domain-specific methods.", "meta": {}, "annotation_approver": null, "labels": [[990, 996, "Method"], [1050, 1073, "Method"], [1109, 1136, "Method"], [1153, 1159, "Method"], [1296, 1309, "Material"], [1316, 1327, "Material"], [1329, 1335, "Method"], [1447, 1461, "Metric"], [1630, 1636, "Method"], [1676, 1701, "Material"], [1752, 1760, "Metric"], [553, 585, "Task"], [840, 895, "Method"], [1344, 1352, "Metric"]]}
{"id": 4639, "text": "The design of a Web search evaluation metric is closely related with how the user's interaction process is modeled. Each behavioral model results in a different metric used to evaluate search performance. In these models and the user behavior assumptions behind them, when a user ends a search session is one of the prime concerns because it is highly related to both benefit and cost estimation. Existing metric design usually adopts some simplified criteria to decide the stopping time point: (1) upper limit for benefit (e.g. RR, AP); (2) upper limit for cost (e.g. Precision@N, DCG@N). However, in many practical search sessions (e.g. exploratory search), the stopping criterion is more complex than the simplified case. Analyzing benefit and cost of actual users' search sessions, we find that the stopping criteria vary with search tasks and are usually combination effects of both benefit and cost factors. Inspired by a popular computer game named Bejeweled, we propose a Bejeweled Player Model (BPM) to simulate users' search interaction processes and evaluate their search performances. In the BPM, a user stops when he/she either has found sufficient useful information or has no more patience to continue. Given this assumption, a new evaluation framework based on upper limits (either fixed or changeable as search proceeds) for both benefit and cost is proposed. We show how to derive a new metric from the framework and demonstrate that it can be adopted to revise traditional metrics like Discounted Cumulative Gain (DCG), Expected Reciprocal Rank (ERR) and Average Precision (AP). To show effectiveness of the proposed framework, we compare it with a number of existing metrics in terms of correlation between user satisfaction and the metrics based on a dataset that collects users' explicit satisfaction feedbacks and assessors' relevance judgements. Experiment results show that the framework is better correlated with user satisfaction feedbacks.", "meta": {}, "annotation_approver": null, "labels": [[173, 203, "Task"], [1104, 1107, "Method"], [1247, 1289, "Method"], [569, 580, "Metric"], [582, 587, "Metric"], [980, 1008, "Method"], [1505, 1537, "Metric"], [1539, 1569, "Metric"], [1574, 1596, "Metric"], [4, 44, "Task"]]}
{"id": 4640, "text": "With the exponential growth of online social network services such as Facebook and Twitter, social networks and social medias become more and more important, directly influencing politics, economics, and our daily life. Mining big social networks aims to collect and analyze web-scale social data to reveal patterns of individual and group behaviors. It is an inherently interdisciplinary academic field which emerged from sociology, psychology, statistics, and graph theory. In this article, I briefly survey recent progress on social network mining with an emphasis on understanding the interactions among users in the large dynamic social networks. I will start with some basic knowledge for social network analysis, including methodologies and tools for macro-level, meso-level and microlevel social network analysis. Then I will give an overall roadmap of social network mining. After that, I will describe methodologies for modeling user behavior including state-of-the-art methods for learning user profiles, and introduce recent progress on modeling dynamics of user behaviors using deep learning. Then I will present models and algorithms for quantitative analysis on social interactions including homophily and social influence.Finally, I will introduce network structure model including social group formation, and network topology generation. We will introduce recent developed network embedding algorithms for modeling social networks with the embedding techniques. Finally, I will use several concrete examples from Alibaba, the largest online shopping website in the world, and WeChat, the largest social messaging service in China, to explain how online social networks influence our offline world.", "meta": {}, "annotation_approver": null, "labels": [[529, 550, "Task"], [1091, 1104, "Method"], [695, 718, "Task"], [797, 820, "Task"], [861, 882, "Task"], [1593, 1599, "Material"], [1530, 1537, "Material"]]}
{"id": 4641, "text": "One of the central issues in learning to rank for information retrieval is to develop algorithms that construct ranking models by directly optimizing evaluation measures such as normalized discounted cumulative gain~(ND CG). Existing methods usually focus on optimizing a specific evaluation measure calculated at a fixed position, e.g., NDCG calculated at a fixed position K. In information retrieval the evaluation measures, including the widely used NDCG and P@K, are usually designed to evaluate the document ranking at all of the ranking positions, which provide much richer information than only measuring the document ranking at a single position. Thus, it is interesting to ask if we can devise an algorithm that has the ability of leveraging the measures calculated at all of the ranking postilions, for learning a better ranking model. In this paper, we propose a novel learning to rank model on the basis of Markov decision process (MDP), referred to as MDPRank. In the learning phase of MDPRank, the construction of a document ranking is considered as a sequential decision making, each corresponds to an action of selecting a document for the corresponding position. The policy gradient algorithm of REINFORCE is adopted to train the model parameters. The evaluation measures calculated at every ranking positions are utilized as the immediate rewards to the corresponding actions, which guide the learning algorithm to adjust the model parameters so that the measure is optimized. Experimental results on LETOR benchmark datasets showed that MDPRank can outperform the state-of-the-art baselines.", "meta": {}, "annotation_approver": null, "labels": [[338, 342, "Metric"], [453, 457, "Metric"], [461, 465, "Metric"], [50, 71, "Task"], [178, 223, "Metric"], [380, 401, "Task"], [880, 902, "Method"], [919, 948, "Method"], [965, 972, "Method"], [999, 1006, "Method"], [1556, 1563, "Method"], [1519, 1543, "Material"], [29, 45, "Method"], [112, 119, "Task"]]}
{"id": 4642, "text": "Recently, deep learning has been widely studied to recognize ground objects with satellite imageries. However, finding ground truths especially for developing and rural areas is quite hard and manually labeling a large set of training data is costly. In this work, we propose an ongoing research named DeepVGI which aims at deeply learning from satellite imageries with the supervision of Volunteered Geographic Information (VGI). VGI data from OpenStreetMap (OSM) and a crowdsourcing mobile application named MapSwipe which allows volunteers to label images with buildings or roads for humanitarian aids are utilized. Meanwhile, an active learning framework with deep neural networks is developed by incorporating both VGI data with more complete supervision knowledge. Our experiments show that DeepVGI can achieve high building detection performance for humanitarian mapping in rural African areas.", "meta": {}, "annotation_approver": null, "labels": [[51, 75, "Task"], [10, 23, "Method"], [324, 339, "Method"], [445, 464, "Material"], [664, 685, "Method"], [633, 648, "Method"]]}
{"id": 4643, "text": "In this study, we investigate diversified recommendation problem by supervised learning, seeking significant improvement in diversity while maintaining accuracy. In particular, we regard each user as a training instance, and heuristically choose a subset of accurate and diverse items as groundtruth for each user. We then represent each user or item as a vector resulted from the factorization of the user-item rating matrix. In our paper, we try to discover a factorization for matching the following supervised learning task. In doing this, we define two coupled optimization problems, parameterized matrix factorization and structural learning, to formulate our task. And we propose a diversified collaborative filtering algorithm (DCF) to solve the coupled problems. We also introduce a new pairwise accuracy metric and a normalized topic coverage diversity metric to measure the performance of accuracy and diversity respectively. Extensive experiments on benchmark datasets show the performance gains of DCF in comparison with the state-of-the-art algorithms.", "meta": {}, "annotation_approver": null, "labels": [[30, 56, "Task"], [152, 160, "Metric"], [900, 908, "Metric"], [503, 522, "Task"], [689, 740, "Method"], [628, 647, "Task"], [589, 624, "Task"], [796, 820, "Metric"], [827, 869, "Metric"], [913, 922, "Metric"], [1011, 1014, "Method"], [68, 87, "Task"]]}
{"id": 4644, "text": "We consider the reachability indexing problem for privatepublic directed graphs. In these graphs nodes come in three flavors: public—nodes visible to all users, private—nodes visible to a specific set of users, and protected—nodes visible to any user who can see at least one of the node’s parents. We are interested in computing the set of nodes visible to a specific user online. There are two obvious algorithms: precompute the result for every user, or run a reachability algorithm at query time. This paper explores the trade-off between these two strategies. Our approach is to identify a set of additional visible seed nodes for each user. The online reachability algorithm explores the graph starting at these nodes. We first formulate the problem as asymmetric k-center with outliers, and then give an efficient and practical algorithm. We prove new theoretical guarantees for this problem and show empirically that it performs very well in practice.", "meta": {}, "annotation_approver": null, "labels": [[16, 37, "Task"]]}
{"id": 4645, "text": "A fundamental concern in search engines is to determine which documents have the best content for satisfying the user, based on analysis of the user's query and the text of documents. For this query-content match, many learning to rank systems make use of IR features developed in the 1990s in the TREC framework. Such features are still important in a variety of search tasks, and particularly in the long tail where clicks, links and social media signals become sparse. I will present our current progress, in particular three different neural models, with the goal of surpassing the 1990s models in full text search. This will include evidence, using proprietary Bing datasets, that large-scale training data can be useful. I will also argue that for the field to make progress on query-content relevance modeling, it may be valuable to set up a shared blind evaluation similar to 1990s TREC, possibly with large-scale training data.", "meta": {}, "annotation_approver": null, "labels": [[666, 679, "Material"], [193, 212, "Task"], [219, 235, "Method"], [298, 312, "Material"], [539, 552, "Method"]]}
{"id": 4646, "text": "The availability of novel digital data streams that can be used as proxy for monitoring infectious disease incidence is ushering in a new era for real-time forecast approaches to disease spreading. Here, we propose the first seasonal influenza forecast framework based on a stochastic, spatially structured mechanistic model (individual level microsimulation) initialized with geo-localized microblogging data. The framework provides for more than 600 census areas in the United States, Italy and Spain, the initial conditions for a stochastic epidemic computational model that generates an ensemble of forecasts for the main indicators of the epidemic season: peak time and intensity. We evaluate the forecasts accuracy and reliability by comparing the results from our framework with the data from the official influenza surveillance systems in the US, Italy and Spain in the seasons 2014/15 and 2015/16. In all countries studied, the proposed framework provides reliable results with leads of up to 6 weeks that became more stable and accurate with progression of the season. The results for the United States have been generated in real-time in the context of the Centers for Disease Control and Prevention “Forecasting the Influenza Season Challenge”. A characteristic feature of the mechanistic modeling approach is in the explicit estimate of key epidemiological parameters relevant for public health decision-making that cannot be achieved with statistical models not considering the disease dynamic. Furthermore, the presented framework allows the fusion of multiple data streams in the initialization stage and can be enriched with census, weather and socioeconomic data.", "meta": {}, "annotation_approver": null, "labels": [[77, 106, "Task"], [452, 502, "Material"], [225, 262, "Method"], [377, 409, "Material"], [702, 720, "Metric"]]}
{"id": 4647, "text": "Increasing test collection sizes and limited judgment budgets create measurement challenges for IR batch evaluations, challenges that are greater when using deep effectiveness metrics than when using shallow metrics, because of the increased likelihood that unjudged documents will be encountered. Here we study the problem of metric score adjustment, with the goal of accurately estimating system performance when using deep metrics and limited judgment sets, assuming that dynamic score adjustment is required per topic due to the variability in the number of relevant documents. We seek to induce system orderings that are as close as is possible to the orderings that would arise if full judgments were available. Starting with depth-based pooling, and no prior knowledge of sampling probabilities, the first phase of our two-stage process computes a background gain for each document based on rank-level statistics. The second stage then accounts for the distributional variance of relevant documents. We also exploit the frequency statistics of pooled relevant documents in order to determine a threshold for dynamically determining the set of topics to be adjusted. Taken together, our results show that: (i) better score estimates can be achieved when compared to previous work; (ii) by setting a global threshold, we are able to adapt our methods to different collections; and (iii) the proposed estimation methods reliably approximate the system orderings achieved when many more relevance judgments are available. We also consider pools generated by a two-strata sampling approach.", "meta": {}, "annotation_approver": null, "labels": [[327, 350, "Task"], [732, 751, "Method"]]}
{"id": 4648, "text": "In this work we address the development of a smart personal assistant that is capable of anticipating a user's information needs based on a novel type of context: the person's activity inferred from her check-in records on a location-based social network. Our main contribution is a method that translates a check-in activity into an information need, which is in turn addressed with an appropriate information card. This task is challenging because of the large number of possible activities and related information needs, which need to be addressed in a mobile dashboard that is limited in size. Our approach considers each possible activity that might follow after the last (and already finished) activity, and selects the top information cards such that they maximize the likelihood of satisfying the user's information needs for all possible future scenarios. The proposed models also incorporate knowledge about the temporal dynamics of information needs. Using a combination of historical check-in data and manual assessments collected via crowdsourcing, we show experimentally the effectiveness of our approach.", "meta": {}, "annotation_approver": null, "labels": [[985, 1009, "Material"], [28, 69, "Task"], [1047, 1060, "Method"]]}
{"id": 4649, "text": "Migration estimates are sensitive to definitions of time interval and duration. For example, when does a tourist become a migrant? As a result, harmonizing across different kinds of estimates or data sources can be difficult. Moreover in countries like the United States, that do not have a national registry system, estimates of internal migration typically rely on survey data that can require over a year from data collection to publication. In addition, each survey can ask only a limited set questions about migration (e.g., where did you live a year ago? where did you live five years ago?). We leverage a sample of geo-referenced Twitter tweets for about 62,000 users, spanning the period between 2010 and 2016, to estimate a series of US internal migration flows under varying time intervals and durations. Our findings, expressed in terms of 'migration curves', document, for the first time, the relationships between short-term mobility and long-term migration. The results open new avenues for demographic research. More specifically, future directions include the use of migration curves to produce probabilistic estimates of long-term migration from short-term (and vice versa) and to nowcast mobility rates at different levels of spatial and temporal granularity using a combination of previously published American Community Survey data and up-to-date data from a panel of Twitter users.", "meta": {}, "annotation_approver": null, "labels": [[0, 19, "Task"], [622, 651, "Material"]]}
{"id": 4650, "text": "Exploring demographics and social networks of Internet users are widely used for many applications such as recommendation systems. The popularity of mobile devices (e.g., smartphones) and location-based Internet services (e.g., Google Maps) facilitates the collection of users’ locations over time. Despite recent efforts to predict users’ attributes (e.g., age and gender) and social networks based on utilizing the rich location context knowledge (e.g., name, type, and description) of places of interest (e.g., restaurants and hotels) they checked-in on location-based online social networks such as Foursqure and Gowalla, little attention has been given to inferring attributes and social networks of mobile device users based on their spatiotemporal trajectories with less/no location context knowledge. In this paper we collect logs of thousands of mobile devices’ network connections to wireless access points (APs) of two campuses, and investigate whether one can infer mobile device users’ demographic attributes and social networks solely from their spatiotemporal AP-trajectories. We develop a tensor factorization based method Dinfer to infer mobile device users’ demographic attributes from their AP-trajectories by leveraging prior knowledge, such as users’ social networks. We also propose a novel method Sinfer to learn social networks between mobile device users by exploring patterns of their AP-trajectories, such as fine-grained cooccurrence events (e.g., co-coming, co-leaving, and co-presenting duration). Experimental results on real-word datasets demonstrate the effectiveness of our methods.", "meta": {}, "annotation_approver": null, "labels": [[1105, 1138, "Method"], [834, 890, "Material"], [0, 60, "Task"], [325, 350, "Task"], [1149, 1198, "Task"], [1330, 1379, "Task"]]}
{"id": 4651, "text": "Nowadays we are drowning in data of various varieties. For all these mixed types and categories of data there exist even more different analysis approaches, often done in single hand-written solutions. We propose to extend HyPer, a main memory database system to a uniform data agent platform following the one system fits all approach for solving a wide variety of data analysis problems. We achieve this by applying a flexible operator concept to a set of various important data exploration algorithms. With that, HyPer solves analytical questions using clustering, classification, association rule mining and graph mining besides standard HTAP (Hybrid Transaction and Analytical Processing) workloads on the same database state. It enables to approach the full variety and volume of HTAP extended for data exploration (HTAPx), and only needs knowledge of already introduced SQL extensions that are automatically optimized by the database's standard optimizer. In this demo we will focus on the benefits and flexibility we create by using the SQL extensions for several well-known mining workloads. In our interactive webinterface for this project named HyPerInsight we demonstrate how HyPer outperforms the best open source competitor Apache Spark in common use cases in social media, geo-data, recommender systems and several other.", "meta": {}, "annotation_approver": null, "labels": [[366, 379, "Task"], [556, 566, "Method"], [568, 582, "Method"], [584, 607, "Method"], [612, 624, "Method"]]}
{"id": 4652, "text": "Recognizing definition sentences from free text corpora often requires hand-crafted patterns or explicitly labeled training instances. We present a distant supervision approach addressing this challenge without using explicitly labeled data. We use plausibly good but imperfect definition sentences from Wikipedia as references to annotate sentences in a target corpus based on text similarity measures such as ROUGE. Experimental results show our approach is highly effective, generating noisy but large, useful, and localized training instances. Definition sentence retrieval models trained using the synthesized training examples are more effective than those learned from manual judgments of a few thousand sentences. We also examine different text similarity measures for annotation, including both unsupervised and supervised ones. We show that our method can significantly benefit from supervised text similarity measures learned from either external training data (from the SemEval Semantic Text Similarity task) or local ones (a few hundred judged sentences on the target corpus). Our method offers a cheap, effective, and flexible solution to this task and can benefit a broad range of applications such as web search engines and QA systems.", "meta": {}, "annotation_approver": null, "labels": [[304, 313, "Material"], [411, 416, "Metric"], [0, 55, "Task"], [148, 167, "Method"]]}
{"id": 4653, "text": "Ensuring access to the most relevant knowledge contained in large ontologies has been identified as an important challenge. To this end, minimal modules (sub-ontologies that preserve all entailments over a given vocabulary) and excerpts (certain, small number of axioms that best capture the knowledge regarding the vocabulary by allowing for a degree of semantic loss) have been proposed. In this paper, we introduce the notion of subsumption justification as an extension of justification (a minimal set of axioms needed to preserve a logical consequence) to capture the subsumption knowledge between a term and all other terms in the vocabulary. We present algorithms for computing subsumption justifications based on a simulation notion developed for the problem of deciding the logical difference between ontologies. We show how subsumption justifications can be used to obtain minimal modules and to compute best excerpts by additionally employing a partial Max-SAT solver. This yields two state-of-the-art methods for computing all minimal modules and all best excerpts, which we evaluate over large biomedical ontologies.", "meta": {}, "annotation_approver": null, "labels": [[432, 457, "Task"], [0, 76, "Task"], [675, 711, "Task"], [834, 860, "Task"], [1101, 1128, "Material"]]}
{"id": 4654, "text": "We demonstrate BioNex, a system to mine, rank and visualize biomedical news events. BioNex takes biomedical queries such as \"Ebola virus disease\" and retrieves the k most relevant news events for them. To achieve this we first mine the generic news events by clustering them on a daily basis using general named entities and textual features. These clusters are also tagged with disambiguated biomedical entities which aid in biomedical news event exploration. The clusters are then used to compute the importance scores for the event clusters based on a combination of textual, semantic, popularity and historical importance features. BioNex also visualizes the retrieved event clusters to highlight the top news events and corresponding news articles for the given query. The visualization also provides the context for news events using (1) a chain of historically relevant news event clusters, and (2) other non-biomedical events from the same day.", "meta": {}, "annotation_approver": null, "labels": [[259, 269, "Method"], [426, 459, "Task"], [35, 82, "Task"]]}
{"id": 4655, "text": "A fundamental issue with current truth discovery methods is that they generally assume only one true value for each object, while in reality objects may have multiple true values. We propose a graph-based approach, called SmartMTD, to relax this assumption in truth discovery. SmartMTD models and quantifies two types of source relations to estimate source reliability precisely and to detect malicious agreement among sources for multi-truth discovery. Two graphs are constructed based on the modeled source relations, which are further used to derive two aspects of source reliability via random walk computation.", "meta": {}, "annotation_approver": null, "labels": [[193, 213, "Method"], [260, 275, "Task"]]}
{"id": 4656, "text": "Counting the frequency of small subgraphs is a fundamental technique in network analysis across various domains, most notably in bioinformatics and social networks. The special case of triangle counting has received much attention. Getting results for 4-vertex or 5-vertex patterns is highly challenging, and there are few practical results known that can scale to massive sizes. We introduce an algorithmic framework that can be adopted to count any small pattern in a graph and apply this framework to compute exact counts for all 5-vertex subgraphs. Our framework is built on cutting a pattern into smaller ones, and using counts of smaller patterns to get larger counts. Furthermore, we exploit degree orientations of the graph to reduce runtimes even further. These methods avoid the combinatorial explosion that typical subgraph counting algorithms face. We prove that it suffices to enumerate only four specific subgraphs (three of them have less than 5 vertices) to exactly count all 5-vertex patterns. We perform extensive empirical experiments on a variety of real-world graphs. We are able to compute counts of graphs with tens of millions of edges in minutes on a commodity machine. To the best of our knowledge, this is the first practical algorithm for 5-vertex pattern counting that runs at this scale. A stepping stone to our main algorithm is a fast method for counting all 4-vertex patterns. This algorithm is typically ten times faster than the state of the art 4-vertex counters.", "meta": {}, "annotation_approver": null, "labels": [[699, 750, "Method"], [72, 88, "Task"], [1267, 1292, "Task"]]}
{"id": 4657, "text": "Network representation is the basis of many applications and of extensive interest in various fields such as information retrieval, social network analysis, and recommendation systems. Majority of previous methods on network representation only considered incomplete aspects of the problem, such as link structure, node information, or partial integration. The present paper proposes a comprehensive network representation model, which seamlessly integrates the text information, node label, and first-order and second-order proximity of a network. The effectiveness of the introduced strategies is experimentally evaluated. Results demonstrate that our method is better than state-of-the-art techniques.", "meta": {}, "annotation_approver": null, "labels": [[0, 22, "Task"], [386, 428, "Method"], [217, 239, "Task"]]}
{"id": 4658, "text": "In this paper, we investigate a new form of blackhat search engine optimization that targets local listing services like Google Maps. Miscreants register abusive business listings in an attempt to siphon search traffic away from legitimate businesses and funnel it to deceptive service industries—such as unaccredited locksmiths—or to traffic-referral scams, often for the restaurant and hotel industry. In order to understand the prevalence and scope of this threat, we obtain access to over a hundred-thousand business listings on Google Maps that were suspended for abuse. We categorize the types of abuse affecting Google Maps; analyze how miscreants circumvented the protections against fraudulent business registration such as postcard mail verification; identify the volume of search queries affected; and ultimately explore how miscreants generated a profit from traffic that necessitates physical proximity to the victim. This physical requirement leads to unique abusive behaviors that are distinct from other online fraud such as pharmaceutical and luxury product scams.", "meta": {}, "annotation_approver": null, "labels": [[44, 79, "Task"], [533, 544, "Material"], [121, 132, "Material"], [619, 630, "Material"], [733, 759, "Task"]]}
{"id": 4659, "text": "Pseudo-relevance feedback (PRF) refers to a query expansion strategy based on top-retrieved documents, which has been shown to be highly effective in many retrieval models. Previous work has introduced a set of constraints (axioms) that should be satisfied by any PRF model. In this paper, we propose three additional constraints based on the proximity of feedback terms to the query terms in the feedback documents. As a case study, we consider the log-logistic model, a state-of-the-art PRF model that has been proven to be a successful method in satisfying the existing PRF constraints, and show that it does not satisfy the proposed constraints. We further modify the log-logistic model based on the proposed proximity-based constraints. Experiments on four TREC collections demonstrate the effectiveness of the proposed constraints. Our modification the log-logistic model leads to significant and substantial (up to 15%) improvements. Furthermore, we show that the proposed proximity-based function outperforms the well-known Gaussian kernel which does not satisfy all the proposed constraints.", "meta": {}, "annotation_approver": null, "labels": [[0, 31, "Method"], [44, 59, "Task"], [343, 415, "Method"], [762, 778, "Material"], [450, 468, "Method"], [672, 690, "Method"], [859, 877, "Method"]]}
{"id": 4660, "text": "Graph construction is an important process in graph-based semi-supervised learning. Presently, the mutual kNN graph is the most preferred as it reduces hub nodes which can be a cause of failure during the process of label propagation. However, the mutual kNN graph, which is usually very sparse, suffers from over sparsification problem. That is, although the number of edges connecting nodes that have different labels decreases in the mutual kNN graph, the number of edges connecting nodes that have the same labels also reduces. In addition, over sparsification can produce a disconnected graph, which is not desirable for label propagation. So we present a new graph construction method, the centered kNN graph, which not only reduces hub nodes but also avoids the over sparsification problem.", "meta": {}, "annotation_approver": null, "labels": [[99, 115, "Method"], [0, 18, "Task"], [46, 82, "Task"], [626, 643, "Task"], [248, 264, "Method"], [437, 453, "Method"], [696, 714, "Method"], [309, 328, "Task"], [545, 564, "Task"], [769, 788, "Task"]]}
{"id": 4661, "text": "CNN (Convolution Neural Network) is widely used in visual analysis and achieves exceptionally high performances in image classification, face detection, object recognition, image recoloring, and other learning jobs. Using deep learning frameworks, such as Torch and Tensorflow, CNN can be efficiently computed by leveraging the power of GPU. However, one drawback of GPU is its limited memory which prohibits us from handling large images. Passing a 4K resolution image to the VGG network will result in an exception of out-of-memory for Titan-X GPU. In this paper, we propose a new approach that adopts the BSP (bulk synchronization parallel) model to compute CNNs for images of any size. Before fed to a specific CNN layer, the image is split into smaller pieces which go through the neural network separately. Then, a specific padding and normalization technique is adopted to merge sub-images back into one image. Our approach can be easily extended to support distributed multi-GPUs. In this paper, we use neural style network as our example to illustrate the effectiveness of our approach. We show that using one Titan-X GPU, we can transfer the style of an image with 10,000×10,000 pixels within 1 minute.", "meta": {}, "annotation_approver": null, "labels": [[830, 865, "Method"], [0, 32, "Method"], [608, 649, "Method"], [51, 66, "Task"], [661, 665, "Method"], [670, 676, "Material"], [222, 246, "Method"], [278, 281, "Method"], [715, 718, "Method"], [730, 735, "Material"], [786, 800, "Method"]]}
{"id": 4662, "text": "Social media response to catastrophic events, such as natural disasters or terrorist attacks, has received a lot of attention. However, social media are also extremely important in the context of planned events, such as fairs, exhibits, festivals, as they play an essential role in communicating them to fans, interest groups, and the general population. These kinds of events are geo-localized within a city or territory and are scheduled within a public calendar. We consider a specific scenario, the Milano Fashion Week (MFW), which is an important event in our city. We focus our attention on the coverage of social content in space, measuring the propagation of the event in the territory. We build different clusters of fashion brands, we characterize several features of propagation in space and we correlate them to the popularity of involved actors. We show that the clusters along space and popularity dimensions are loosely correlated, and that domain experts are typically able to understand and identify only popularity aspects, while they are completely unaware of spatial dynamics of social media response to the events.", "meta": {}, "annotation_approver": null, "labels": [[0, 44, "Task"], [601, 636, "Task"]]}
{"id": 4663, "text": "Complex machine learning models are now an integral part of modern, large-scale retrieval systems. However, collection size growth continues to outpace advances in efficiency improvements in the learning models which achieve the highest effectiveness. In this paper, we re-examine the importance of tightly integrating feature costs into multi-stage learning-to-rank (LTR) IR systems. We present a novel approach to optimizing cascaded ranking models which can directly leverage a variety of different state-of-the-art LTR rankers such as LambdaMART and Gradient Boosted Decision Trees. Using our cascade model, we conclusively show that feature costs and the number of documents being re-ranked in each stage of the cascade can be balanced to maximize both efficiency and effectiveness. Finally, we also demonstrate that our cascade model can easily be deployed on commonly used collections to achieve state-of-the-art effectiveness results while only using a subset of the features required by the full model.", "meta": {}, "annotation_approver": null, "labels": [[597, 610, "Method"], [416, 450, "Task"], [349, 372, "Task"], [519, 522, "Task"], [539, 549, "Method"], [554, 585, "Method"]]}
{"id": 4664, "text": "Opioid (e.g., heroin and morphine) addiction has become one of the largest and deadliest epidemics in the United States. To combat such deadly epidemic, there is an urgent need for novel tools and methodologies to gain new insights into the behavioral processes of opioid abuse and addiction. The role of social media in biomedical knowledge mining has turned into increasingly significant in recent years. In this paper, we propose a novel framework named AutoDOA to automatically detect the opioid addicts from Twitter, which can potentially assist in sharpening our understanding toward the behavioral process of opioid abuse and addiction. In AutoDOA, to model the users and posted tweets as well as their rich relationships, a structured heterogeneous information network (HIN) is first constructed. Then meta-path based approach is used to formulate similarity measures over users and different similarities are aggregated using Laplacian scores. Based on HIN and the combined meta-path, to reduce the cost of acquiring labeled examples for supervised learning, a transductive classification model is built for automatic opioid addict detection. To the best of our knowledge, this is the first work to apply transductive classification in HIN into drug-addiction domain. Comprehensive experiments on real sample collections from Twitter are conducted to validate the effectiveness of our developed system AutoDOA in opioid addict detection by comparisons with other alternate methods. The results and case studies also demonstrate that knowledge from daily-life social media data mining could support a better practice of opioid addiction prevention and treatment.", "meta": {}, "annotation_approver": null, "labels": [[513, 520, "Material"], [1335, 1342, "Material"], [321, 348, "Task"], [468, 507, "Task"], [686, 692, "Material"], [1047, 1066, "Task"], [1117, 1150, "Task"], [1070, 1104, "Method"], [1214, 1241, "Method"], [1422, 1445, "Task"], [1568, 1592, "Task"], [743, 782, "Method"], [1245, 1248, "Method"], [962, 965, "Method"]]}
{"id": 4665, "text": "Evaluating the impact of scholarly papers plays an important role for addressing recruitment decision, funding allocation and promotion, etc. Yet little is known how actual geographic distance influences the impact of scholarly papers. In this paper, we leverage the law of geographic distance and citations between different institutions to weight quantum Pagerank algorithm for objectively measuring the impact of scholarly papers. The results indicate that the weighted quantum PageRank algorithm can better differentiate the impact of scholarly papers compared to PageRank algorithm.", "meta": {}, "annotation_approver": null, "labels": [[0, 41, "Task"], [464, 499, "Method"], [568, 586, "Method"]]}
{"id": 4666, "text": "Increasing amount of urban data are being accumulated and released to public; this enables us to study the urban dynamics and address urban issues such as crime, traffic, and quality of living. In this paper, we are interested in learning vector representations for regions using the large-scale taxi flow data. These representations could help us better measure the relationship strengths between regions, and the relationships can be used to better model the region properties. Different from existing studies, we propose to consider both temporal dynamics and multi-hop transitions in learning the region representations. We propose to jointly learn the representations from a flow graph and a spatial graph. Such a combined graph could simulate individual movements and also addresses the data sparsity issue. We demonstrate the effectiveness of our method using three different real datasets.", "meta": {}, "annotation_approver": null, "labels": [[541, 584, "Method"], [21, 31, "Material"], [588, 623, "Task"], [230, 261, "Task"], [284, 310, "Material"]]}
{"id": 4667, "text": "Image search engines show results differently from general Web search engines in three key ways: (1) most Web-based image search engines adopt the two-dimensional result placement instead of the linear result list; (2) image searches show snapshots instead of snippets (query-dependent abstracts of landing pages) on search engine result pages (SERPs); and (3) pagination is usually not (explicitly) supported on image search SERPs, and users can view results without having to click on the \"next page'' button. Compared with the extensive study of user behavior in general Web search scenarios, there exists no thorough investigation how the different interaction mechanism of image search engines affects users' examination behavior. To shed light on this research question, we conducted an eye-tracking study to investigate users' examination behavior in image searches. We focus on the impacts of factors in examination including position, visual saliency, edge density, the existence of textual information, and human faces in result images. Three interesting findings indicate users' behavior biases: (1) instead of the traditional \"Golden Triangle'' phenomena in the user examination patterns of general Web search, we observe a middle-position bias, (2) besides the position factor, the content of image results (e.g., visual saliency) affects examination behavior, and (3) some popular behavior assumptions in general Web search (e.g., examination hypothesis) do not hold in image search scenarios. We predict users' examination behavior with different impact factors. Results show that combining position and visual content features can improve prediction in image searches.", "meta": {}, "annotation_approver": null, "labels": [[0, 20, "Task"], [793, 805, "Method"], [814, 872, "Task"], [1511, 1546, "Task"]]}
{"id": 4668, "text": "Preparing a comprehensive, accurate, and unbiased report on a given topic or question is a challenging task. The first step is often a daunting discovery task that requires searching through an overwhelming number of information sources without introducing bias from the analyst’s current knowledge or limitations of the information sources. A common requirement for many analysis reports is a deep understanding of various kinds of historical and ongoing events that are reported in the media. To enable better analysis based on events, there exist several event databases containing structured representations of events extracted from news articles. Examples include GDELT [4], ICEWS [1], and EventRegistry [3]. These event databases have been successfully used to perform various kinds of analysis tasks, e.g., forecasting societal events [6]. However, there has been little work on the discovery aspect of the analysis, that results in a gap between the information requirements and the available data, and potentially a biased view of the available information. In this presentation, we describe a framework for concept discovery over event databases using semantic technologies. Unlike existing concept discovery solutions that perform discovery over text documents and in isolation from the remaining data analysis tasks [5, 8], our goal is providing a unified solution that allows deep understanding of the same data that will be used to perform other analysis tasks (e.g., hypothesis generation [7] or building models for forecasting [2]). Figure 1 shows the architecture of our system. The system takes in as input a set of event databases and RDF knowledge bases and provides as output a set of APIs that provide a unified retrieval mechanism over input data and knowledge bases, and an interface to a number of concept discovery algorithms. Figures 2 shows different portions of our system’s UI that is built using our concept discovery framework APIs. The analyst can enter a natural language question or a set of concepts, and retrieve collections of relevant concepts identified and ranked using different concept discovery algorithms. A key aspect of our framework is the use of semantic technologies. In particular:", "meta": {}, "annotation_approver": null, "labels": [[695, 708, "Material"], [680, 685, "Material"], [669, 674, "Material"], [0, 85, "Task"], [1117, 1134, "Task"], [1201, 1218, "Task"]]}
{"id": 4669, "text": "Recommending lifestyle articles is of immediate interest to the e-commerce industry and is beginning to attract research attention. Often followed strategies, such as recommending popular items are inadequate for this vertical because of two reasons. Firstly, users have their own personal preference over items, referred to as personal styles, which lead to the long-tail phenomenon. Secondly, each user displays multiple personas, each persona has a preference over items which could be dictated by a particular occasion, e.g. dressing for a party would be different from dressing to go to office. Recommendation in this vertical is crucially dependent on discovering styles for each of the multiple personas. There is no literature which addresses this problem.\n We posit a generative model which describes each user by a Simplex Over PERsona, SOPER, where a persona is described as the individuals preferences over prevailing styles modelled as topics over items. The choice of simplex and the long-tail nature necessitates the use of stick-breaking process. The main technical contribution is an efficient collapsed Gibbs sampling based algorithm for solving the attendant inference problem.\n Trained on large-scale interaction logs spanning more than half-a-million sessions collected from an e-commerce portal, SOPER outperforms previous baselines such as [9] by a large margin of 35% in identifying persona. Consequently it outperforms several competitive baselines comprehensively on the task of recommending from a catalogue of roughly 150 thousand lifestyle articles, by improving the recommendation quality as measured by AUC by a staggering 12.23%, in addition to aiding the interpretability of uncovered personal and fashionable styles thus advancing our precise understanding of the underlying phenomena.", "meta": {}, "annotation_approver": null, "labels": [[0, 31, "Task"], [777, 793, "Method"], [1634, 1637, "Metric"], [825, 852, "Method"], [1168, 1195, "Task"], [1209, 1237, "Material"], [1299, 1316, "Material"], [1318, 1323, "Method"], [1395, 1414, "Task"], [1559, 1577, "Material"]]}
{"id": 4670, "text": "Rule-based diagnostics of equipment is an important task in industry. In this paper we present how semantic technologies can enhance diagnostics. In particular, we present our semantic rule language sigRL that is inspired by the real diagnostic languages used in Siemens. SigRL allows to write compact yet powerful diagnostic programs by relying on a high level data independent vocabulary, diagnostic ontologies, and queries over these ontologies. We study computational complexity of SigRL: execution of diagnostic programs, provenance computation, as well as automatic verification of redundancy and inconsistency in diagnostic programs.", "meta": {}, "annotation_approver": null, "labels": [[0, 35, "Task"], [272, 277, "Method"], [486, 491, "Method"], [176, 204, "Method"]]}
{"id": 4671, "text": "Counting graphlets is a well-studied problem in graph mining and social network analysis. Recently, several papers explored very simple and natural approaches based on Monte Carlo sampling of Markov Chains (MC), and reported encouraging results. We show, perhaps surprisingly, that this approach is outperformed by a carefully engineered version of color coding (CC) [1], a sophisticated algorithmic technique that we extend to the case of graphlet sampling and for which we prove strong statistical guarantees. Our computational experiments on graphs with millions of nodes show CC to be more accurate than MC. Furthermore, we formally show that the mixing time of the MC approach is too high in general, even when the input graph has high conductance. All this comes at a price however. While MC is very efficient in terms of space, CC's memory requirements become demanding when the size of the input graph and that of the graphlets grow. And yet, our experiments show that a careful implementation of CC can push the limits of the state of the art, both in terms of the size of the input graph and of that of the graphlets.", "meta": {}, "annotation_approver": null, "labels": [[0, 18, "Task"], [349, 361, "Method"], [594, 602, "Metric"], [47, 60, "Task"], [65, 88, "Task"], [168, 210, "Method"], [349, 366, "Method"], [440, 457, "Task"], [580, 582, "Method"], [608, 610, "Method"], [670, 672, "Method"], [795, 797, "Method"], [835, 837, "Method"], [1005, 1008, "Method"]]}
{"id": 4672, "text": "Political texts are pervasive on the Web covering laws and policies in national and supranational jurisdictions. Access to this data is crucial for government transparency and accountability to the population. The main aim of our research is developing a ranking method for political documents which captures the interesting content within political documents. Text interestingness is a measure of assessing the quality of documents from users' perspective which shows their willingness to read a document. Different approaches are proposed for measuring the interestingness of texts. In this research we focus on measuring political texts' interestingness. As political data sources, we use publicly available parliamentary proceedings.", "meta": {}, "annotation_approver": null, "labels": [[255, 293, "Task"], [711, 736, "Material"], [545, 583, "Task"], [614, 656, "Task"], [661, 683, "Material"], [0, 15, "Material"]]}
{"id": 4673, "text": "User attributes including online behavior history and demographic information are the keys to decide whether a user is the right audience for an advertisement. When a user visits a website, the website generally plugs a browser cookie string (`bcookie' for short). The bcookie is then used as an identifier to collect the user's online behavior, as well as the joint key to link user profile attributes, such as demographic information and browsing history. However, the same users can have different bcookies across different browsers and devices. Moreover, bcookies can expire after some period, be cleared by browsers or users. This situation of bcookie discounting typically introduces both performance and delivery problems in online advertising since advertisers are hard to find the most receptive audiences based on the user profile information. In this paper, we try to tackle this problem by using an `assistant identifier' to find the linkage between different bcookies. For most of the Internet company, in addition to the bcookie information, there are always other identifiers such as IP address, user agent, OS type and version, etc., stored in the serving log data. Therefore, we propose an unified framework to link different bcookies from the same users according to those assistant identifiers. Specifically, our proposed method first constructs a bipartite graph with linkages between the assistant identifiers and the bcookies. Next all attributes associated with each bcookie are propagated along the graph using the state-of-the-art random walk model. Offline comparative experimental studies are conducted to confirm that by enriching the bcookie attributes we can recover 20% more online users whose bcookie information is lost, which is greatly helpful to delivery more budget spending with a little loss in precision of predicting converted users. On-product evaluation further confirms the effectiveness of the proposed method.", "meta": {}, "annotation_approver": null, "labels": [[946, 980, "Task"], [912, 932, "Method"], [310, 344, "Task"], [374, 402, "Task"], [1556, 1573, "Method"]]}
{"id": 4674, "text": "This work studies the combination of a document retrieval and a relation extraction system for the purpose of identifying query-relevant relational facts. On the TREC Web collection, we assess extracted facts separately for correctness and relevance. Despite some TREC topics not being covered by the relation schema, we find that this approach reveals relevant facts, and in particular those not yet known in the knowledge base DBpedia. The study confirms that mention frequency, document relevance, and entity relevance are useful indicators for fact relevance. Still, the task remains an open research problem.", "meta": {}, "annotation_approver": null, "labels": [[162, 181, "Material"], [429, 436, "Material"], [39, 90, "Method"], [39, 57, "Task"], [64, 83, "Task"], [110, 153, "Task"], [264, 268, "Material"]]}
{"id": 4675, "text": "The web is a huge source of valuable information. However, in recent times, there is an increasing trend towards false claims in social media, other web-sources, and even in news. Thus, factchecking websites have become increasingly popular to identify such misinformation based on manual analysis. Recent research proposed methods to assess the credibility of claims automatically. However, there are major limitations: most works assume claims to be in a structured form, and a few deal with textual claims but require that sources of evidence or counter-evidence are easily retrieved from the web. None of these works can cope with newly emerging claims, and no prior method can give user-interpretable explanations for its verdict on the claim’s credibility. This paper overcomes these limitations by automatically assessing the credibility of emerging claims, with sparse presence in web-sources, and generating suitable explanations from judiciously selected sources. To this end, we retrieve diverse articles about the claim, and model the mutual interaction between: the stance (i.e., support or refute) of the sources, the language style of the articles, the reliability of the sources, and the claim’s temporal footprint on the web. Extensive experiments demonstrate the viability of our method and its superiority over prior works. We show that our methods work well for early detection of emerging claims, as well as for claims with limited presence on the web and social media.", "meta": {}, "annotation_approver": null, "labels": [[1388, 1416, "Task"], [1477, 1489, "Material"], [129, 141, "Material"], [186, 198, "Task"], [335, 381, "Task"], [805, 863, "Task"]]}
{"id": 4676, "text": "Product search is an important part of online shopping. In contrast to many search tasks, the objectives of product search are not confined to retrieving relevant products. Instead, it focuses on finding items that satisfy the needs of individuals and lead to a user purchase. The unique characteristics of product search make search personalization essential for both customers and e-shopping companies. Purchase behavior is highly personal in online shopping and users often provide rich feedback about their decisions (e.g. product reviews). However, the severe mismatch found in the language of queries, products and users make traditional retrieval models based on bag-of-words assumptions less suitable for personalization in product search. In this paper, we propose a hierarchical embedding model to learn semantic representations for entities (i.e. words, products, users and queries) from different levels with their associated language data. Our contributions are three-fold: (1) our work is one of the initial studies on personalized product search; (2) our hierarchical embedding model is the first latent space model that jointly learns distributed representations for queries, products and users with a deep neural network; (3) each component of our network is designed as a generative model so that the whole structure is explainable and extendable. Following the methodology of previous studies, we constructed personalized product search benchmarks with Amazon product data. Experiments show that our hierarchical embedding model significantly outperforms existing product search baselines on multiple benchmark datasets.", "meta": {}, "annotation_approver": null, "labels": [[0, 14, "Task"], [108, 122, "Task"], [307, 321, "Task"], [732, 746, "Task"], [776, 804, "Method"], [808, 851, "Task"], [1046, 1060, "Task"], [1070, 1098, "Method"], [1218, 1237, "Method"], [1441, 1455, "Task"], [1472, 1491, "Material"], [1519, 1548, "Method"], [1583, 1597, "Task"]]}
{"id": 4677, "text": "We tackle the problem of improving microblog retrieval algorithms by proposing a Feedback Concept Model for query expansion. In particular, we expand the query using knowledge information derived from Probase so that the expanded one could better reflect users’ search intent, which allows for microblog retrieval at a concept-level, rather than termlevel. In the proposed feedback concept model: (i) we mine the concept information implicit in short-texts based on the external knowledge bases; (ii) with the relevant concepts associated with short-texts, a mixture model is generated to estimate a concept language model; (iii) finally, we utilize the concept language model for query expansion. Moreover, we incorporate temporal prior into the proposed query expansion method to satisfy real-time information need. Finally, we test the generalization power of the feedback concept model on the TREC Microblog corpora. The experimental results demonstrate that the proposed model outperforms the previous methods for microblog retrieval significantly.", "meta": {}, "annotation_approver": null, "labels": [[35, 54, "Task"], [81, 103, "Method"], [294, 313, "Task"], [373, 395, "Method"], [479, 494, "Material"], [600, 622, "Method"], [654, 676, "Method"], [681, 696, "Task"], [756, 771, "Task"], [108, 123, "Task"], [867, 889, "Method"], [897, 919, "Material"], [1019, 1038, "Task"], [201, 208, "Material"]]}
{"id": 4678, "text": "Many of today's most widely used computing applications utilize social networking features and allow users to connect, follow each other, share content, and comment on others' posts. However, despite the widespread adoption of these features, there is little understanding of the consequences that social networking has on user retention, engagement, and online as well as offline behavior.\n Here, we study how social networks influence user behavior in a physical activity tracking application. We analyze 791 million online and offline actions of 6 million users over the course of 5 years, and show that social networking leads to a significant increase in users' online as well as offline activities. Specifically, we establish a causal effect of how social networks influence user behavior. We show that the creation of new social connections increases user online in-application activity by 30%, user retention by 17%, and user offline real-world physical activity by 7% (about 400 steps per day). By exploiting a natural experiment we distinguish the effect of social influence of new social connections from the simultaneous increase in user's motivation to use the app and take more steps. We show that social influence accounts for 55% of the observed changes in user behavior, while the remaining 45% can be explained by the user's increased motivation to use the app. Further, we show that subsequent, individual edge formations in the social network lead to significant increases in daily steps. These effects diminish with each additional edge and vary based on edge attributes and user demographics. Finally, we utilize these insights to develop a model that accurately predicts which users will be most influenced by the creation of new social network connections.", "meta": {}, "annotation_approver": null, "labels": [[259, 389, "Task"], [751, 794, "Task"], [401, 450, "Task"]]}
{"id": 4679, "text": "The major task of network embedding is to learn low-dimensional vector representations of social-network nodes. It facilitates many analytical tasks such as link prediction and node clustering and thus has attracted increasing attention. The majority of existing embedding algorithms are designed for unsigned social networks. However, many social media networks have both positive and negative links, for which unsigned algorithms have little utility. Recent findings in signed network analysis suggest that negative links have distinct properties and added value over positive links. This brings about both challenges and opportunities for signed network embedding. In addition, user attributes, which encode properties and interests of users, provide complementary information to network structures and have the potential to improve signed network embedding. Therefore, in this paper, we study the novel problem of signed social network embedding with attributes. We propose a novel framework SNEA, which exploits the network structure and user attributes simultaneously for network representation learning. Experimental results on link prediction and node clustering with real-world datasets demonstrate the effectiveness of SNEA.", "meta": {}, "annotation_approver": null, "labels": [[18, 35, "Task"], [42, 110, "Task"], [157, 172, "Task"], [177, 192, "Task"], [836, 860, "Task"], [472, 495, "Task"], [642, 666, "Task"], [918, 965, "Task"], [1078, 1109, "Task"], [1135, 1150, "Task"], [1155, 1170, "Task"], [1021, 1058, "Method"]]}
{"id": 4680, "text": "Building of an accurate predictive model of clinical time series for a patient is critical for understanding of the patient condition, its dynamics, and optimal patient management. Unfortunately, this process is not straightforward. First, patient-specific variations are typically large and population-based models derived or learned from many different patients are often unable to support accurate predictions for each individual patient. Moreover, time series observed for one patient at any point in time may be too short and insufficient to learn a high-quality patient-specific model just from the patient's own data. To address these problems we propose, develop and experiment with a new adaptive forecasting framework for building multivariate clinical time series models for a patient and for supporting patient-specific predictions. The framework relies on the adaptive model switching approach that at any point in time selects the most promising time series model out of the pool of many possible models, and consequently, combines advantages of the population, patient-specific and short-term individualized predictive models. We demonstrate that the adaptive model switching framework is very promising approach to support personalized time series prediction, and that it is able to outperform predictions based on pure population and patient-specific models, as well as, other patient-specific model adaptation strategies.", "meta": {}, "annotation_approver": null, "labels": [[95, 133, "Task"], [697, 727, "Method"], [873, 906, "Method"], [1166, 1200, "Method"], [1239, 1274, "Task"], [741, 781, "Method"], [0, 40, "Task"]]}
{"id": 4681, "text": "Displaying banner advertisements (in short, ads) on webpages has usually been discussed as an Internet economics topic where a publisher uses auction models to sell an online user's page view to advertisers and the one with the highest bid can have her ad displayed to the user. This is also called real-time bidding (RTB) and the ad displaying process ensures that the publisher's benefit is maximized or there is an equilibrium in ad auctions. However, the benefits of the other two stakeholders - the advertiser and the user - have been rarely discussed. In this paper, we propose a two-stage computational framework that selects a banner ad based on the optimized trade-offs among all stakeholders. The first stage is still auction based and the second stage re-ranks ads by considering the benefits of all stakeholders. Our metric variables are: the publisher's revenue, the advertiser's utility, the ad memorability, the ad click-through rate (CTR), the contextual relevance, and the visual saliency. To the best of our knowledge, this is the first work that optimizes trade-offs among all stakeholders in RTB by incorporating multimedia metrics. An algorithm is also proposed to determine the optimal weights of the metric variables. We use both ad auction datasets and multimedia datasets to validate the proposed framework. Our experimental results show that the publisher can significantly improve the other stakeholders' benefits by slightly reducing her revenue in the short-term. In the long run, advertisers and users will be more engaged, the increased demand of advertising and the increased supply of page views can then boost the publisher's revenue.", "meta": {}, "annotation_approver": null, "labels": [[855, 874, "Metric"], [880, 900, "Metric"], [927, 954, "Metric"], [990, 1005, "Metric"], [1252, 1272, "Material"], [1277, 1296, "Material"], [299, 322, "Task"], [906, 921, "Metric"], [960, 980, "Metric"], [586, 619, "Method"], [625, 701, "Task"]]}
{"id": 4682, "text": "Social media platforms such as weblogs and social networking sites provide Internet users with an unprecedented means to express their opinions and debate on a wide range of issues. Concurrently with their growing importance in public communication, social media platforms may foster echo chambers and filter bubbles: homophily and content personalization lead users to be increasingly exposed to conforming opinions. There is therefore a need for unbiased systems able to identify and provide access to varied viewpoints. To address this task, we propose in this paper a novel unsupervised topic model, the Social Network Viewpoint Discovery Model (SNVDM). Given a specific issue (e.g., U.S. policy) as well as the text and social interactions from the users discussing this issue on a social networking site, SNVDM jointly identifies the issue's topics, the users' viewpoints, and the discourse pertaining to the different topics and viewpoints. In order to overcome the potential sparsity of the social network (i.e., some users interact with only a few other users), we propose an extension to SNVDM based on the Generalized Pólya Urn sampling scheme (SNVDM-GPU) to leverage \"acquaintances of acquaintances\" relationships. We benchmark the different proposed models against three baselines, namely TAM, SN-LDA, and VODUM, on a viewpoint clustering task using two real-world datasets. We thereby provide evidence that our model SNVDM and its extension SNVDM-GPU significantly outperform state-of-the-art baselines, and we show that utilizing social interactions greatly improves viewpoint clustering performance.", "meta": {}, "annotation_approver": null, "labels": [[608, 656, "Method"], [473, 521, "Task"], [811, 816, "Method"], [1098, 1103, "Method"], [1117, 1166, "Method"], [1431, 1436, "Method"], [1455, 1464, "Method"], [1582, 1602, "Task"], [1331, 1351, "Task"], [578, 602, "Method"]]}
{"id": 4683, "text": "Text queries are naturally encoded with user intentions. An intention detection task tries to model and discover intentions that user encoded in text queries. Unlike conventional text classification tasks where the label of text is highly correlated with some topic-specific words, words from different topic categories tend to co-occur in medical related queries. Besides the existence of topic-specific words and word order, word correlations and the way words organized into sentence are crucial to intention detection tasks. In this paper, we present a neural network based jointly modeling approach to model and capture user intentions in medical related text queries. Regardless of the exact words in text queries, the proposed method incorporates two types of heterogeneous information: 1) pairwise word feature correlations and 2) part-of-speech tags of a sentence to jointly model user intentions. Variable-length text queries are first inherently taken care of by a fixed-size pairwise feature correlation matrix. Moreover, convolution and pooling operations are applied on feature correlations to fully exploit latent semantic structure within the query. Sentence rephrasing is finally introduced as a data augmentation technique to improve model generalization ability during model training. Experiment results on real world medical queries have shown that the proposed method is able to extract complete and precise user intentions from text queries.", "meta": {}, "annotation_approver": null, "labels": [[60, 79, "Task"], [179, 198, "Task"], [502, 521, "Task"], [557, 603, "Method"], [707, 719, "Material"], [145, 157, "Material"], [923, 935, "Material"], [340, 363, "Material"], [1326, 1352, "Material"], [1450, 1462, "Material"], [617, 640, "Task"], [644, 672, "Material"]]}
{"id": 4684, "text": "Advances made in sequencing technology have resulted in the sequencing of thousands of genomes. Novel analysis tools are needed to process these data and extract useful information. Such tools could aid in personalized medicine. As an example, we could identify the causes for a disease by comparing the genomes of people who have the disease and those who do not have this disease. Given that human variability happens due to single nucleotide polymorphisms (SNPs), we could focus our attention on these SNPs. Investigations that try to understand human variability using SNPs fall under genome-wide association study (GWAS). A crucial step in GWAS is the identification of the correlation between genotypes (SNPs) and phenotypes (i.e., characteristics such as the presence of a disease). This step can be modeled as the k-locus problem (where k is any integer). A number of algorithms have been proposed in the literature for this problem when k = 2. In this paper we present an algorithm for solving the 2-locus problem that is up to two orders of magnitude faster than the previous best known algorithms.", "meta": {}, "annotation_approver": null, "labels": [[253, 286, "Task"], [656, 730, "Task"], [60, 94, "Task"], [822, 837, "Method"]]}
{"id": 4685, "text": "Mobile digital assistants such as Microsoft Cortana and Google Now currently offer appealing proactive experiences to users, which aim to deliver the right information at the right time. To achieve this goal, it is crucial to precisely predict users’ real-time intent. Intent is closely related to context, which includes not only the spatial-temporal information but also users’ current activities that can be sensed by mobile devices. The relationship between intent and context is highly dynamic and exhibits chaotic sequential correlation. The context itself is often sparse and heterogeneous. The dynamics and co-movement among contextual signals are also elusive and complicated. Traditional recommendation models cannot directly apply to proactive experiences because they fail to tackle the above challenges. Inspired by the nowcasting practice in meteorology and macroeconomics, we propose an innovative collaborative nowcasting model to effectively resolve these challenges. The proposed model successfully addresses sparsity and heterogeneity of contextual signals. It also effectively models the convoluted correlation within contextual signals and between context and intent. Specifically, the model first extracts collaborative latent factors, which summarize shared temporal structural patterns in contextual signals, and then exploits the collaborative Kalman Filter to generate serially correlated personalized latent factors, which are utilized to monitor each user’s realtime intent. Extensive experiments with real-world data sets from a commercial digital assistant demonstrate the effectiveness of the collaborative nowcasting model. The studied problem and model provide inspiring implications for new paradigms of recommendations on mobile intelligent devices.", "meta": {}, "annotation_approver": null, "labels": [[236, 267, "Task"], [698, 719, "Method"], [1624, 1654, "Method"], [913, 943, "Method"], [1738, 1783, "Task"]]}
{"id": 4686, "text": "To establish an automatic conversation system between humans and computers is regarded as one of the most hardcore problems in computer science, which involves interdisciplinary techniques in information retrieval, natural language processing, artificial intelligence, etc. The challenges lie in how to respond so as to maintain a relevant and continuous conversation with humans. Along with the prosperity of Web 2.0, we are now able to collect extremely massive conversational data, which are publicly available. It casts a great opportunity to launch automatic conversation systems. Owing to the diversity of Web resources, a retrieval-based conversation system will be able to find at least some responses from the massive repository for any user inputs. Given a human issued message, i.e., query, our system would provide a reply after adequate training and learning of how to respond. In this paper, we propose a retrieval-based conversation system with the deep learning-to-respond schema through a deep neural network framework driven by web data. The proposed model is general and unified for different conversation scenarios in open domain. We incorporate the impact of multiple data inputs, and formulate various features and factors with optimization into the deep learning framework. In the experiments, we investigate the effectiveness of the proposed deep neural network structures with better combinations of all different evidence. We demonstrate significant performance improvement against a series of standard and state-of-art baselines in terms of p@1, MAP, nDCG, and MRR for conversational purposes.", "meta": {}, "annotation_approver": null, "labels": [[3, 74, "Task"], [464, 483, "Material"], [919, 954, "Method"], [1006, 1035, "Method"], [1046, 1054, "Material"], [1272, 1295, "Method"], [1366, 1385, "Method"], [1568, 1571, "Metric"], [1573, 1576, "Metric"], [1578, 1582, "Metric"], [1588, 1591, "Metric"]]}
{"id": 4687, "text": "We study the problem of using the crowd to perform entity resolution (ER) on a set of records. For many types of records, especially those involving images, such a task can be difficult for machines, but relatively easy for humans. Typical crowd-based ER approaches ask workers for pairwise judgments between records, which quickly becomes prohibitively expensive even for moderate numbers of records. In this paper, we reduce the cost of pairwise crowd ER approaches by soliciting the crowd for attribute labels on records, and then asking for pairwise judgments only between records with similar sets of attribute labels. However, due to errors induced by crowd-based attribute labeling, a naive attribute-based approach becomes extremely inaccurate even with few attributes. To combat these errors, we use error mitigation strategies which allow us to control the accuracy of our results while maintaining significant cost reductions. We develop a probabilistic model which allows us to determine the optimal, lowest-cost combination of error mitigation strategies needed to achieve a minimum desired accuracy. We test our approach with actual crowdworkers on a dataset of celebrity images, and find that our results yield crowd ER strategies which achieve high accuracy yet are significantly lower cost than pairwise-only approaches.", "meta": {}, "annotation_approver": null, "labels": [[51, 73, "Task"], [240, 254, "Task"], [439, 456, "Task"], [692, 722, "Method"], [1104, 1112, "Metric"], [1176, 1192, "Material"], [1226, 1234, "Task"], [1265, 1273, "Metric"], [1312, 1336, "Method"], [951, 970, "Method"]]}

{"id": 1554, "text": "In the last years, job recommender systems have become popular since they successfully reduce information overload by generating personalized job suggestions. Although in the literature exists a variety of techniques and strategies used as part of job recommender systems, most of them fail to recommending job vacancies that fit properly to the job seekers profiles. Thus, the contributions of this work are threefold, we: i) made publicly available a new dataset formed by a set of job seekers profiles and a set of job vacancies collected from different job search engine sites; ii) put forward the proposal of a framework for job recommendation based on professional skills of job seekers; and iii) carried out an evaluation to quantify empirically the recommendation abilities of two state-of-the-art methods, considering different configurations, within the proposed framework. We thus present a general panorama of job recommendation task aiming to facilitate research and real-world application design regarding this important issue.", "annotations": [{"label": 14, "start_offset": 457, "end_offset": 464, "user": 2}, {"label": 15, "start_offset": 477, "end_offset": 495, "user": 2}, {"label": 14, "start_offset": 616, "end_offset": 625, "user": 2}, {"label": 18, "start_offset": 629, "end_offset": 648, "user": 2}, {"label": 16, "start_offset": 649, "end_offset": 677, "user": 3}, {"label": 18, "start_offset": 630, "end_offset": 648, "user": 3}, {"label": 18, "start_offset": 457, "end_offset": 464, "user": 3}, {"label": 18, "start_offset": 718, "end_offset": 728, "user": 3}, {"label": 15, "start_offset": 511, "end_offset": 531, "user": 2}, {"label": 18, "start_offset": 922, "end_offset": 940, "user": 2}, {"label": 14, "start_offset": 941, "end_offset": 945, "user": 2}], "meta": {}, "annotation_approver": null}
{"id": 1555, "text": "We present an Information Retrieval framework that leverages Heterogeneous Information Network (HIN) embeddings for contextual suggestion. Our method represents users, documents and other context-related documents as heterogeneous objects in a HIN. Using meta-paths, selected based on domain knowledge, we create graph embeddings from this network, thereby learning a representation of users and objects in the same semantic vector space. This allows inferences of user interest on unseen objects based on distance in the embedding space. These object distances are then incorporated as features in a well-established learning to rank (LTR) framework. We make use of the 2016 TREC Contextual Suggestion (TRECCS) dataset, which contains user profiles in the form of relevance-rated documents, and demonstrate the competitiveness of our approach by comparing our system to the best performing systems of the TRECCS task.", "annotations": [{"label": 18, "start_offset": 116, "end_offset": 137, "user": 2}, {"label": 15, "start_offset": 670, "end_offset": 719, "user": 2}, {"label": 14, "start_offset": 587, "end_offset": 595, "user": 2}, {"label": 16, "start_offset": 61, "end_offset": 94, "user": 3}, {"label": 18, "start_offset": 14, "end_offset": 34, "user": 3}], "meta": {}, "annotation_approver": null}
{"id": 1556, "text": "Entity Mixture refers to a phenomenon that the information on an entity is mistaken as attributes of another entity in information extraction during knowledge base (KB) construction and population. To improve the quality of knowledge-based services, data accuracy and validity in KBs should be enhanced. This paper presents a clustering analysis-based approach for detecting potentially mixed entities in a KB. Our approach aims at detecting the inconsistency of the attribute values of a KB instance as an indication of entity mixture occurrence. This paper also presents an experiment conducted on a data set of industrial applications to demonstrate the process of entity mixture detection. Experimental results show that our proposed methodology performs well in detecting mixed entities.", "annotations": [{"label": 18, "start_offset": 668, "end_offset": 692, "user": 2}, {"label": 16, "start_offset": 326, "end_offset": 360, "user": 2}, {"label": 16, "start_offset": 326, "end_offset": 351, "user": 3}, {"label": 18, "start_offset": 365, "end_offset": 409, "user": 3}], "meta": {}, "annotation_approver": null}
{"id": 1557, "text": "Starting with the earliest studies showing that the spread of new trends, information, and innovations is closely related to the social influence exerted on people by their social networks, the research on social influence theory took off, providing remarkable evidence on social influence induced viral phenomena. Fueled by the extreme popularity of online social networks and social media, computational social influence has emerged as a subfield of data mining whose goal is to analyze and optimize social influence using computational frameworks such as algorithm design and theoretical modeling. One of the fundamental problems in this field is the problem of influence maximization, primarily motivated by the application of viral marketing. The objective is to identify a small set of users in a social network who, when convinced to adopt a product, shall influence others in the network in a manner that leads to a large number of adoptions.\n In this tutorial, we extensively survey the research on social influence propagation and maximization, with a focus on the recent algorithmic and theoretical advances. To this end, we provide detailed reviews of the latest research effort devoted to (i) improving the efficiency and scalability of the influence maximization algorithms; (ii) context-aware modeling of the influence maximization problem to better capture real-world marketing scenarios; (iii) modeling and learning of real-world social influence; (iv) bridging the gap between social advertising and viral marketing.", "annotations": [{"label": 18, "start_offset": 665, "end_offset": 687, "user": 2}, {"label": 18, "start_offset": 1008, "end_offset": 1053, "user": 2}, {"label": 18, "start_offset": 665, "end_offset": 687, "user": 3}, {"label": 16, "start_offset": 1294, "end_offset": 1307, "user": 3}], "meta": {}, "annotation_approver": null}
{"id": 1558, "text": "Problems involving multiple networks are prevalent in many scientific and other domains. In particular, network alignment, or the task of identifying corresponding nodes in different networks, has applications across the social and natural sciences. Motivated by recent advancements in node representation learning for single-graph tasks, we propose REGAL (REpresentation learning-based Graph ALignment), a framework that leverages the power of automatically-learned node representations to match nodes across different graphs. Within REGAL we devise xNetMF, an elegant and principled node embedding formulation that uniquely generalizes to multi-network problems. Our results demonstrate the utility and promise of unsupervised representation learning-based network alignment in terms of both speed and accuracy. REGAL runs up to 30x faster in the representation learning stage than comparable methods, outperforms existing network alignment methods by 20 to 30% accuracy on average, and scales to networks with millions of nodes each.", "annotations": [{"label": 16, "start_offset": 350, "end_offset": 403, "user": 2}, {"label": 18, "start_offset": 104, "end_offset": 121, "user": 2}, {"label": 18, "start_offset": 138, "end_offset": 191, "user": 2}, {"label": 18, "start_offset": 138, "end_offset": 191, "user": 3}, {"label": 17, "start_offset": 794, "end_offset": 799, "user": 3}, {"label": 17, "start_offset": 804, "end_offset": 812, "user": 3}, {"label": 16, "start_offset": 445, "end_offset": 487, "user": 3}], "meta": {}, "annotation_approver": "vinicius"}
{"id": 1559, "text": "Learning node representations for networks has attracted much attention recently due to its effectiveness in a variety of applications. This paper focuses on learning node representations for heterogeneous star networks, which have a center node type linked with multiple attribute node types through different types of edges. In heterogeneous star networks, we observe that the training order of different types of edges affects the learning performance significantly. Therefore we study learning curricula for node representation learning in heterogeneous star networks, i.e., learning an optimal sequence of edges of different types for the node representation learning process. We formulate the problem as a Markov decision process, with the action as selecting a specific type of edges for learning or terminating the training process, and the state as the sequence of edge types selected so far. The reward is calculated as the performance on external tasks with node representations as features, and the goal is to take a series of actions to maximize the cumulative rewards. We propose an approach based on deep reinforcement learning for this problem. Our approach leverages LSTM models to encode states and further estimate the expected cumulative reward of each state-action pair, which essentially measures the long-term performance of different actions at each state. Experimental results on real-world heterogeneous star networks demonstrate the effectiveness and efficiency of our approach over competitive baseline approaches.", "annotations": [{"label": 18, "start_offset": 158, "end_offset": 219, "user": 2}, {"label": 14, "start_offset": 1097, "end_offset": 1105, "user": 2}, {"label": 16, "start_offset": 1115, "end_offset": 1142, "user": 2}, {"label": 18, "start_offset": 158, "end_offset": 187, "user": 3}, {"label": 16, "start_offset": 1183, "end_offset": 1188, "user": 3}, {"label": 16, "start_offset": 712, "end_offset": 735, "user": 3}, {"label": 17, "start_offset": 1460, "end_offset": 1473, "user": 3}, {"label": 17, "start_offset": 1478, "end_offset": 1488, "user": 3}], "meta": {}, "annotation_approver": null}
{"id": 1560, "text": "At its core, the Document Object Model (DOM) defines a tree-like data structure for representing documents in general and HTML documents in particular. It is the heart of any modern web browser. Formalizing the key concepts of the DOM is a prerequisite for the formal reasoning over client-side JavaScript programs and for the analysis of security concepts in modern web browsers. We present a formalization of the core DOM, with focus on the node-tree and the operations defined on node-trees, in Isabelle/HOL. We use the formalization to verify the functional correctness of the most important functions defined in the DOM standard. Moreover, our formalization is (1) extensible, i.e., can be extended without the need of re-proving already proven properties and (2) executable, i.e., we can generate executable code from our specification.", "annotations": [{"label": 18, "start_offset": 84, "end_offset": 106, "user": 3}, {"label": 18, "start_offset": 394, "end_offset": 423, "user": 3}, {"label": 15, "start_offset": 122, "end_offset": 136, "user": 2}, {"label": 18, "start_offset": 540, "end_offset": 633, "user": 2}, {"label": 16, "start_offset": 394, "end_offset": 423, "user": 2}], "meta": {}, "annotation_approver": null}
{"id": 1561, "text": "The value of microblogging services (such as Twitter) and social networks (such as Facebook) in disseminating and discussing important events is currently under serious threat from automated or human contributors employed to distort information. While detecting coordinated attacks by their behaviour (e.g. different accounts posting the same images or links, fake profiles, etc.) has been already explored, here we look at detecting coordination in the content (words, phrases, sentences). We are proposing a metric capable of capturing the differences between organic and coordinated posts, which is based on the estimated probability of coincidentally repeating a word sequence. Our simulation results support our conjecture that only when the metric takes the context and the properties of the repeated sequence into consideration, it is capable of separating organic and coordinated content. We also demonstrate how those context-specific adjustments can be obtained using existing resources.", "annotations": [{"label": 18, "start_offset": 252, "end_offset": 281, "user": 3}, {"label": 16, "start_offset": 510, "end_offset": 591, "user": 3}, {"label": 15, "start_offset": 45, "end_offset": 52, "user": 2}, {"label": 15, "start_offset": 83, "end_offset": 91, "user": 2}, {"label": 18, "start_offset": 252, "end_offset": 300, "user": 2}, {"label": 14, "start_offset": 510, "end_offset": 516, "user": 2}, {"label": 18, "start_offset": 528, "end_offset": 591, "user": 2}, {"label": 16, "start_offset": 615, "end_offset": 680, "user": 2}], "meta": {}, "annotation_approver": "vinicius"}
{"id": 1562, "text": "In this paper, we improve the low-rank matrix completion algorithm by assuming that the data points lie in a union of low dimensional subspaces. We applied the self-expressiveness, which is a property of a dataset when the data points lie in a union of low dimensional subspaces, to the low-rank matrix completion. By considering self-expressiveness of low dimensional subspaces, the proposed low-rank matrix completion may perform well even with little information, leading to the robust completion on a dataset with high missing rate. In our experiments on movie rating datasets, the proposed model outperforms state-of-the-art matrix completion models. In clustering experiments conducted on MNIST dataset, the result indicates that our method closely recovers the subspaces of original dataset even with the high missing rate.", "annotations": [{"label": 18, "start_offset": 18, "end_offset": 66, "user": 3}, {"label": 16, "start_offset": 160, "end_offset": 179, "user": 3}, {"label": 15, "start_offset": 695, "end_offset": 700, "user": 3}, {"label": 16, "start_offset": 160, "end_offset": 179, "user": 2}, {"label": 18, "start_offset": 39, "end_offset": 56, "user": 2}, {"label": 16, "start_offset": 393, "end_offset": 419, "user": 2}, {"label": 15, "start_offset": 559, "end_offset": 580, "user": 2}, {"label": 15, "start_offset": 694, "end_offset": 708, "user": 2}, {"label": 14, "start_offset": 740, "end_offset": 746, "user": 2}], "meta": {}, "annotation_approver": "vinicius"}
{"id": 1563, "text": "Our usage of language is not solely reliant on cognition but is arguably determined by myriad external factors leading to a global variability of linguistic patterns. This issue, which lies at the core of sociolinguistics and is backed by many small-scale studies on faceto-face communication, is addressed here by constructing a dataset combining the largest French Twitter corpus to date with detailed socioeconomic maps obtained from national census in France. We show how key linguistic variables measured in individual Twitter streams depend on factors like socioeconomic status, location, time, and the social network of individuals. We found that (i) people of higher socioeconomic status, active to a greater degree during the daytime, use a more standard language; (ii) the southern part of the country is more prone to use more standard language than the northern one, while locally the used variety or dialect is determined by the spatial distribution of socioeconomic status; and (iii) individuals connected in the social network are closer linguistically than disconnected ones, even after the effects of status homophily have been removed. Our results inform sociolinguistic theory and may inspire novel learning methods for the inference of socioeconomic status of people from the way they tweet.", "annotations": [{"label": 14, "start_offset": 330, "end_offset": 337, "user": 2}, {"label": 15, "start_offset": 360, "end_offset": 381, "user": 2}, {"label": 18, "start_offset": 4, "end_offset": 21, "user": 3}, {"label": 15, "start_offset": 367, "end_offset": 381, "user": 3}], "meta": {}, "annotation_approver": "vinicius"}
{"id": 1564, "text": "We will demonstrate a reusable framework for developing knowledge graphs that supports general, open-ended development of knowledge curation, interaction, and inference. Knowledge graphs need to be easily maintainable and usable in sometimes complex application settings. Often, scaling knowledge graph updates can require developing a knowledge curation pipeline that either replaces the graph wholesale whenever updates are made, or requires detailed tracking of knowledge provenance across multiple data sources. Fig. 1 shows how Whyis provides a semantic analysis ecosystem: an environment that supports research and development of semantic analytics for which we previously had to build custom applications [3,4]. Users interact through a suite of knowledge graph views driven by the node type and view requested in the URL. Knowledge curation methods include Semantic ETL, external linked data mapping,and Natural Language Processing (NLP). Autonomous inference agents expand the available knowledge using traditional deductive reasoning as well as inductive methods that can include predictive models, statistical reasoners, and machine learning. Whyis is used in a number of areas today, including nanopolymers, spectrum policy, and health informatics. We demonstrate Whyis by creating and deploying an example Biological Knowledge Graph (BioKG), using data from DrugBank and Uniprot1, and briefly discuss benefits of using our approach over a conventional knowledge graph pipeline.", "annotations": [{"label": 14, "start_offset": 31, "end_offset": 40, "user": 2}, {"label": 18, "start_offset": 121, "end_offset": 168, "user": 2}, {"label": 16, "start_offset": 865, "end_offset": 877, "user": 2}, {"label": 16, "start_offset": 879, "end_offset": 907, "user": 2}, {"label": 16, "start_offset": 912, "end_offset": 945, "user": 2}, {"label": 18, "start_offset": 830, "end_offset": 848, "user": 2}, {"label": 14, "start_offset": 1065, "end_offset": 1072, "user": 2}, {"label": 16, "start_offset": 1090, "end_offset": 1107, "user": 2}, {"label": 16, "start_offset": 1109, "end_offset": 1130, "user": 2}, {"label": 16, "start_offset": 1136, "end_offset": 1152, "user": 2}, {"label": 15, "start_offset": 1371, "end_offset": 1379, "user": 2}, {"label": 15, "start_offset": 1384, "end_offset": 1392, "user": 2}, {"label": 18, "start_offset": 122, "end_offset": 140, "user": 3}, {"label": 16, "start_offset": 550, "end_offset": 567, "user": 3}, {"label": 14, "start_offset": 912, "end_offset": 939, "user": 3}, {"label": 14, "start_offset": 1319, "end_offset": 1345, "user": 3}, {"label": 15, "start_offset": 1371, "end_offset": 1379, "user": 3}], "meta": {}, "annotation_approver": null}
{"id": 1565, "text": "Friend and item recommendation on a social media site is an important task, which not only brings conveniences to users but also benefits platform providers. However, recommendation for newly launched social media sites is challenging because they often lack user historical data and encounter data sparsity and cold-start problem. Thus, it is important to exploit auxiliary information to help improve recommendation performances on these sites. Existing approaches try to utilize the knowledge transferred from other mature sites, which often require overlapped users or similar items to ensure an effective knowledge transfer. However, these assumptions may not hold in practice because 1) Overlapped user set is often unavailable and costly to identify due to the heterogeneous user profile, content and network data, and 2) Different schemes to show item attributes across sites cause the attribute values inconsistent, incomplete, and noisy. Thus, how to transfer knowledge when no direct bridge is given between two social media sites remains a challenge. In addition, another auxiliary information we can exploit is the mutual benefit between social relationships and rating preferences within the platform. User-user relationships are widely used as side information to improve item recommendation, whereas how to exploit user-item interactions for friend recommendation is rather limited. To tackle these challenges, we propose aCross media jointF riend andI temRe commendation framework (CrossFire ), which can capture both 1) cross-platform knowledge transfer, and 2) within-platform correlations among user-user relations and user-item interactions. Empirical results on real-world datasets demonstrate the effectiveness of the proposed framework.", "annotations": [{"label": 14, "start_offset": 70, "end_offset": 74, "user": 2}, {"label": 18, "start_offset": 1287, "end_offset": 1306, "user": 2}, {"label": 18, "start_offset": 1358, "end_offset": 1379, "user": 2}, {"label": 16, "start_offset": 1439, "end_offset": 1510, "user": 2}, {"label": 16, "start_offset": 1553, "end_offset": 1571, "user": 3}, {"label": 17, "start_offset": 1720, "end_offset": 1733, "user": 3}, {"label": 16, "start_offset": 1596, "end_offset": 1608, "user": 3}, {"label": 18, "start_offset": 16, "end_offset": 48, "user": 3}], "meta": {}, "annotation_approver": null}
{"id": 1566, "text": "Intelligent personal assistant systems with either text-based or voice-based conversational interfaces are becoming increasingly popular around the world. Retrieval-based conversation models have the advantages of returning fluent and informative responses. Most existing studies in this area are on open domain ''chit-chat'' conversations or task / transaction oriented conversations. More research is needed for information-seeking conversations. There is also a lack of modeling external knowledge beyond the dialog utterances among current conversational models. In this paper, we propose a learning framework on the top of deep neural matching networks that leverages external knowledge for response ranking in information-seeking conversation systems. We incorporate external knowledge into deep neural models with pseudo-relevance feedback and QA correspondence knowledge distillation. Extensive experiments with three information-seeking conversation data sets including both open benchmarks and commercial data show that, our methods outperform various baseline methods including several deep text matching models and the state-of-the-art method on response selection in multi-turn conversations. We also perform analysis over different response types, model variations and ranking examples. Our models and research findings provide new insights on how to utilize external knowledge with deep neural models for response selection and have implications for the design of the next generation of information-seeking conversation systems.", "annotations": [{"label": 16, "start_offset": 155, "end_offset": 190, "user": 2}, {"label": 18, "start_offset": 414, "end_offset": 447, "user": 2}, {"label": 16, "start_offset": 595, "end_offset": 613, "user": 2}, {"label": 16, "start_offset": 628, "end_offset": 657, "user": 2}, {"label": 18, "start_offset": 696, "end_offset": 712, "user": 2}, {"label": 16, "start_offset": 797, "end_offset": 815, "user": 2}, {"label": 16, "start_offset": 821, "end_offset": 846, "user": 2}, {"label": 16, "start_offset": 851, "end_offset": 891, "user": 2}, {"label": 14, "start_offset": 989, "end_offset": 999, "user": 2}, {"label": 14, "start_offset": 1015, "end_offset": 1019, "user": 2}, {"label": 14, "start_offset": 1035, "end_offset": 1042, "user": 2}, {"label": 18, "start_offset": 716, "end_offset": 756, "user": 3}, {"label": 16, "start_offset": 628, "end_offset": 657, "user": 3}, {"label": 14, "start_offset": 851, "end_offset": 853, "user": 3}, {"label": 14, "start_offset": 1131, "end_offset": 1147, "user": 3}, {"label": 14, "start_offset": 314, "end_offset": 323, "user": 3}], "meta": {}, "annotation_approver": null}
{"id": 1567, "text": "Collaborative filtering techniques are a common approach for building recommendations, and have been widely applied in real recommender systems. However, collaborative filtering usually suffers from limited performance due to the sparsity of user-item interaction. To address this issue, auxiliary information is usually used to improve the performance. Transfer learning provides the key idea of using knowledge from auxiliary domains. An assumption of transfer learning in collaborative filtering is that the source domain is a full rating matrix, which may not hold in many real-world applications. In this paper, we investigate how to leverage rating patterns from multiple incomplete source domains to improve the quality of recommender systems. First, by exploiting the transferred learning, we compress the knowledge from the source domain into a cluster-level rating matrix. The rating patterns in the low-level matrix can be transferred to the target domain. Specifically, we design a knowledge extraction method to enrich rating patterns by relaxing the full rating restriction on the source domain. Finally, we propose a robust multiple-rating-pattern transfer learning model for cross-domain collaborative filtering, which is called MINDTL, to accurately predict missing values in the target domain. Extensive experiments on real-world datasets demonstrate that our proposed approach is effective and outperforms several alternative methods.", "annotations": [{"label": 18, "start_offset": 61, "end_offset": 85, "user": 2}, {"label": 16, "start_offset": 154, "end_offset": 178, "user": 2}, {"label": 16, "start_offset": 354, "end_offset": 371, "user": 2}, {"label": 16, "start_offset": 1138, "end_offset": 1187, "user": 2}, {"label": 18, "start_offset": 1191, "end_offset": 1227, "user": 2}, {"label": 16, "start_offset": 1245, "end_offset": 1251, "user": 2}, {"label": 18, "start_offset": 124, "end_offset": 143, "user": 3}, {"label": 16, "start_offset": 776, "end_offset": 796, "user": 3}, {"label": 16, "start_offset": 1245, "end_offset": 1251, "user": 3}], "meta": {}, "annotation_approver": null}
{"id": 1568, "text": "Offering products in the forms of menu bundles is a common practice in marketing to attract customers and maximize revenues. In crowdfunding platforms such as Kickstarter, rewards also play an important part in influencing project success. Designing rewards consisting of the appropriate items is a challenging yet crucial task for the project creators. However, prior research has not considered the strategies project creators take to offer and bundle the rewards, making it hard to study the impact of reward designs on project success. In this paper, we raise a novel research question: understanding project creators' decisions of reward designs to level their chance to succeed. We approach this by modeling the design behavior of project creators, and identifying the behaviors that lead to project success. We propose a probabilistic generative model, Menu-Offering-Bundle (MOB) model, to capture the offering and bundling decisions of project creators based on collected data of 14K crowdfunding projects and their 149K reward bundles across a half-year period. Our proposed model is shown to capture the offering and bundling topics, outperform the baselines in predicting reward designs. We also find that the learned offering and bundling topics carry distinguishable meanings and provide insights of key factors on project success.", "annotations": [{"label": 18, "start_offset": 240, "end_offset": 258, "user": 2}, {"label": 16, "start_offset": 705, "end_offset": 753, "user": 2}, {"label": 16, "start_offset": 759, "end_offset": 813, "user": 2}, {"label": 16, "start_offset": 828, "end_offset": 886, "user": 2}, {"label": 14, "start_offset": 887, "end_offset": 892, "user": 2}, {"label": 15, "start_offset": 988, "end_offset": 1013, "user": 2}, {"label": 15, "start_offset": 1024, "end_offset": 1044, "user": 2}, {"label": 18, "start_offset": 1172, "end_offset": 1197, "user": 2}, {"label": 16, "start_offset": 759, "end_offset": 784, "user": 3}, {"label": 18, "start_offset": 798, "end_offset": 813, "user": 3}, {"label": 14, "start_offset": 128, "end_offset": 140, "user": 3}, {"label": 14, "start_offset": 860, "end_offset": 880, "user": 3}], "meta": {}, "annotation_approver": null}
{"id": 1569, "text": "This paper examines how legacies of thinking about ontology, logic, and how best to approach knowledge representation have become interwoven in the architecture of the technologies that enable a Semantic Web. As a cultural anthropologist, I approach this study with qualitative historical and ethnographic methodologies, positioning the community of researchers that have been involved in the design and implementation of Semantic Web protocols and technologies as my primary field site. Two concepts from Science and Technology Studies are introduced - thought styles and design logics. The paper demonstrates how diverse thinking about how to approach knowledge representation on the Web is rooted in debates that emerged in artificial intelligence in the 1970s and 1980s. It then goes on to discuss how the diverse approaches to Web semantics that emerged from these legacies have cultural and political implications. The paper concludes with a call for further research that positions Web architectures as objects of social and cultural study.", "annotations": [{"label": 18, "start_offset": 93, "end_offset": 117, "user": 2}, {"label": 16, "start_offset": 266, "end_offset": 319, "user": 2}, {"label": 18, "start_offset": 404, "end_offset": 434, "user": 3}, {"label": 16, "start_offset": 11, "end_offset": 19, "user": 3}], "meta": {}, "annotation_approver": null}
{"id": 1570, "text": "Social media has quickly established itself as an important means that people, NGOs and governments use to spread information during natural or man-made disasters, mass emergencies and crisis situations. Given this important role, real-time analysis of social media contents to locate, organize and use valuable information for disaster management is crucial. In this paper, we propose self-learning algorithms that, with minimal supervision, construct a simple bag-of-words model of information expressed in the news about various natural disasters. Such a model is human-understandable, human-modifiable and usable in a realtime scenario. Since tweets are a diffferent category of documents than news, we next propose a model transfer algorithm, which essentially refines the model learned from news by analyzing a large unlabeled corpus of tweets. We show empirically that model transfer improves the predictive accuracy of the model. We demonstrate empirically that our model learning algorithm is better than several state of the art semi-supervised learning algorithms.", "annotations": [{"label": 18, "start_offset": 231, "end_offset": 274, "user": 2}, {"label": 16, "start_offset": 386, "end_offset": 410, "user": 2}, {"label": 16, "start_offset": 462, "end_offset": 480, "user": 2}, {"label": 14, "start_offset": 79, "end_offset": 83, "user": 3}, {"label": 18, "start_offset": 328, "end_offset": 347, "user": 3}, {"label": 16, "start_offset": 231, "end_offset": 265, "user": 3}, {"label": 16, "start_offset": 722, "end_offset": 736, "user": 3}, {"label": 17, "start_offset": 915, "end_offset": 923, "user": 3}], "meta": {}, "annotation_approver": null}
{"id": 1571, "text": "Solving technical problems with complex systems and integrating the many technologies employed in these multifaceted structures has been a recurring theme in Smart Cities research. This paper presents an analysis of the reason this problem has been so well explored but persists with no solution widely available. The problem is viewed as a combination of Smart City needs, governance, and increasingly technically difficult decisions. The paper describes the requirements that must be met to develop a framework that can address this seeming intractable and expanding integration concern, identifies the governance processes that can be used to address this problem, and to manage integration in Smart Cities, The solution proposed is a formalized accepted and managed technology regulated environment introduced by governance groups composed of city planners/managers, citizen, stake holders, and technology delivery organizations. The solution requirements dictate the establishment of a standard that would guide the development and usage of automated, autonomous components, integrating dynamically with software agents. All of this working to rapidly optimize shared resources through error handling processes executing largely at no cost except those of processing time, meeting safety guidelines, satisfying operational monitoring needs, and meeting post issue liability guidelines. This technical standard would obligate developers and vendors to meet safety standards and accept liability for malfeasance. As initiating steps, Smart City managers must come together and establish a basic understanding of the goals and regulations, and the methodologies for implementing them.", "annotations": [{"label": 18, "start_offset": 682, "end_offset": 709, "user": 2}, {"label": 14, "start_offset": 1537, "end_offset": 1547, "user": 3}, {"label": 16, "start_offset": 204, "end_offset": 212, "user": 3}], "meta": {}, "annotation_approver": null}
{"id": 1572, "text": "Verifiability is one of the core editing principles in Wikipedia, where editors are encouraged to provide citations for the added statements. Statements can be any arbitrary piece of text, ranging from a sentence up to a paragraph. However, in many cases, citations are either outdated, missing, or link to non-existing references (e.g. dead URL, moved content etc.). In total, 20% of the cases such citations refer to news articles and represent the second most cited source. Even in cases where citations are provided, there are no explicit indicators for the span of a citation for a given piece of text. In addition to issues related with the verifiability principle, many Wikipedia entity pages are incomplete, with relevant information that is already available in online news sources missing. Even for the already existing citations, there is often a delay between the news publication time and the reference time. In this thesis, we address the aforementioned issues and propose automated approaches that enforce the verifiability principle in Wikipedia, and suggest relevant and missing news references for further enriching Wikipedia entity pages. To this end we make the following contributions as part of this thesis [1, 2, 3, 4]", "annotations": [{"label": 18, "start_offset": 1124, "end_offset": 1156, "user": 2}, {"label": 18, "start_offset": 0, "end_offset": 13, "user": 3}, {"label": 16, "start_offset": 987, "end_offset": 1007, "user": 3}, {"label": 14, "start_offset": 1052, "end_offset": 1061, "user": 3}, {"label": 14, "start_offset": 677, "end_offset": 686, "user": 3}], "meta": {}, "annotation_approver": null}
{"id": 1573, "text": "Social media, as a major platform to disseminate information, has changed the way users and communities contribute content. In this paper, we aim to study content modifications on public Facebook pages operated by news media, community groups, and bloggers. We also study the possible reasons behind them, and their effects on user interaction. We conducted a detailed study of Content Censorship (CC) and Content Edit (CE) in Facebook using a detailed longitudinal dataset consisting of 57 public Facebook pages over 3 weeks covering 145,955 posts and 9,379,200 comments. We detected many CC and CE activities between 28% and 56% of these pages (in both Facebook Posts and Comments). Manual judgements on these post/comment removals and edits show that majority of the content censorship is related to negative reports on events and personal grouses, and content edit is mainly performed to improve content quality and correctness. Furthermore, recency effect is also observed as part of Facebook content modification behavior.", "annotations": [{"label": 18, "start_offset": 378, "end_offset": 436, "user": 2}, {"label": 15, "start_offset": 488, "end_offset": 571, "user": 2}, {"label": 14, "start_offset": 378, "end_offset": 396, "user": 3}, {"label": 14, "start_offset": 406, "end_offset": 418, "user": 3}, {"label": 16, "start_offset": 149, "end_offset": 176, "user": 3}], "meta": {}, "annotation_approver": null}
{"id": 1574, "text": "We consider the problem of discovering local events on the web, where events are entities extracted from webpages. Examples of such local events include small venue concerts, farmers markets, sports activities, etc. Given an event entity, we propose a graph-based framework for retrieving a ranked list of related events that a user is likely to be interested in attending. Due to the difficulty of obtaining ground-truth labels for event entities, which are temporal and are constrained by location, our retrieval framework is unsupervised, and its graph-based formulation addresses (a) the challenge of feature sparseness and noisiness, and (b) the semantic mismatch problem in a self-contained and principled manner.\n To validate our methods, we collect human annotations and conduct a comprehensive empirical study, analyzing the performance of our methods with regard to relevance, recall, and diversity. This study shows that our graph-based framework is significantly better than any individual feature source, and can be further improved with minimal supervision.", "annotations": [{"label": 18, "start_offset": 27, "end_offset": 62, "user": 2}, {"label": 16, "start_offset": 252, "end_offset": 273, "user": 2}, {"label": 16, "start_offset": 749, "end_offset": 774, "user": 2}, {"label": 17, "start_offset": 876, "end_offset": 909, "user": 2}, {"label": 18, "start_offset": 27, "end_offset": 51, "user": 3}, {"label": 16, "start_offset": 936, "end_offset": 957, "user": 3}], "meta": {}, "annotation_approver": null}
{"id": 1575, "text": "While search technology is widely used for learning-oriented information needs, the results provided by popular services such as Web search engines are optimized primarily for generic relevance, not effective learning outcomes. As a result, the typical information trail that a user must follow while searching to achieve a learning goal may be an inefficient one involving unnecessarily easy or difficult content, or material that is irrelevant to actual learning progress relative to a user's existing knowledge. We address this problem by introducing a novel theoretical framework, algorithms, and empirical analysis of an information retrieval model that is optimized for learning outcomes instead of generic relevance. We do this by formulating an optimization problem that incorporates a cognitive learning model into a retrieval objective, and then give an algorithm for an efficient approximate solution to find the search results that represent the best 'training set' for a human learner. Our model can personalize results for an individual user's learning goals, as well as account for the effort required to achieve those goals for a given set of retrieval results. We investigate the effectiveness and efficiency of our retrieval framework relative to a commercial search engine baseline ('Google') through a crowdsourced user study involving a vocabulary learning task, and demonstrate the effectiveness of personalized results from our model on word learning outcomes.", "annotations": [{"label": 18, "start_offset": 43, "end_offset": 78, "user": 3}, {"label": 17, "start_offset": 1197, "end_offset": 1210, "user": 3}, {"label": 17, "start_offset": 1215, "end_offset": 1225, "user": 3}, {"label": 16, "start_offset": 1233, "end_offset": 1252, "user": 3}, {"label": 14, "start_offset": 1292, "end_offset": 1300, "user": 3}, {"label": 16, "start_offset": 626, "end_offset": 653, "user": 2}, {"label": 14, "start_offset": 1378, "end_offset": 1382, "user": 2}, {"label": 18, "start_offset": 301, "end_offset": 337, "user": 2}], "meta": {}, "annotation_approver": null}
{"id": 1576, "text": "Current search and recommendation engines enable us to effectively retrieve a set of documents based on topical relevance. What is not taken into account is the knowledge a user may already have about a topic, e.g., whether information is redundant or whether he/she is able to understand the results. We propose a method to measure demonstrated potential domain knowledge (DPDK) as a proxy for knowledge and use this metric to analyse the query log of a user spanning over 10 years.", "annotations": [{"label": 18, "start_offset": 428, "end_offset": 459, "user": 3}, {"label": 16, "start_offset": 325, "end_offset": 372, "user": 3}, {"label": 15, "start_offset": 440, "end_offset": 449, "user": 2}, {"label": 18, "start_offset": 333, "end_offset": 372, "user": 2}], "meta": {}, "annotation_approver": null}
{"id": 1577, "text": "In the current online Open Science context, scientific datasets and tools for deep text analysis, visualization and exploitation play a major role. We present a system for deep analysis and annotation of scientific text collections. We also introduce the first version of the SEPLN Anthology, a bi-lingual (Spanish and English) fully annotated text resource in the field of natural language processing that we created with our system. Moreover, a faceted-search and visualization system to explore the created resource is introduced. All resources created for this paper will be available to the research community.", "annotations": [{"label": 14, "start_offset": 22, "end_offset": 34, "user": 3}, {"label": 18, "start_offset": 177, "end_offset": 219, "user": 3}, {"label": 16, "start_offset": 161, "end_offset": 167, "user": 3}, {"label": 16, "start_offset": 282, "end_offset": 291, "user": 3}, {"label": 18, "start_offset": 78, "end_offset": 128, "user": 2}, {"label": 15, "start_offset": 44, "end_offset": 73, "user": 2}, {"label": 15, "start_offset": 204, "end_offset": 231, "user": 2}, {"label": 15, "start_offset": 276, "end_offset": 357, "user": 2}], "meta": {}, "annotation_approver": "vinicius"}
{"id": 1578, "text": "An automated-based system has been developed in order to gather stories from iCanQuit. Similarly, using Twitter Streaming API, geolocated tweets within UK region have been collected during a three months period, and those related to smoking cessation, according to the semantic text matching, are extracted and stored in a database. An automated classifier has been employed to identify the four most dominant categories in Health field for each document of blogs or Twitter dataset. A total of 880 stories from iCanQuit and 22155 relevant tweets have been collected and indexed in SQLite database. The automated classifier highlighted four categories: Weight Loss, Mental Health, Addiction, Support Group. The analysis surprisingly reveals that both blogs and Twitter datasets agree that the dominant source of smoking cessation is related to weight loss (shape body appearance), or body-look, while the support group, which includes any clinician supports, plays little impact on the smokersâ€™ quit motivation, a result that maybe precious for health authorities.", "annotations": [{"label": 18, "start_offset": 57, "end_offset": 71, "user": 3}, {"label": 15, "start_offset": 77, "end_offset": 85, "user": 3}, {"label": 15, "start_offset": 104, "end_offset": 111, "user": 3}, {"label": 16, "start_offset": 711, "end_offset": 719, "user": 3}, {"label": 15, "start_offset": 495, "end_offset": 520, "user": 2}, {"label": 15, "start_offset": 525, "end_offset": 546, "user": 2}, {"label": 15, "start_offset": 582, "end_offset": 597, "user": 2}, {"label": 16, "start_offset": 336, "end_offset": 356, "user": 2}, {"label": 18, "start_offset": 377, "end_offset": 436, "user": 2}, {"label": 16, "start_offset": 603, "end_offset": 623, "user": 2}, {"label": 15, "start_offset": 751, "end_offset": 777, "user": 2}], "meta": {}, "annotation_approver": "vinicius"}